---
title: "tf3820_tc3_machine_learning_server_EN"
product: "tf3820"
category: "Machine_Learning"
tags: []
language: "EN"
document_type: "Manual"
version: "1.1.0"
source_pdf: "https://download.beckhoff.com/download/Document/automation/twincat3/tf3820_tc3_machine_learning_server_EN.pdf"
release_date: "2025-06-12"
---
Manual | EN TF3820 Twin CAT 3 | Machine Learning Server 2025-06-12 | Version: 1.1.0
## Page 3

Table of contents Table of contents 1 Foreword.................................................................................................................................................... 5 1.1 Notes on the documentation............................................................................................................. 5 1.2 For your safety.................................................................................................................................. 5 1.3 Notes on information security............................................................................................................ 7 1.4 Documentation issue status.............................................................................................................. 8 2 Overview.................................................................................................................................................... 9 3 Installation............................................................................................................................................... 11 3.1 Licensing......................................................................................................................................... 12 3.2 System configuration for GPU operation......................................................................................... 15 3.2.1 Setting up an NVIDIA graphics card................................................................................ 15 4 Quickstart................................................................................................................................................ 17 5 Technical introduction............................................................................................................................ 20 5.1 Workflow.......................................................................................................................................... 20 5.1.1 Preparing ONNX for use with Twin CAT Machine Learning Server.................................. 20 5.1.2 Make model description files available on the Server Device.......................................... 22 5.1.3 Configuring the server from the PLC client...................................................................... 23 5.1.4 Execute AI model............................................................................................................. 24 5.1.5 Updating the AI model...................................................................................................... 26 5.2 Twin CAT Machine Learning Model Manager.................................................................................. 27 5.2.1 Graphical user interface................................................................................................... 27 5.2.2 Python interface............................................................................................................... 29 5.2.3 Command Line Interface.................................................................................................. 30 5.3 ONNX Support................................................................................................................................ 31 5.4 Tc Ml Server Service......................................................................................................................... 32 5.4.1 Execution Provider........................................................................................................... 33 5.5 Performance, latencies and configuration....................................................................................... 33 6 API............................................................................................................................................................ 36 6.1 Function blocks............................................................................................................................... 36 6.1.1 FB_Ml Svr Prediction......................................................................................................... 36 6.2 Data types....................................................................................................................................... 41 6.2.1 E_Execution Provider........................................................................................................ 41 6.2.2 ST_Prediction Parameter.................................................................................................. 41 6.2.3 ST_Latency Monitor.......................................................................................................... 42 7 Samples................................................................................................................................................... 43 7.1 AI-based image processing............................................................................................................. 43 7.2 Custom attributes............................................................................................................................ 45 8 Appendix.................................................................................................................................................. 49 8.1 Error Codes..................................................................................................................................... 49 8.2 Log files........................................................................................................................................... 52 8.3 Third-party components.................................................................................................................. 52 8.4 Support and Service........................................................................................................................ 53 TF3820 Version: 1.1.0 3
## Page 4

Table of contents 4 Version: 1.1.0 TF3820
## Page 5

Foreword 1 Foreword 1.1 Notes on the documentation This description is intended exclusively for trained specialists in control and automation technology who are familiar with the applicable national standards. The documentation and the following notes and explanations must be complied with when installing and commissioning the components. The trained specialists must always use the current valid documentation. The trained specialists must ensure that the application and use of the products described is in line with all safety requirements, including all relevant laws, regulations, guidelines, and standards. Disclaimer The documentation has been compiled with care. The products described are, however, constantly under development. We reserve the right to revise and change the documentation at any time and without notice. Claims to modify products that have already been supplied may not be made on the basis of the data, diagrams, and descriptions in this documentation. Trademarks Beckhoff®, ATRO® , Ether CAT®, Ether CAT G®, Ether CAT G10®, Ether CAT P®, MX-System®, Safety over Ether CAT®, TC/BSD®, Twin CAT®, Twin CAT/BSD®, Twin SAFE®, XFC®, XPlanar®, and XTS® are registered and licensed trademarks of Beckhoff Automation Gmb H. If third parties make use of the designations or trademarks contained in this publication for their own purposes, this could infringe upon the rights of the owners of the said designations. Ether CAT® is a registered trademark and patented technology, licensed by Beckhoff Automation Gmb H, Germany. Copyright © Beckhoff Automation Gmb H & Co. KG, Germany. The distribution and reproduction of this document, as well as the use and communication of its contents without express authorization, are prohibited. Offenders will be held liable for the payment of damages. All rights reserved in the event that a patent, utility model, or design are registered. Third-party trademarks Trademarks of third parties may be used in this documentation. You can find the trademark notices here: https://www.beckhoff.com/trademarks. 1.2 For your safety Safety regulations Read the following explanations for your safety. Always observe and follow product-specific safety instructions, which you may find at the appropriate places in this document. Exclusion of liability All the components are supplied in particular hardware and software configurations which are appropriate for the application. Modifications to hardware or software configurations other than those described in the documentation are not permitted, and nullify the liability of Beckhoff Automation Gmb H & Co. KG. TF3820 Version: 1.1.0 5
## Page 6

Foreword Personnel qualification This description is only intended for trained specialists in control, automation, and drive technology who are familiar with the applicable national standards. Signal words The signal words used in the documentation are classified below. In order to prevent injury and damage to persons and property, read and follow the safety and warning notices. Personal injury warnings DANGER Hazard with high risk of death or serious injury. WARNING Hazard with medium risk of death or serious injury. CAUTION There is a low-risk hazard that could result in medium or minor injury. Warning of damage to property or environment NOTICE The environment, equipment, or data may be damaged. Information on handling the product This information includes, for example: recommendations for action, assistance or further information on the product. 6 Version: 1.1.0 TF3820
## Page 7

Foreword 1.3 Notes on information security The products of Beckhoff Automation Gmb H & Co. KG (Beckhoff), insofar as they can be accessed online, are equipped with security functions that support the secure operation of plants, systems, machines and networks. Despite the security functions, the creation, implementation and constant updating of a holistic security concept for the operation are necessary to protect the respective plant, system, machine and networks against cyber threats. The products sold by Beckhoff are only part of the overall security concept. The customer is responsible for preventing unauthorized access by third parties to its equipment, systems, machines and networks. The latter should be connected to the corporate network or the Internet only if appropriate protective measures have been set up. In addition, the recommendations from Beckhoff regarding appropriate protective measures should be observed. Further information regarding information security and industrial security can be found in our https://www.beckhoff.com/secguide. Beckhoff products and solutions undergo continuous further development. This also applies to security functions. In light of this continuous further development, Beckhoff expressly recommends that the products are kept up to date at all times and that updates are installed for the products once they have been made available. Using outdated or unsupported product versions can increase the risk of cyber threats. To stay informed about information security for Beckhoff products, subscribe to the RSS feed at https:// www.beckhoff.com/secinfo. TF3820 Version: 1.1.0 7
## Page 8

Foreword 1.4 Documentation issue status Version Modifications 1.1.x • Notes on compatibility between client and server [} 11] • Adjustments to the installation of the Python package [} 29] NEW: • System configuration for GPU operation [} 15] • Chapter Performance, latencies and configuration [} 33] • Data type ST_Latency Monitor [} 42] 1.0.2 The hexadecimal values have been added to the error codes. 1.0.x 1.0.0 First release 8 Version: 1.1.0 TF3820
## Page 9

Overview 2 Overview Introduction Twin CAT Machine Learning Server enables the execution of AI models directly on the control IPC or on an Edge Device. The Twin CAT Machine Learning Server consists of three components: • The PLC function block as a client of the Machine Learning Server. • The Machine Learning Server as a provider of services (loading, execution, ... of AI models). • The Twin CAT Machine Learning Model Manager (for engineering only) for preparing the ONNX files (AI model file) These components provide asynchronous execution functionality for PLC programs. The concept of asynchronous calculation effectively decouples the AI model execution time from the cyclic operation of the PLC. The Machine Learning Server enables the execution of any sophisticated AI models on both CPUs and NVIDIA GPUs and is therefore particularly suitable for use with the C6043 Industrial PC. The Machine Learning Server is executed in the user mode of the operating system. This results in non- deterministic behavior that can only be partially mitigated by configuring the user mode components accordingly. The Twin CAT Machine Learning Server loads AI models that are provided as ONNX files. All relevant AI frameworks, such as Tensor Flow, Pytorch, Scikit Learn, etc. support this interoperability standard. This decouples the training environment from the execution environment. Any training environment can be used to create AI models, which can then be executed with the Twin CAT Machine Learning Server. Target groups and use cases The Machine Learning Server is aimed at the following use cases, among others: • Use of computationally intensive AI models where the expected reduction in computing time due to acceleration on a GPU overcompensates for the expected computing time fluctuations (jitter). ◦ In particular, vision AI models for image classification, object recognition or segmentation should be mentioned here. • Use of AI models in low-priority tasks that are only loosely coupled with the deterministic PLC program. ◦ AI models whose results are not used by the control system, but are communicated to systems above the control level. For example, process analysis models in which the machine operator is informed, predictive maintenance models in which the service personnel are informed, etc. ◦ AI models whose results are not required by the controller at a specific point in time. For example, AI models to provide optimized or adapted process parameters. Differentiation and comparison with similar Twin CAT products In addition to the Twin CAT Machine Learning Server, there are other Twin CAT products with similar functionality, i.e. the execution of AI models. • TF3800 Twin CAT Machine Learning Inference Engine • TF3810 Twin CAT Neural Network Inference Engine • TF7800 Twin CAT Vision Machine Learning • TF7810 Twin CAT Vision Neural Network The main differences between the listed products and the Twin CAT Machine Learning Server are listed in the following table. Table 1: Product properties in comparison Deterministic AI: TF3800, TF3810, TF7810 Accelerated AI: TF3820 Deterministic AI execution in the Twin CAT process Near-real-time execution in a separate process Execution on standard x64 CPUs Hardware acceleration on NVIDIA GPUs possible TF3820 Version: 1.1.0 9
## Page 10

Overview Deterministic AI: TF3800, TF3810, TF7810 Accelerated AI: TF3820 Supports selected AI models and operators Supports current ONNX Opset version and thus current and diverse AI models Standard PLC function block for easy integration in Standard PLC function block for easy integration in Twin CAT Twin CAT Interoperability through ONNX support Interoperability through ONNX support License bundle: TF3810 includes TF3800, TF7800 Can also be used as a server in a network with and TF7810 several clients 10 Version: 1.1.0 TF3820
## Page 11

Installation 3 Installation To use the Twin CAT Machine Learning Server you need three components: • TF3820 | Twin CAT Machine Learning Server Install and license this workload either directly on your control IPC or on an Edge IPC. ◦ Installs the Tc Ml Server [} 32] service on the system. • TF3830 | Twin CAT Machine Learning Server Client Install this workload on your engineering PC to use the required PLC library. ◦ Installs the PLC library Tc3_Ml Server [} 36] for the Twin CAT 3 XAE. • TF38xx | Twin CAT Machine Learning Model Manager Install this workload on your engineering PC to prepare the ONNX file for use in Twin CAT [} 20]. System requirements: Twin CAT Machine Learning Server (TF3820) Technical data Requirements Operating system Windows10 Target platform x64 Minimum Twin CAT version Twin CAT 3.1 Build 4026 Required Twin CAT setup level Twin CAT 3 XAR Required Twin CAT license TC1000 System requirements: Twin CAT Machine Learning Server Client (TF3830) Technical data Requirements Operating system Windows10 Target platform x64 Minimum Twin CAT version Twin CAT 3.1 Build 4026 Required Twin CAT setup level Twin CAT 3 XAE Required Twin CAT license TC1200 Twin CAT Package Manager: Installation (Twin CAT 3.1 Build 4026) Detailed instructions on installing products can be found in the chapter Installing workloads in the Twin CAT 3.1 Build 4026 installation instructions. Install the following workload to be able to use the product: Twin CAT Package Manager UI: TF3820 | Twin CAT 3 Machine Learning Server Twin CAT Package Manager CLI: tcpkg install TF3820. Machine Learning Server. XAR TF3820 Version: 1.1.0 11
## Page 12

Installation Twin CAT Package Manager UI: TF3830 | Twin CAT 3 Machine Learning Server Client Twin CAT Package Manager CLI: tcpkg install TF3830. Machine Learning Server Client. XAE Twin CAT Package Manager UI: TF38xx | Twin CAT 3 Machine Learning Model Manager Twin CAT Package Manager CLI: tcpkg install TF38xx. Machine Learning Model Manager. XAE Notes on compatibility between client and server Workload version 3.3.1 creates a versioned communication interface between client and server. • The Twin CAT Machine Learning Server from version 3.3.1 is fully backwards compatible with all versions of the Twin CAT Machine Learning Server Client. • The Twin CAT Machine Learning Server Client from version 3.3.1 is not backwards compatible with Twin CAT Machine Learning Server versions older than or equal to 3.2.x. Installation on systems with TF3800 or TF3810 version 3.2.6 and later Potential conflict with old installations There may be conflicts regarding the packages for the Twin CAT Machine Learning Model Manager. This conflict is quickly resolved by uninstalling. If you have installed a version of the workload TF3800. Machine Learning Inference Engine. XAE or TF3810. Neural Network Inference Engine. XAE in version 3.2.6 or lower, you should first uninstall these workloads: tcpkg uninstall TF3800. Machine Learning Inference Engine. XAE --include-dependencies tcpkg uninstall TF3810. Neural Network Inference Engine. XAE --include-dependencies Only then should you install the workloads in version 3.2.10 or higher. 3.1 Licensing The TF3820 Twin CAT Machine Learning Server license is requested by the server component (Tc MLServer service). The license must be stored on the IPC on which the server process is executed. The TF3820 license includes the TF3830 license, so that local communication between the function block and server is possible without additional licenses. 12 Version: 1.1.0 TF3820
## Page 13

Installation The TF3830 Twin CAT Machine Learning Server Client license is queried by the function block of the PLC library Tc3_Ml Server. The license is required on runtime systems that access the Twin CAT Machine Learning Server remotely. No separate TF3830 license is required for local communication between function block and server. The Twin CAT 3 function can be activated as a full version or as a 7-day test version. Both license types can be activated via the Twin CAT 3 development environment (XAE). Cyclic license query of the Twin CAT Machine Learning Server If no or only a 7-day test license is available for the Twin CAT Machine Learning Server, a license query is performed cyclically every 10 seconds. Licensing the full version of a Twin CAT 3 Function A description of the procedure to license a full version can be found in the Beckhoff Information System in the documentation "Twin CAT 3 Licensing". Licensing the 7-day test version of a Twin CAT 3 Function A 7-day test version cannot be enabled for a Twin CAT 3 license dongle. 1. Start the Twin CAT 3 development environment (XAE). 2. Open an existing Twin CAT 3 project or create a new project. 3. If you want to activate the license for a remote device, set the desired target system. To do this, select the target system from the Choose Target System drop-down list in the toolbar. ð The licensing settings always refer to the selected target system. When the project is activated on the target system, the corresponding Twin CAT 3 licenses are automatically copied to this system. 4. In the Solution Explorer, double-click License in the SYSTEM subtree. ð The Twin CAT 3 license manager opens. 5. Open the Manage Licenses tab. 6. In the Add License column, activate the check box for the license you want to add to your project (in this case "TF3820 TC3 Machine Learning Server" and/or "TF3830 TC3 Machine Learning Server Client"). Please note that TF3820 includes the license TF3830 for the local system. If a TF3820 is on the TF3820 Version: 1.1.0 13
## Page 14

Installation local system, you do not need the TF3830 for the client. TF3830 is only required on remote clients. 7. Open the Order Information (Runtime) tab. ð In the tabular overview of licenses, the previously selected license is displayed with the status “missing”. 8. Click 7 Days Trial License... to activate the 7 days trial license. ð A dialog box opens, prompting you to enter the security code displayed in the dialog. 9. Enter the code exactly as it is displayed and confirm the entry. 10. Confirm the subsequent dialog, which indicates the successful activation. ð In the tabular overview of licenses, the license status now indicates the expiry date of the license. 11. Restart the Twin CAT system. ð The 7-day trial version is enabled. 14 Version: 1.1.0 TF3820
## Page 15

Installation 3.2 System configuration for GPU operation If you want to run AI models on a GPU in an accelerated manner, the following requirements and configurations must be observed. Requirements Use a Beckhoff image for IPCs with an ex factory installed GPU to ensure that all necessary requirements for the operation of the Twin CAT Machine Learning Server are already fulfilled. If you are operating a third-party PC, your own image or a Beckhoff IPC with a third-party GPU, please refer to the section Setting up an NVIDIA graphics card [} 15]. Configuration of the GPU operation mode You can put NVIDIA® GPUs into so-called TCC mode and thus reserve all of the GPU's computing resources for computing tasks such as AI model inference. What is the TCC mode? The NVIDIA® TCC mode (Tesla Compute Cluster) is a special operation mode for NVIDIA® GPUs in which the graphics output is disabled in order to optimize the GPU exclusively for computing tasks. Note: Many CPUs are equipped with an integrated GPU (i GPU) so that a connected monitor can still be used despite TCC mode being enabled. This applies, for example, to the IPCs of the C6043 series. Please note that TCC mode is not available for all GPUs. You can find information on the compatibility of your GPU directly from NVIDIA®. All GPUs offered by Beckhoff support the TCC mode. What are the advantages of the TCC mode? • The entire memory and all computing units are available for AI calculations • No loss of time due to competing tasks such as rendering or display interrupts. • Improved response times and reduced jitter • More usable v RAM (especially important for large models or several models on one GPU) How do I set the TCC mode? Open the Windows Command Prompt with administrator rights. Enable the TCC mode with the command nvidia-smi -g <GPU-Index> -dm 1 -g stands for the GPU ID (e.g. 0, 1, 2 …) -dm 1 means switch on TCC mode Examples Enable TCC mode for the first GPU nvidia-smi -g 0 -dm 1 Enable TCC mode for all compatible GPUs nvidia-smi -dm 1 A restart of the system is recommended. 3.2.1 Setting up an NVIDIA graphics card If no Beckhoff hardware with the associated Beckhoff image is used for the GPU-accelerated AI model execution, the customer is independently responsible for creating the necessary framework conditions on his system for operating the graphics card. If you are using a Beckhoff IPC with GPU and associated Beckhoff image, you can skip this section. TF3820 Version: 1.1.0 15
## Page 16

Installation System requirements System requirements are specific to NVIDIA components. The software versions listed below should not be newer than the specified versions (e.g. do not install CUDA 12.6), otherwise incompatibilities may occur. NVIDIA drivers • Normal NVIDIA GPU: Version 560.67 • Quadro/RTX GPU: Version 522.86 CUDA (Compute Unified Device Architecture) • CUDA Version12.5.1 cu DNN • cu DNN Version 9.2.1.18 (as zip file, do not use as Windows Installer) • Installation instructions can be found at NVIDIA®: Tarball Installation. • It is necessary to add the path to the cu DNN libraries to the PATH variable of the system, not that of the user. In general, the above measures meet the following system requirements: • Availability of the cuda.dll library • Availability of the nvml.dll library Runtime behavior When using third-party GPUs, considerable runtime fluctuations may occur during model execution, depending on the specific GPU and its settings. The Twin CAT Machine Learning Server informs about possible challenges of the used graphics card in the log file [} 52] immediately after starting the server. 16 Version: 1.1.0 TF3820
## Page 17

Quickstart 4 Quickstart Create or download ONNX file If you do not have your own ONNX to hand for an initial test, you can use the ONNX Model Zoo on Git Hub for tests, for example. The Res Net50 from the ONNX Model Zoo is used as an example in the following. Netron can be used to easily inspect whether the requirements [} 31] for execution with the Twin CAT Machine Learning Server are met. The Input Nodes are not dynamic and the ONNX Opset used is also supported. Preparing ONNX file with Twin CAT Machine Learning Model Manager Open Twin CAT XAE and navigate to Twin CAT > Machine Learning > Machine Learning Model Manager [} 27]. Load the downloaded ONNX with "Select files" and then select "Convert files". The ONNX and the associated JSON and Plc Open Xml created are now displayed in the target path. Select "Open target path" to open the File Explorer on this path. TF3820 Version: 1.1.0 17
## Page 18

Quickstart Making files available on the target system In this Quickstart, it is assumed that the Twin CAT Machine Learning Server is operated on the same device as the PLC. Accordingly, the model files (res2net50_48w_2s_Opset18.onnx and res2net50_48w_2s_Opset18.json) are stored on the target device under the path C:\models. Further information on this step can be found here: Make model description files available on the Server Device [} 22]. Writing source code It starts with an empty PLC project. First import the created res2net50_48w_2s_Opset18_plcopen.xml by right-clicking on the DUTs folder and selecting "Import PLCopen XML". Also add the PLC library Tc3_Ml Server under References. In the minimal sample, the code consists of two steps. First, a session is created on the Twin CAT Machine Leanring Server and then the inference of the loaded model is executed. Declaration st Model Input : ST_res2net50_48w_2s_Opset18Input; st Model Output : ST_res2net50_48w_2s_Opset18Output; fb Ml Svr : FB_Ml Svr Prediction; b Configured : BOOL := FALSE; b Error : BOOL := FALSE; s Success : T_Max String; n Inference Count : UDINT := 0; 18 Version: 1.1.0 TF3820
## Page 19

Quickstart Code IF NOT b Configured AND NOT b Error THEN fb Ml Svr.st Prediction Parameter.s Ml Model File Path := 'C:\models\res2net50_48w_2s_Opset18.json'; fb Ml Svr.st Prediction Parameter.s Ml Svr Net Id := '127.0.0.1.1.1'; fb Ml SVr.st Prediction Parameter.e Execution Provider := E_Execution Provider. CPU; IF fb Ml Svr. Configure(n Timeout := 10000, n Priority:=0) THEN IF fb Ml Svr.n Error Code <> 0 THEN b Error := TRUE; ELSE b Configured := TRUE; END_IF END_IF END_IF IF b Configured AND NOT b Error THEN IF fb Ml Svr. Predict( p Data In := ADR(st Model Input), n Data In Size := SIZEOF(st Model Input), p Data Out := ADR(st Model Output), n Data Out Size := SIZEOF(st Model Output), n Timeout := 1000, n Priority := 0) THEN IF fb Ml Svr.n Error Code <> 0 THEN b Error := TRUE; ELSE s Success := 'You made your first inference'; n Inference Count := n Inference Count + 1; // use st Model Output here END_IF END_IF END_IF Activating the configuration Activate your configuration and start the PLC. The result is shown below. The counter value n Inference Count increases and the variable s Success displays a success message. TF3820 Version: 1.1.0 19
## Page 20

Technical introduction 5 Technical introduction 5.1 Workflow The workflow consists of the following steps: • Extract PLC interface description [} 20] from the ONNX file (with the Twin CAT Machine Learning Model Manager [} 27]). ◦ A Plc Open Xml is created from the ONNX file, which provides the input and output data type of the model for the PLC. ◦ A JSON file is generated which contains metadata about the model. Metadata is Twin CAT-specific on the one hand and application-specific from the user on the other. • Provide JSON file and ONNX file on the target system [} 22]. • Import Plc Open Xml in Twin CAT Engineering [} 20] and incorporate it into the PLC program using FB_Ml Svr Prediction [} 36]. • Configure the Twin CAT Machine Learning Server from the PLC [} 23] and load the AI model. • Call the loaded AI model asynchronously to the PLC task cycle [} 24]. • Exchange/update an AI model at runtime of the machine [} 26]. 5.1.1 Preparing ONNX for use with Twin CAT Machine Learning Server Generate interface descriptions for the PLC To be able to use an ONNX with the FB_Ml Svr Prediction [} 36] in the Twin CAT PLC, interface information is required. These are generated by the Twin CAT Machine Learning Model Manager [} 27]. Information on the supported ONNX Opset and restrictions can be found here: ONNX Support [} 31]. JSON file The JSON file contains metadata required by the function block FB_Ml Svr Prediction. Users can also add their own metadata to the JSON file via the Custom Attributes [} 45]. The JSON file is loaded by the function block, see Configure method [} 37]. Both the JSON file and the ONNX file must be available for loading on the runtime PC, see Make model description files available on the Server Device [} 22]. 20 Version: 1.1.0 TF3820
## Page 21

Technical introduction Plc Open Xml The Plc Open Xml contains the PLC type description of the input and output nodes. The automatically generated input/output structures (DUTs) reflect the input/output definition of the ONNX file provided by you. Therefore, make sure that you use meaningful names for the input and output nodes in your ONNX. The use of these created data types is strictly required. Sample of name generation in the Plc Open Xml: File name: lemon_model.onnx Name Input node: input.1 Name Output node: 367 Two DUTs of type STRUCT are generated with the names ST_lemon_model Input and ST_lemon_model Output. Each STRUCT has a header (do not change!) and a data area. The data area elements are named according to the input and output nodes, in the above case in_input1 and out_367. Characters not permitted in the PLC are automatically removed. If an AI model has several input or output nodes, these are each displayed as an element of the STRUCT. Sample of the generated input data types TYPE ST_lemon_model Input : STRUCT {attribute 'hide'} _header_DO_NOT_CHANGE : ARRAY[0..63] OF SINT := [-1,-112,120,86,52,18,-1,-1,1, -1,0,0,-1,-1,-112,-112,104,-72,-97,-115,-17,4,98,-85,-50,-67,-12,-50,-6,33,-10,-11,88,80,121,12,119, -56,-24,8,110,-32,-69,113,-21,3,102,-60,64,48,9,0,0,0,0,0,0,0,0,0,0,0,0,0]; in_input1 : ARRAY[0..0,0..2,0..223,0..223] OF REAL; END_STRUCT END_TYPE The header represents a hash of the shape of the input tensors (or the output tensors for the output data type) of the AI model. This ensures that the data type is used together with the correct ONNX. This also means that models with identical input and output shapes and lexicographical order carry the same hashes and therefore the data types are interoperable. This is especially important for model updates [} 26]. TF3820 Version: 1.1.0 21
## Page 22

Technical introduction The byte array of the header must not be changed! The names of the elements of the STRUCT can be adjusted if required, but not their order in the STRUCT. Import of a Plc Open Xml in Twin CAT 3 The generated Plc Open Xml can be transferred to the PLC project in the PLC by right-clicking on a folder (e.g. DUTs) via the "Import PLCopen XML" field. NOTICE Verified signature: Operation of the Machine Learning Server only via generated input and output model types Note that the use of the input/output model types defined in the supplied Plc Open file is mandatory. The types have a signature verified by the Tc Ml Server to ensure secure inference operations. 5.1.2 Make model description files available on the Server Device In order to be able to load models, they must be made known to the Twin CAT Machine Learning Server. This means that the server must know the location of the created JSON and ONNX file on the file system in order to successfully load the AI model. This announcement is made using the Configure [} 37] method. The method is passed the full path to the JSON file, which contains the interface description. The associated ONNX file must be stored in the same folder path. A hash is always used to validate the connection between JSON and ONNX. The ONNX file is required on the device on which the Twin CAT Machine Learning Server is installed. The JSON file, in turn, is required on the client device. The storage path itself is arbitrary; but JSON and ONNX must have the same path. Variant 1 - Client and server are installed on the same IPC: The JSON and ONNX file must be stored on the IPC in any path. Variant 2 - Client and server are installed on different IPCs: The ONNX file can be stored directly on the server. The JSON file must then be stored on the client device under the same path. 22 Version: 1.1.0 TF3820
## Page 23

Technical introduction Variant 3 - Client and server are installed on different IPCs: The model files (ONNX and JSON) can alternatively both be stored on the client. When the Configure method is called, the path on the Server Device is checked first. If the ONNX file is not found there, the path on the Client Device is checked. If the JSON and ONNX are found on the Client Device, the ONNX file is transferred to the server device via ADS and then loaded from the server. This variant is time-consuming and is only carried out once for this model. The model is then loaded directly from the Server Device. The ADS router memory must be large enough to be able to send the model via ADS. 5.1.3 Configuring the server from the PLC client Calling the method configure [} 37] instantiates a session for the respective instance of the function block FB_Ml Svr Prediction in the Tc Ml Server. The configuration defined in the FB member st Prediction Parameter [} 41] is used for instantiation. As a rule, each instance of FB_Ml Svr Prediction is assigned its own session on the server. However, sessions can also be used together via the parameter b Exclusive Session. During the configuration call, the following is defined in particular: • Where is the Twin CAT Machine Learning Server? ◦ Is specified via the AMS Net Id of the Server Device. The default value is "local". TF3820 Version: 1.1.0 23
## Page 24

Technical introduction • Which AI model should be loaded? ◦ Is specified via the path to the corresponding JSON file. • On which hardware should the AI model be executed? ◦ Is defined via the Execution Provider (E_Execution Provider [} 41]) and optionally the Device Id of the GPU. Each session that is opened allocates resources on the Server Device. The number of parallel sessions is not limited on the software side, but is restricted solely by the available hardware resources. If there is not enough memory (RAM or v RAM) available to open another session, the Configure command will fail. The deconfigure [} 38] method can be used to close a session and thus release the resources. If a client does not send a request to the server for a defined period of time, the so-called session time-out period, the server assumes that the client is no longer active. The session is automatically closed when the configured session timeout [} 41] is reached. Sample Declaration fb Ml Svr : FB_Ml Svr Prediction(); Code // configure session paramaters fb Ml Svr.st Prediction Parameter.s Ml Model File Path := 'C:\mdl\lemon_model.json'; fb Ml Svr.st Prediction Parameter.s Ml Svr Net Id := '127.0.0.1.1.1'; fb Ml SVr.st Prediction Parameter.e Execution Provider := E_Execution Provider. CPU; // Submit configuration request to the Tc Ml Server // Provide a generous n Timeout, as the configuration can take a substantial amount of time IF fb Ml Svr. Configure(n Timeout := 1000, n Priority:=0) THEN // check for error // change state END_IF; 5.1.4 Execute AI model The execution of an AI model with the FB_Ml Svr Prediction [} 36] is triggered via the Predict [} 38] method (or Predict Batched [} 39] in the case of a batched call), which is asynchronous to the PLC task cycle. The input data type and the output data type (see Plc Open Xml in the Machine Learning Model Manager [} 27] section), as well as a timeout and a priority are passed to the method. Send inference order: When the method is called, the input data area is sent to the server via ADS. The copying process required for this is carried out synchronously in the task cycle. Please note that larger amounts of data require more time. When transferring large amounts of data, such as large image data, the PLC task cycle time must be configured to prevent cycle timeouts. Edit inference order: The inference order is then accepted by the Twin CAT Machine Learning Server. If there are multiple requests that cannot be processed at the same time, a queue is created, with higher priorities moving up in the queue. CPU-based inference orders are always processed sequentially, i.e. the queue is particularly relevant here. GPU-based inferences, on the other hand, can be processed in parallel. Each FB instance can only ever have one outstanding request on the server, i.e. the maximum number of requests on the server is the number of active clients. Request inference result: 24 Version: 1.1.0 TF3820
## Page 25

Technical introduction It is important to note that the processing of the asynchronous request can only be monitored with the temporal granularity of the PLC task cycle time. It is therefore important that applications with a low delay budget have the lowest possible cycle times and correspondingly high sampling rates in order to minimize delays caused by time resolution. This is particularly important when interacting with other, computationally intensive and synchronously executed algorithms, e.g. when pre- or post-processing data. Sample Predict() Declaration st Model Input : ST_Lemon Model Input; // DUT from Plc Open Xml produced with TC ML Model Manager st Model Output : ST_Lemon Model Output; // DUT from Plc Open Xml produced with TC ML Model Manager fb Ml Svr : FB_Ml Svr Prediction(); Code: // Submission of an inference request at the Tc Ml Server // and subsequent postprocessing of the inference result // Submission of the asynchronous inference request to the Tc Ml Server IF fb Ml Svr. Predict( p Data In := ADR(st Model Input), n Data In Size := SIZEOF(ST_Lemon Model Input), p Data Out := ADR(st Model Output), n Data Out Size := SIZEOF(ST_Lemon Model Output), n Timeout := 100, n Priority := 0) THEN IF fb Ml Svr.n Error Code <> 0 AND NOT fb Ml Svr.b Configured THEN // If n Error Code -1 is encountered, increase n Timeout e State := E_State.e Error; ELSE // Postprocessing of the inference results END_IF END_IF Sample Predict Batched() Declaration st Model Input Batch : ARRAY[0..3] OF ST_Lemon Model Input; st Model Output Batch : ARRAY[0..3] OF ST_Lemon Model Output; Code TF3820 Version: 1.1.0 25
## Page 26

Technical introduction IF fb Ml Svr. Predict Batched( p Data In := ADR(st Model Input Batch), n Data In Size := SIZEOF(ST_Lemon Model Input), n Batch Size := 4, p Data Out := ADR(st Model Output Batch), n Data Out Size := SIZEOF(ST_Lemon Model Output), n Timeout := 100, n Priority := 0) THEN In the sample calls, the timeout for the inference request is set to 100 cycles in each case. In the figure above, the result is available after 3 cycles (top) or 2 cycles (bottom). The jitter that may be experienced can be queried via fb Ml Svr.n Max Inference Duration. This shows the maximum number of PLC cycles that were required to execute an inference. Based on this value, the timeout value can usually be interpreted well. 5.1.5 Updating the AI model AI models can be exchanged at runtime. The following describes two cases and the steps to follow. Case 1: Model update without changing the model interface Definition of the case: In this case, the input and output interface to the AI model remains identical when the model is replaced. To do this, the input and output nodes must remain unchanged in their sequence (if there are several nodes) and in their shape. It is recommended not to change the entire model architecture when updating the model, i.e. to carry out transfer training/fine-tuning on the existing AI model. As a result, the interfaces to the model do not change and the runtime behavior remains the same. A model update can be carried out without a compile process and without a Twin CAT stop. Steps to update the model: • Create JSON and Plc Open Xml with the Twin CAT Machine Learning Model Manager. • Make JSON and ONNX available on the relevant systems. If the full path has changed, make the new path known in a variable via ADS, for example. • The hash of the input and output data types does not change. This means that no new Plc Open Xml needs to be read. The new Plc Open Xml remains unused. • Change the state in the running PLC, e.g. set a corresponding variable via ADS, and call Deconfigure [} 38]() to close the current session. • Call Configure to load the new JSON. During the time from Deconfigure to the completion of the Configure method, no inference calls can be sent to the server with this function block. Case 2: Model update with model interface change Definition of the case: In this case, the input and output interface to the AI model changes when the model is swapped. As a result, the input and output data types in the PLC no longer match the model. As a rule, several places in the source code have to be changed. It is recommended to revise the source code in Twin CAT XAE, retest the project and only then load it onto the machine. Please note that the runtime behavior of the AI model may also have changed. In this case, a model update is connected with a Twin CAT stop when the new, modified Twin CAT project is loaded. 26 Version: 1.1.0 TF3820
## Page 27

Technical introduction 5.2 Twin CAT Machine Learning Model Manager The Twin CAT Machine Learning Model Manager is used to manage the ONNX model files and prepare them for use with Twin CAT. The functions of the Twin CAT Machine Learning Model Manager are summarized: • Creation of a metadata file (JSON file). • Creation of a Plc Open Xml, which describes a PLC data type description of the model input and output. • Optional: Inserting application-specific metadata (Custom attributes). A user can add the Custom attributes to the JSON. The attributes can then be read out in the PLC at runtime using methods at FB_Ml Svr Prediction [} 36]. The Twin CAT Machine Learning Model Manager has three different interfaces: 1. A graphical user interface (Visual Studio plugin) 2. A Python interface (Python package) 3. A Command Line Interface (CLI) These are described in more detail below. 5.2.1 Graphical user interface The Twin CAT 3 Machine Learning Model Manager is the central UI for editing ONNX files. The tool is integrated in Visual Studio and can be opened via the menu bar under Twin CAT > Machine Learning > Machine Learning Model Manager. Required Visual Studio version The graphic interface of the Twin CAT 3 Machine Learning Model Manager is compatible with Visual Studio 2017, 2019, 2022 and the Tc Xae Shell. Creating the JSON and Plc Open Xml 1. Open the Convert Tool tab. ð Click Select files to open the file browser. 2. Select ONNX files (multi-select possible by Ctrl-clicking). ð Selected ONNX files are listed on the left-hand side with their path and file name. 3. If required, you can select files and remove them from the list again by clicking the Remove selected from list button. 4. Click on Convert files to create the required JSON and Plc Open Xml. ð The files are stored in the converted file path. The default path is <Twin CATPath>\Functions\TF38xx- Machine-Learning\Convert Tool Files. 5. Click on Open target path to open the converted file path in the file browser. ð The path can be changed with Select target path. The change is retained even after restarting the PC. TF3820 Version: 1.1.0 27
## Page 28

Technical introduction The tool described here is also the central tool for managing ONNX files for the Twin CAT Machine Learning Inference Engine and Twin CAT Neural Network Inference Engine. For this reason, description files for the optional execution of the AI model are also generated with these products if the ONNX is compatible. The Convert to *.xml drop-down menu is relevant for these products. This is not relevant for the Twin CAT Machine Learning Server. A pop-up window indicates the Twin CAT product with which the ONNX is compatible for each converted ONNX. Creating Custom attributes A JSON can be selected via Select File and then edited. After editing, the original file is overwritten using Save changes. 28 Version: 1.1.0 TF3820
## Page 29

Technical introduction The Custom attributes are edited using the buttons: • Add attribute entry: Adds an attribute to the selected Tree item. The Tree item must be selected in the left-hand list. ◦ If an attribute is created, then the name, the data type and the value or values respectively must be specified. ◦ The attributes are deleted by selecting the attribute and pressing the Delete button. • Add tree entry: Adds a Tree item under the selected Tree item (as a Sub Tree Item). • Rename tree entry: The selected Tree item can be renamed. • Remove tree entry: The selected Tree item incl. the Sub Tree Items is deleted. 5.2.2 Python interface Installation of the Python package The Python package is stored as a whl file in the folder <Twin Cat Install Dir>\Functions\TF38xx-Machine- Learning\Utilities\Model Manager API\Python Package. The package is compatible with: • Windows 10 • Windows 11 • Debian 12 • Ubuntu 24.04 The Clang C++ libraries are required for use under Linux® distributions. Install libc++1-XX with XX>=14 via the Advanced Package Tool (apt). >># apt install libc++1-19 -y To install the package, use : pip install <Twin Cat Install Dir>\Functions\TF38xx-Machine- Learning\Utilities\Model Manager API\Python Package\<whl-file-name> TF3820 Version: 1.1.0 29
## Page 30

Technical introduction The Twin Cat Install Dir folder may contain different versions of the package (if you have installed a new setup on top of an old Twin CAT Machine Learning Setup). Make sure you always use the current version. Using the API The package is loaded with import beckhoff.toolbox as tb Creating a JSON and Plc Open Xml from an ONNX file tb.onnxprep("C:\\Path To\\my ONNX.onnx") Display of model information tb.info("C:\\Path To\\my ONNX.json") Add Custom Attributes new_ca = { 'n ID' : -34234, 'b Tested' : True, 'f Num' : 324.3E-12, 'Another Tree Item' : { 'f Pi' : 3.134 12, 'b False Flag' : False }} tb.modify_ca("C:\\Path To\\my ONNX.json","C:\\Path To\\my ONNX_ca.json", new_ca) Add Model Description (name, version, author, etc. of a model) model_description = { "new_version" : "2.3.1.0", "new_name" : "Current Pre Control Axis42", "new_desc":"This is the most awesome model to control Axis42", "new_author":"Max", "new_tags":"awesome, ingenious, astounding", } tb.modify_md("C:\\Path To\\my ONNX.json", "C:\\Path To\\my ONNX_md.json", **model_description) 5.2.3 Command Line Interface The Model Manager can also be used via the Command Prompt. The mllib_toolbox.exe is available for this purpose. The Executable is located in <Twin Cat Install Dir>\Functions\TF38xx-Machine- Learning\Utilities\Model Manager API. Help is displayed by running the exe without arguments. 30 Version: 1.1.0 TF3820
## Page 31

Technical introduction Using the API Creating a JSON and Plc Open Xml from an ONNX file mllib_toolbox.exe onnxprep C:\Path To\mymodel.onnx Display of model information mllib_toolbox.exe info C:\Path To\mymodel.json The creation of Custom attributes is not supported in the CLI. 5.3 ONNX Support Supported ONNX opset version The Twin CAT Machine Learning Server, more precisely the Tc Ml Server service, supports ONNX Opset version 21. Backward compatibility with more recent ONNX Opset versions is generally provided by ONNX. The Opset version used is normally listed in the ONNX file under imports. In the image below, visualized with Netron, ONNX Opset version 18 as an example. Restrictions on supported ONNX properties No dynamic input or output shapes Dynamic input or output shapes are not supported. Only the leading batchsize parameter may be a dynamic value (see next section). Permitted are for example: float32[244, 244, 1] float32[1, 3, 244, 244] float32[5, 244, 244, 3] Not permitted are: float32[244, 244, ?] float32[244, height, 3] float32[?, 244, 244, ?] float32[1, 244, 244, unknown] You can quickly fix the shape of ONNX nodes, e.g. using the onnxruntime package in Python, see Make dynamic input shape fixed | onnxruntime. The shape... float32[-1, 3, ?, ?] becomes with... python -m onnxruntime.tools.make_dynamic_shape_fixed --input_name x --input_shape 1,3,960,960 model.onnx model.fixed.onnx float32[1, 3, 960, 960] Batchsize must be leading The Twin CAT Machine Learning Server only supports models with Batchsize as the leading parameter of the input node. The batchsize parameter may be dynamic (unlike all other parameters). Permitted are for example: float32[batch, 3, 244, 244] TF3820 Version: 1.1.0 31
## Page 32

Technical introduction float32[?, 3, 244, 244] float32[unknown, 244, 244, 3] The Predict Batched() [} 39] method can only be used if the model contains a dynamic batchsize as the leading parameter. ONNX file size Currently the size limit for a loadable ONNX file is 2 GB. Contact Beckhoff (see Support and Service [} 53]) if this limit is a challenge for your application. 5.4 Tc Ml Server Service The installation of TF3820 can be verified by checking the presence of the Tc Ml Server.exe service. Note that the Tc Ml Server service offers a delayed autonomous start on restart. Please note the delayed start time here. The Tc Ml Server allocates ADS port 19900 on the system at startup. System requirements Apart from a 64-bit Windows and a TC1000 Twin CAT 3 ADS installation (workload TC1000. ADS. XAR), the Tc Ml Server does not formally place any further requirements on the system apart from a hard disk requirement of approx. 500 MB. However, the highest possible number of UM cores is recommended for high-performance operation of the Tc Ml Server. License query When starting, the Tc Ml Server checks whether the TF3820 license is available. If the license is not initially available, the license status is queried every ten seconds. It may therefore take a short time for a license to become effective. A corresponding error code is issued for requests if the Tc Ml Server is not correctly licensed. Installation path and log files: C:\Program Data\Beckhoff\Twin CAT\Functions\TF38xx-Machine-Learning\Tc Ml Server In support cases, log files with any recorded messages are available in the logs folder. 32 Version: 1.1.0 TF3820
## Page 33

Technical introduction The logs folder has the following structure: After each restart, the Tc Ml Server creates a new directory whose name is made up of the creation date and time. The logs are stored in JSON file format in the directory and regularly written by the Tc Ml Server, if available. In the event of an error, there may therefore be several JSON files whose names specify an interval of the errors or warnings they contain. 5.4.1 Execution Provider The following Execution Providers are currently available: • CPU • CUDA The Execution Provider [} 41] is transferred with the Configure [} 37] method of the FB_Ml Svr Prediction [} 36]. CPU The loaded AI model is executed on the CPU resources of the IPC. If a Twin CAT runtime is active on the same device, only the CPU resources that are not used by Twin CAT can be used (isolated cores are only used by Twin CAT, shared cores can only be used for a limited time). The operating system takes over the parallelization of the calculation on all available threads. If several clients create a session with Execution Provider CPU on a server, the inference requests are processed one after the other. Please note the Priority parameter when calling Predict. CUDA The loaded AI model is executed on the GPU resources of the IPC. Several sessions can be run in parallel on one GPU if the GPU resources are sufficient. The n Device Id field allows you to distinguish between multiple graphics cards that may be installed. For computers with a maximum of one graphics card, the value can be left at the default value, otherwise the field corresponds to the CUDA compute index of the graphics cards. If a computer with several graphics cards is used, the following system environment variable should be set: CUDA_DEVICE_ORDER='PCI_BUS_ID' Furthermore, the Tc Ml Server allows sharing of inference resources between different FBs that provide the same specification for their inference session. The use of a shared inference engine (b Exclusive Session = FALSE) can occur in cases where a bottleneck - for example in memory on graphics cards - would otherwise be expected. For stateful models (e.g. recurrent models), however, such a configuration should be avoided. 5.5 Performance, latencies and configuration Latency times and their measurability As described under Execute AI model [} 24], the total execution time or total latency of an AI model on the Twin CAT Machine Learning Server consists of different time components. TF3820 Version: 1.1.0 33
## Page 34

Technical introduction The "model execution time" shown in the figure above can be divided into three parts: • ADS communication between PLC and server • Server overhead (receiving data, processing and forwarding to the execution unit) • Model inference time In addition, there is the time delay caused by the cyclic code execution of the PLC, which is indicated in the figure above as "waiting for next cycle". The FB_Ml Svr Prediction [} 36] has a structure st Latency Monitor of type ST_Latency Monitor [} 42] as output. This includes the elements: • n Inference Latency: Describes the pure model inference runtime. • n Server Latency: Describes the server overhead. • n Communication Latency: ADS communication + "waiting for next cycle" The data is determined in milliseconds and is measured and updated with each inference. Configuration options and latency effects Reduction of the model inference runtime You can reduce this proportion by providing more computing resources for the Tc Ml Server. If the Execution Provider is set to CPU, the Tc Ml Server uses the CPU computing resources available in user mode for inference. Please note that cores isolated from Twin CAT are not available to the user mode and that shared cores with Twin CAT are only partially available to the user mode. The Tc Ml Server also shares resources with other User Mode applications, such as the Twin CAT HMI Server, the Twin CAT Scope Server and others. Please also observe the instructions when using CPUs with hybrid architecture [} 35]. If you have the option of running the calculation on a GPU, you can set it to Tesla Compute Cluster (TCC) mode (for Windows operating systems). NVIDIA® TCC mode is an operation mode for NVIDIA® GPUs that disables the graphics output and optimizes the GPU exclusively for computing tasks. This means that the GPU is only available for dedicated computing operations. The Tc Ml Server can then execute inferences on the GPU without competing processes. Details on the TCC mode can be found here: System configuration for GPU operation [} 15]. Reduction of server overhead 34 Version: 1.1.0 TF3820
## Page 35

Technical introduction The part of server overhead is usually negligible. The other parts should rather be minimized. The server uses the CPU computing capacity of the user mode for the necessary actions. Reduction of ADS communication latency To minimize ADS communication latency, it is recommended to install the client and the server on the same IPC. This means that ADS communication is handled locally on the device using the main memory. Latency increases considerably with remote ADS communication via the network. An important factor is the communicated data size. In the case of image processing models, the image size in particular must be considered as an input to the model. The Resize operation can be a decisive factor here. You can execute this in the PLC before the Predict call or insert it as an ONNX operator in the ONNX. The Resize operator, executed on the GPU with the Twin CAT Machine Learning Server, is executed faster than on the CPU with the Twin CAT Vision function, but the difference in ADS communication can be significant. Reduce the waiting time until the next cycle You can only shorten this latency by reducing the task cycle time. A short cycle time means that you check at short intervals whether the result is available from the server. This may compete with other states of the PLC program that require longer runtimes. It may be useful to run the Predict call in another, faster task. Configuration of the Tc Ml Server on CPUs with hybrid architecture On hybrid architectures with both performance and efficiency cores, the Tc Ml Server only works on one core type to minimize latency jitter. By default, the Tc Ml Server only works on performance cores that are available to the user mode of the IPC. The preferred core type will be changed in the server configuration file. This can be found under C: \Program Data\Beckhoff\Twin CAT\Functions\TF38xx-Machine- Learning\Tc Ml Server\config\server\server.json. The behavior of the Tc Ml Server can be changed by manipulating the entry e_core_preference_in_hybrid_architecture. Only use P cores (default): "e_core_preference_in_hybrid_architecture": false Only use E cores: "e_core_preference_in_hybrid_architecture": true Changes to the server configuration only take effect after a server restart. If none of the configured core types are available, the Tc Ml Server falls back to the available ones and acknowledges the fact with a warning in the log file. TF3820 Version: 1.1.0 35
## Page 36

API 6 API 6.1 Function blocks 6.1.1 FB_Ml Svr Prediction FB_Ml Svr Prediction is a Tc Ml Server client that provides asynchronous and optionally hardware- accelerated AI model inference for the PLC. The function block is located in the PLC library Tc3_Ml Server. Syntax Declaration: fb Ml Svr : FB_Ml Svr Prediction; Definition: FUNCTION_BLOCK FB_Ml Svr Prediction VAR_INPUT st Prediction Parameter : ST_Prediction Parameter; END_VAR VAR_OUTPUT b Error : BOOL; n Error Code : HRESULT; b Configured : BOOL; n Max Inference Duration : UDINT; st Latency Monitor : ST_Latency Monitor; END_VAR Inputs Name Type Description st Prediction Paramete ST_Prediction Parameter Configuration structure of the Machine Learning Server session r [} 41] Outputs Name Type Description b Error BOOL TRUE if the currently pending asynchronous request to the Tc MLServer was canceled with an error. n Error Code HRESULT Return code [} 49] for the currently pending asynchronous request to the Tc Ml Server. b Configured BOOL TRUE if the configuration was successful. Indicates whether the function block has a valid session with the Tc Ml Server. If FALSE, "configure" must be called. Please note: b Configured can change to FALSE due to an error during the execution of the request at the Tc Ml Server. n Max Inference Durati UDINT Maximum number of PLC cycles required to execute an on inference. See also Execute AI model [} 24]. st Latency Monitor ST_Latency Monitor [} 42] Detailed monitoring of the latency of the last completed inference. 36 Version: 1.1.0 TF3820
## Page 37

API Methods Name Definition location Description Configure [} 37] Local Create a session for the FB instance on the Tc Ml Server according to the configuration defined in st Prediction Parameter [} 41]. Deconfigure [} 38] Local End a session of the function block on the Tc Ml Server. The allocated resources on the server are released again. Get Custom Attribute_ Local Get custom attributes of type ARRAY of the AI model array [} 40] Get Custom Attribute_ Local Get custom attributes of type BOOL of the AI model bool [} 40] Get Custom Attribute_ Local Get custom attributes of type LREAL of the AI model fp64 [} 40] Get Custom Attribute_ Local Get custom attributes of type LINT of AI model int64 [} 41] Get Custom Attribute_ Local Get custom attributes of type STRING of the AI model str [} 41] Predict [} 38] Local Transmission of an asynchronous inference request to the Tc Ml Server Predict Batched Local Transmission of an asynchronous, bundled inference request to the Tc MLServer [} 39] 6.1.1.1 Asynchronous methods The asynchronous methods of FB_Ml Svr Prediction are characterized in their signature by the fact that they accept a timeout parameter and return a bool indicating the execution state of the asynchronous request. The PLC program must therefore wait for the asynchronous methods to finish executing and configure the timeout according to the requirements of the application so that the application can respond appropriately to any delays in the Tc Ml Server. Note that the specified timeout is given in the unit PLC task cycles. It is also important to note that the processing of the asynchronous request can of course only be monitored with the temporal granularity of the PLC task cycle time. It is therefore important that applications with a low delay budget have the lowest possible cycle times and correspondingly high sampling rates in order to minimize delays caused by time resolution. This is particularly important when interacting with other, computationally intensive and synchronously executed algorithms, e.g. when pre- or post-processing data. However, a restriction regarding the minimum PLC task cycle time is imposed by the amount of data that has to be transported from the client to the server. When transferring large amounts of data, such as large image data, the PLC task cycle time must be configured to prevent cycle timeouts. 6.1.1.1.1 Configure() Calling the method configure instantiates a session for the respective instance of the function block FB_Ml Svr Prediction in the Tc Ml Server. The configuration defined in the FB member st Prediction Parameter is used for instantiation. The instantiation of a session can take a considerable amount of time, in particular because several inferences are made to temper the inference engine (model warm-up). This fact should be taken into account when selecting the timeout parameter of the method call. It should also be taken into account that calling the method when configuring a CUDA accelerated session temporarily requires exclusive access to the GPU. The configuration of such a session can therefore interfere considerably with the inference performance of other FBs operating in parallel. TF3820 Version: 1.1.0 37
## Page 38

API After the method indicates the completion of the processing of the asynchronous instantiation call by returning TRUE, the result can be evaluated via the FB members b Error and, in the event of an error, n Error Code. Once an inference session has been successfully instantiated, the FB will set the member b Configured to TRUE. See also Configuring the server from the PLC client [} 23]. Parameter Type Default Description INPUT n Timeout ULINT Number of PLC task cycles before the timeout error is returned. INPUT n Priority UDINT 0 Priority of the request. Bigger means higher priority. OUTPUT Configure BOOL Return value. TRUE as soon as the result of the asynchronous call is available. The result of the call can then be checked using the 'b Error' and 'n Error Code' properties. 6.1.1.1.2 Deconfigure() The Deconfigure method closes the inference session of the FB in the Tc Ml Server. After a successful call to Deconfigure, the FB can set up a new inference session using the Configure method. After a successful configuration of an inference session, all calls to Configure are ignored until a call to Deconfigure has been made. Parameter Type Default Description INPUT n Timeout ULINT Number of PLC task cycles before the timeout error is returned. OUTPUT Deconfigure BOOL Return value. TRUE as soon as the result of the asynchronous call is available. The result of the call can then be checked using the 'b Error' and 'n Error Code' properties. 6.1.1.1.3 Predict() The Predict method submits an asynchronous inference order to the Tc Ml Server. The method expects the provision of input data according to the specification of the ONNX file, see Preparing ONNX for use with Twin CAT Machine Learning Server [} 20]. The provided pointer to the output data must be valid and point to an instance of the output data type according to the created Plc Open Xml. Once the asynchronous inference has been successfully completed, the data in the transferred output memory area is valid and released for further processing. See also Execute AI model [} 24]. Parameter Type Default Description INPUT p Data In PVOID Pointer to the instance of the input data type INPUT n Data In Size UDINT 0 Size of the input data type 38 Version: 1.1.0 TF3820
## Page 39

API Parameter Type Default Description INPUT p Data Out PVOID Pointer to the instance of an output data type INPUT p Data Out Size UDINT Size of the output data type INPUT n Timeout ULINT Number of PLC task cycles before the timeout error is returned. INPUT n Priority UDINT 0 Priority of the request. Bigger means higher priority. OUTPUT Predict BOOL Return value. TRUE as soon as the result of the asynchronous call is available. The result of the call can then be checked using the 'b Error' and 'n Error Code' properties. 6.1.1.1.4 Predict Batched() The Predict Batched method submits an asynchronous batch inference order to the Tc Ml Server. The method expects the provision of an array of the input data type of the model according to the specification of the ONNX file, see Preparing ONNX for use with Twin CAT Machine Learning Server [} 20]. The provided pointer to the output data must be valid and point to an instance of an array of output data types according to the created Plc Open Xml. Once the asynchronous inference has been successfully completed, the data in the transferred output memory area is valid and released for further processing. See also Execute AI model [} 24]. Parameter Type Default Description INPUT p Data In PVOID Pointer to the instance of an array of input data types INPUT n Data In Size UDINT 0 Size of the input data type (size of an element, not of the array) INPUT n Batch Size UINT Size of the batch INPUT p Data Out PVOID Pointer to the instance of an output data type INPUT p Data Out Size UDINT Size of the output data type INPUT n Timeout ULINT Number of PLC task cycles before the timeout error is returned. INPUT n Priority UDINT 0 Priority of the request. Bigger means higher priority. TF3820 Version: 1.1.0 39
## Page 40

API Parameter Type Default Description OUTPUT Predict Batched BOOL Return value. TRUE as soon as the result of the asynchronous call is available. The result of the call can then be checked using the 'b Error' and 'n Error Code' properties. 6.1.1.2 Synchronous methods 6.1.1.2.1 Get Custom Attribute_array Parameter Type Description INPUT s Custom Attribute Name T_Max String Name of the Custom attribute INPUT fmt Attribute Data Type Reference to Data format of the Custom ETc Mll Data Type attribute INPUT p Data Buffer PVOID Destination data buffer into which the user- defined attribute is copied. INPUT n Data Buffer Len UDINT Length of the destination data buffer in bytes INPUT n Array Length Reference To UDINT Number of data type elements (i.e. number of fp32 values) found in the user-defined attribute. INPUT pn Bytes Written Pointer To UDINT Returns the number of bytes written to the destination buffer. OUTPUT Get Custom Attribute_array BOOL TRUE if an error occurred in the method. 6.1.1.2.2 Get Custom Attribute_bool Parameter Type Description INPUT s Custom Attribute Name T_Max String Name of the Custom attribute INPUT n Data Out Reference To BOOL Output value Custom attributes of type Bool OUTPUT Get Custom Attribute_bool BOOL TRUE if an error occurred in the method. 6.1.1.2.3 Get Custom Attribute_fp64 Parameter Type Description INPUT s Custom Attribute Name T_Max String Name of the Custom attribute INPUT n Data Out Reference To LREAL Output value of the Custom attribute fp64 OUTPUT Get Custom Attribute_fp64 BOOL TRUE if an error occurred in the method. 40 Version: 1.1.0 TF3820
## Page 41

API 6.1.1.2.4 Get Custom Attribute_int64 Parameter Type Description INPUT s Custom Attribute Name T_Max String Name of the Custom attribute INPUT n Data Out Reference To LINT Output value of the Custom attribute int64 OUTPUT Get Custom Attribute_int64 BOOL TRUE if an error occurred in the method. 6.1.1.2.5 Get Custom Attribute_str Parameter Type Description INPUT s Custom Attribute Name T_Max String Name of the Custom attribute INPUT n Data Out Reference To Output value of the T_Max String Custom attribute of type String OUTPUT pn String Len Pointer To UDINT (Optional) Actual length of the string attribute OUTPUT Get Custom Attribute_strin BOOL TRUE if an error occurred g in the method. 6.2 Data types 6.2.1 E_Execution Provider Enum describes all supported execution modes of the Tc Ml Server. Name Type Value Description CPU USINT 0 Execution on CPU CUDA USINT 1 Execution on CUDA- capable NVIDIA GPUs 6.2.2 ST_Prediction Parameter Configuration options for an inference session on the Tc Ml Server. Name Type Default Description s Ml Model File Path STRING(255) Fullpath to the created JSON file (see Model Manager [} 27]) e Execution Provider E_Execution Provider Execution Provider. CPU The AI model named under s Ml Model File Path [} 41] is to be executed on this specified hardware. n Device Id UDINT 0 Index of the desired GPU device when using the "CUDA" Execution Provider. The index corresponds to the CUDA Compute Index and is only relevant for IPCs with multiple GPUs. TF3820 Version: 1.1.0 41
## Page 42

API Name Type Default Description b Exclusive Session BOOL TRUE Determines the exclusivity of the inference session that is created for the FB instance on the Tc Ml Server. If TRUE, the Tc Ml Server creates an exclusive session, which is necessary for state- dependent models (e.g. RNNs) in order to avoid interference. If FALSE, the session can be shared with other FB instances that request the same configuration, which can reduce the memory load. n Session Timeout ULINT 72 Duration of inactivity in hours after which the FB session on the Tc Ml Server expires. The server then releases the allocated resources of the relevant client again. s Ml Svr Net Id T_Ams Svr Net Id '127.0.0.1.1.1' AMS Net Id of the device on which the Tc Ml Server service is accessible. Default is ‘local’. 6.2.3 ST_Latency Monitor Latency monitoring of the last inference. The total latency (from sending the data to receiving the result) is made up of the inference time of the model, the overhead of the ML server, the overhead of the ADS communication and the overhead of the PLC cycle. The latter two are referred to as communication latency. The PLC cycle overhead means that the inference result has already been sent to the PLC via ADS, but the task cycle has not yet started executing the predict method. Further information can also be found here: Performance, latencies and configuration [} 33]. Name Type Default Description n Inference Latency LINT Time in microseconds required to execute the inference on the server. n Server Latency LINT Required execution time in microseconds of the Tc Ml Server without inference latency. n Communication Latency LINT ADS communication time plus PLC cycle overhead due to asynchrony. Specified in microseconds. 42 Version: 1.1.0 TF3820
## Page 43

Samples 7 Samples 7.1 AI-based image processing This sample demonstrates how to: • Use Twin CAT Vision to load image data from the local hard disk and make it available in the PLC. • Pre-process images with the Twin CAT Vision library. • Use FB_Ml Svr Prediction to start a session on the locally installed Twin CAT Machine Learning Server and load an AI model (classification model). • Execute the inference on the Twin CAT Machine Learning Server. • Continue to use the result in the PLC. Download and overview of the files You can download the project here: https://infosys.beckhoff.com/content/1033/ TF3820_TC3_Machine_Learning_Server/Resources/17328164363.zip • The ZIP contains a tnzip, which you can open in Twin CAT XAE via File > Open > Open Solution from Archive.... • The models folder contains an ONNX and the already created JSON and Plc Open Xml. • The dataset folder contains sample images that are to be processed. Requirements Install the following workloads: • Twin CAT Standard • TF3820 | Twin CAT Machine Learning Server • TF3830 | Twin CAT Machine Learning Server Client • TF7xxx | Twin CAT 3 Vision Setting up the project • Open the tnzip and save your project. • Set up the File Spource: ◦ In the System Manager, select the Tree Item Vision > File Source > Image Source ◦ Add the images from the models folder. ◦ Set the Cycle Time of the Image Source to 100 ms. • Name the Full Path to the model JSON in line 9 of the MAIN. The ONNX must be in the same folder. • Make sure that all software licenses are available at least in the 7-day trial license: ◦ TC1200 Twin CAT PLC ◦ TF3820 Twin CAT Machine Learning Server ◦ TF7100 Twin CAT Vision Base Optional: Adaptation of the Twin CAT Vision version The example was created with version 5.6.5.0. If you want to use a different version, manually select the version that you have installed and want to use. To do this, first select the version for the corresponding objects. Right-click on the OTCID column to open a context menu. Select “Reload TMI/TMC Description(s) with changed version”. See also Twin CAT Vision documentation. If you have not installed version 5.6.5.0 and do not manually change the setting of the version currently in use, you will receive an error message “Error loading Repository driver”. The latest PLC library version available on the system will be used automatically. TF3820 Version: 1.1.0 43
## Page 44

Samples Executing the project Start the application with Activate Configuration on your target system. If all points are set correctly, after a short time the e State should be set to e Inference and the variable s Label should display the result of the current inference. If an error has occurred, the e State is set to Error. You can then open the fb Ml Svr instance and read out the error code. Use the table of error codes [} 49] to narrow down the problem. Excerpts from the PLC program Declaration In the declaration, the main points concerning the handling of the Twin CAT Machine Learning Server are the input and output data types of the AI model and the instance of the clients to the Machine Learning Server. st Model Input : ST_lemon_model Input; //model input datatype, imported via Plc Open Xml st Model Output : ST_lemon_model Output; //model output datatype, imported via Plc Open Xml fb Ml Svr : FB_Ml Svr Prediction(); // Instance of Client to Tc Ml Server The data types have already been read into the Twin CAT project via the Plc Open Xml (see models folder). The description can be found in the DUTs folder. Configuration of the session In this sample, the client opens a session on the Twin CAT Machine Learning Server in the E_State.e Ml Svr Configuration state. This specifies the system on which the server is accessible, which model is loaded in the session and on which hardware the model is to be executed. fb Ml Svr.st Prediction Parameter.s Ml Model File Path := 'C: \models\lemon_model.json'; // fullpath to model fb Ml Svr.st Prediction Parameter.s Ml Svr Net Id := '127.0.0.1.1.1'; // Server on local system fb Ml SVr.st Prediction Parameter.e Execution Provider := E_EXECUTIONPROVIDER. CPU; // CPU execution 44 Version: 1.1.0 TF3820
## Page 45

Samples // Submit configuration request to the Tc Ml Server // Provide a generous n Timeout, as the configuration can take a substantial amount of time IF fb Ml Svr. Configure(n Timeout := 1000, n Priority:=0) THEN IF fb Ml Svr.n Error Code <> 0 THEN // If n Error Code -1 is encountered, increase n Timeout e State := E_State.e Error; ELSE e State := E_State.e Image Acquisition; END_IF END_IF Calling the method Configure() sends the request to open a session to the server. The call is asynchronous to the PLC task and is acknowledged with a TRUE when the session setup has been successfully completed. Executing the model In the state E_State.e Inference, the Predict call is sent to the Machine Learning Server. This call is also asynchronous to the PLC task. The method returns TRUE if the result is available. In this sample, the image of type ITc Vn Image is copied to the model input data type using the F_VN_Export Image function before the inference call. F_VN_Export Image(ip Tensor Image, ADR(st Model Input.in_input1), n Image Size, hr Vision); // Submission of the asynchronous inference request to the Tc Ml Server IF fb Ml Svr. Predict(p Data In := ADR(st Model Input), n Data In Size := SIZEOF(st Model Input), p Data Out := ADR(st Model Output), n Data Out Size := SIZEOF(st Model Output), n Timeout := 100, n Priority := 0) THEN IF fb Ml Svr.n Error Code <> 0 AND NOT fb Ml Svr.b Configured THEN // If n Error Code -1 is encountered, increase n Timeout e State := E_State.e Error; ELSE // Postprocessing of the inference results F_Softmax(st Model Output.out_367); n Predicted Class := F_Arg Max(st Model Output.out_367); The result of the inference can be used after a successful error check. 7.2 Custom attributes With the Custom attributes, it is possible to provide the AI model with metadata that is to be taken into account in the PLC at runtime. The metadata concept also exists similarly in ONNX, but these cannot be evaluated in the PLC at runtime. In principle, you can use both metadata areas at the same time. Distinguish between information required at runtime in the PLC (Custom attribute area) and information required by the PLC programmer only at the engineering stage (ONNX metadata or Custom attributes). In any case, the aim is for the creator of an ONNX to be able to pass on enough information to the consumer of the ONNX so that the consumer can use the AI model correctly and efficiently. Custom attributes for image preprocessing In the following, the sample AI-based image processing [} 43] is extended. Information about the input image is added. The aim is to adapt the image pre-processing according to the metadata. Implementation in the Twin CAT Machine Learning Model Manager The Configuration Tool can be used in the graphical environment to enter the Custom attributes. TF3820 Version: 1.1.0 45
## Page 46

Samples Implementation in Python From a workflow perspective, it is often more convenient to enter all required Custom attributes directly in Python after model training. new_ca = { 'Image Input' : {'nwidth' : 244, 'nheight' : 244, 'n Max Pixel Value' : 255, 'f Mean' : [0.485 , 0.456, 0.40], 'f Std' : [0.229, 0.224, 0.225]} } tb.modify_ca("C:\\models\\lemon_model.json", "C:\\models\\lemon_model.json", new_ca) The result can be traced in plain text in the JSON. 46 Version: 1.1.0 TF3820
## Page 47

Samples Reading the Custom attributes in Twin CAT The PLC project from the sample is expanded below. Once a session has been successfully configured on the Twin CAT Machine Learning Server, the stored Custom attributes are read out. IF fb Ml Svr. Configure(n Timeout := 1000, n Priority:=0) THEN IF fb Ml Svr.n Error Code <> 0 THEN // If n Error Code -1 is encountered, increase n Timeout e State := E_State.e Error; ELSE // read all relevant meta information from custom attributes IF fb Ml SVr. Get Custom Attribute_int64('Image Input/height', n Height) THEN // check fb Ml SVr.n Error Code for further reference e State := E_State.e Error; END_IF fb Ml SVr. Get Custom Attribute_int64('Image Input/width', n Width); fb Ml SVr. Get Custom Attribute_int64('Image Input/Max Pixel Value', n Max Pixel Value); fb Ml SVr. Get Custom Attribute_array('Image Input/ Mean', dtype Custom Attribute, ADR(a Mean), SIZEOF(a Mean), n Length, ADR(n Bytes)); fb Ml SVr. Get Custom Attribute_array('Image Input/ TF3820 Version: 1.1.0 47
## Page 48

Samples Std', dtype Custom Attribute, ADR(a Std), SIZEOF(a Std), n Length, ADR(n Bytes)); e State := E_State.e Image Acquisition; END_IF END_IF The Custom attributes stored in PLC variables are then used in the pre-processing pipeline. Essentially, all that needs to be considered here are the necessary type conversions. // Adjust the dimensions of the input image to match the model input requirements hr Vision := F_VN_Resize Image Exp(ip Input Image, ip Tensor Image, LINT_TO_UDINT(n Width), LINT_TO_UDINT(n H eight), e Interpolation Type, e Padding Mode, a Black, hr Vision); // Convert the image to type REAL and scale to the range [0.0, 1.0] hr Vision := F_VN_Convert Element Type Exp(ip Tensor Image, ip Tensor Image, TCVN_ET_REAL, 1.0 / LINT_TO_REA L(n Max Pixel Value), 0, hr Vision); // Normalization hr Vision := F_VN_Subtract Vector From Image(ip Tensor Image, a Mean, ip Tensor Image,hr Vision); hr Vision := F_VN_Divide Image By Vector(ip Tensor Image, a Std, ip Tensor Image, hr Vision); Once the configuration has been activated, you can use the Online View to check that the values have been applied correctly. 48 Version: 1.1.0 TF3820
## Page 49

Appendix 8 Appendix 8.1 Error Codes Error Code Error Code (hex) Description Hints (dec) 0 0000 Operation successful - -1 FFFF ML server timeout Increase the timeout argument of your request. Check, that the Tc Ml Server service is running. -3 FFFD ML server malformed data - -5 FFFB PLC protocol error Make sure to call the methods of the FB in a canonical order, i.e. make sure the FB is configured before making inference calls. -8 FFF8 Invalid AMS Net ID Check the syntactical correctness of the provided Ams Net Id. -9 FFF7 Input pointer is null Check the validity of the input data pointer provided to the predict methods. -11 FFF5 Output pointer is null Check the validity of the output data pointer provided to the predict methods. -13 FFF3 Batch size is zero Provide a valid batch size (>= 1). -15 FFF1 Input size is zero Provide a valid input data size (>0). -17 FFEF Output size is zero Check your installation of TF3830. -18 FFEE Driver instantiation failed Check your installation of TF3830. -20 FFEC Driver state propagation to state Check the validity and integrity of OP failed your model file (existing path, valid file format). -22 FFEA Driver loading failed Check your installation of TF3830. -24 FFE8 Driver state depropagation failed Check your installation of TF3830. -26 FFE6 Driver parameter configuration Twin CAT-internal error failed -28 FFE4 Could not instantiate file Twin CAT-internal error accessor -30 FFE2 Could not propagate file accessor Twin CAT-internal error state -32 FFE0 Could not access model file Check the validity of your model file (existing path, valid file format). -34 FFDE Mismatch in read model file bytes Twin CAT-internal error -36 FFDC Could not parse model file Check the integrity of your model file. -38 FFDA String buffer overflow Check the integrity of your model file, especially the length of the contained model hash. -40 FFD8 Could not find model config in file Check the integrity of your model file, especially the defined model configuration. -42 FFD6 Could not retrieve model Check the integrity of your model attributes file, especially the availability of a model hash. -43 FFD5 ADS server connection error ADS messages could not be sent. Check integrity of Ams Router. TF3820 Version: 1.1.0 49
## Page 50

Appendix Error Code Error Code (hex) Description Hints (dec) -44 FFD4 Engine mismatch The model file you intended to load is not designated for the Tc Ml Server. -45 FFD3 Invalid CST attribute name Check the name of the attribute in your model file or your query. The name must not be empty. -47 FFD1 CST attribute retrieval failed The queried attribute could not be found. Check the queried attribute name in query and model file. -49 FFCF CST attribute invalid buffer Check the destination pointer provided to the retrieval function. Must not be a null pointer. -50 FFCE Versioned model could not be Check the valditiy of the model resolved version you are querying. -51 FFCD If a PLC online change is detected, a running predict is discarded with the error -52 Incompatible server-version ML-Server library version 2.x.x is detected. not compatible with ML-Server version 3.x.x. -101 FF9B Invalid PLC request variant Internal error -102 FF9A Invalid device index Verify the device index provided in prediction parameters of the FB. -103 FF99 Invalid request data Internal error -201 FF37 Invalid data Internal error -202 FF36 Invalid model type Requested tensor of unsupported datatype. See log files. -206 FF32 Invalid model The provided model file is not supported. check especially, that no dimension parameters are left except for an optional batch dimension. -300 FED4 Invalid execution provider The requested execution provider is invalid. Make sure to select an EP from the enum E_Execultion Provider. -302 FED2 Unsupported execution provider The requested execution provder is valid but not supported on your system (for instance due to a lack of GPU support). -400 FE70 Session config file not found Internal error -402 FE6E Environment config file not found Internal error -404 FE6C Run config file not found Internal error -406 FE6A Server config file not found Internal error -408 FE68 Device config files not found Internal error -500 FE0C Invalid session config file Internal error -502 FE0A Invalid environment config file Internal error -504 FE08 Invalid run config file Internal error -506 FE06 Invalid server config file Internal error -508 FE04 Invalid device config files Internal error -600 FDA8 Targeted inference provider Internal error unavailable 50 Version: 1.1.0 TF3820
## Page 51

Appendix Error Code Error Code (hex) Description Hints (dec) -602 FDA6 Inference provider session Increase the session timeout in the timeout prediction parameters. -701 FD43 Dynamic loaded library not found Internal error -703 FD41 Symbol not loadable from Internal error dynamic library -801 FCDF Unknown job execution error Internal error -803 FCDD Unknown error Internal error -901 FC7B Failed backend execution Internal error -1100 FBB4 License server connection error Internal licensing error -1102 FBB2 License server ADS error Internal licensing error -1104 FBB0 License ADS error Internal licensing error -1106 FBAE License invalid Make sure, that a license TF3820 is available. -1108 FBAC License invalid unknown Internal licensing error -1201 FB4F Expected tensor collection type Make sure, that only types defined in the PLCopen files generated by the Beckhoff model management products is used. Do not manipulate the contained header -1203 FB4D Mismatching tensor collection Make sure, that only types defined hashes in the PLCopen files generated by the Beckhoff model management products is used. Do not manipulate the contained header -1205 FB4B Invalid tensor collection header Make sure, that only types defined in the PLCopen files generated by the Beckhoff model management products is used. Do not manipulate the contained header -1207 FB49 Tensor collection offset feature Make sure, that only types defined not supported in the PLCopen files generated by the Beckhoff model management products is used. Do not manipulate the contained header -1209 FB47 Internal allocation special type Internal error header -1300 FAEC Server startup cannot build log No Logs directory available and directory could not be generated. -1801 F8F7 NVIDIA NVML library function Make sure the Nvidia nvml.dll is failure avilable. -1803 F8F5 NVIDIA NVML library extraction Make sure the Nvidia nvml.dll is failure avilable. -1901 F893 NVIDIA CUDA library function Make sure the Nvidia cuda.dll is failure available. -1903 F891 NVIDIA CUDA library extraction Make sure the Nvidia cuda.dll is failure available. -2000 F830 Model registry file storage error Internal error -2002 F82E Model registry file deletion error Internal error -2003 F82D Model registry invalid reference Internal error -2004 F82C Model registry entry failed to load Internal error model -2006 F82A Model registry inconsistent model Internal error hashes TF3820 Version: 1.1.0 51
## Page 52

Appendix Error Code Error Code (hex) Description Hints (dec) -2008 F828 ADS could not open remote file Make sure, that an .onnx file with the same name as your .json model description file is located in the same directory. -2010 F826 ADS could not retrieve size of Internal error remote file -2012 F824 ADS could not load remote file Make sure, that your AMS router has sufficient memory for your .onnx file. -2014 F822 ADS could not close remote file Internal error -2018 F81E ADS could not open client port Make sure your Twin CAT installation is valid. -2201 F767 Buffer base is null Internal error -2203 F765 Not enough memory for head Internal error -2205 F763 Buffer out of memory Internal error -2207 F761 Buffer cannot read from null Internal error -2209 F75F Buffer cannot write to null Internal error -2211 F75D Buffer corrupted with invalid Internal error string -2020 F81C Insufficient AMS Router Memory Increase Router Memory on the on Client Client system -2022 F81A Insufficient AMS Router Memory Increase Router Memory on the on Server Server system -2024 F818 Could not read router memory on Internal error client -2026 F816 Could not read router memory on Internal error server -2301 F703 Invalid CUDA setup detected Check your CUDA and cu DNN installation. 8.2 Log files Twin CAT Machine Learning Model Manager Logs: C:\Program Data\Beckhoff\Twin CAT\Functions\TF38xx- Machine-Learning\Logs Twin CAT Machine Learning Server Logs: C:\Program Data\Beckhoff\Twin CAT\Functions\TF38xx-Machine- Learning\Tc Ml Server\Logs 8.3 Third-party components This software contains third-party components. Please refer to the license file provided in the following folder for further information: C:\Program Files (x86)\Beckhoff\Legal\Twin CAT-XAR-Ml Server C:\Program Files (x86)\Beckhoff\Legal\Twin CAT-XAE-Model Manager Core 52 Version: 1.1.0 TF3820
## Page 53

Appendix 8.4 Support and Service Beckhoff and their partners around the world offer comprehensive support and service, making available fast and competent assistance with all questions related to Beckhoff products and system solutions. Download finder Our download finder contains all the files that we offer you for downloading. You will find application reports, technical documentation, technical drawings, configuration files and much more. The downloads are available in various formats. Beckhoff's branch offices and representatives Please contact your Beckhoff branch office or representative for local support and service on Beckhoff products! The addresses of Beckhoff's branch offices and representatives round the world can be found on our internet page: www.beckhoff.com You will also find further documentation for Beckhoff components there. Beckhoff Support Support offers you comprehensive technical assistance, helping you not only with the application of individual Beckhoff products, but also with other, wide-ranging services: • support • design, programming and commissioning of complex automation systems • and extensive training program for Beckhoff system components Hotline: +49 5246 963-157 e-mail: support@beckhoff.com Beckhoff Service The Beckhoff Service Center supports you in all matters of after-sales service: • on-site service • repair service • spare parts service • hotline service Hotline: +49 5246 963-460 e-mail: service@beckhoff.com Beckhoff Headquarters Beckhoff Automation Gmb H & Co. KG Huelshorstweg 20 33415 Verl Germany Phone: +49 5246 963-0 e-mail: info@beckhoff.com web: www.beckhoff.com TF3820 Version: 1.1.0 53
## Page 54

Trademark statements Beckhoff®, Twin CAT®, Twin CAT/BSD®, TC/BSD®, Ether CAT®, Ether CAT G®, Ether CAT G10®, Ether CAT P®, Safety over Ether CAT®, Twin SAFE®, XFC®, XTS® and XPlanar® are registered trademarks of and licensed by Beckhoff Automation Gmb H. Third-party trademark statements Debian is a registered trademark owned by Software in the Public Interest, Inc. The registered trademark Linux® is used pursuant to a sublicense from the Linux Foundation, the exclusive licensee of Linus Torvalds, owner of the mark on a worldwide basis. Microsoft, Microsoft Azure, Microsoft Edge, Power Shell, Visual Studio, Windows and Xbox are trademarks of the Microsoft group of companies. NVIDIA, NVIDIA RTX and CUDA are trademarks of NVIDIA Corporation.
## Page 55

More Information: www.beckhoff.com/tf3820 Beckhoff Automation Gmb H & Co. KG Hülshorstweg 20 33415 Verl Germany Phone: +49 5246 9630 info@beckhoff.com www.beckhoff.com
