---
title: "TF38x0_TC3_ML_NN_Inference_Engine_EN"
product: "TC3"
category: "Machine_Learning"
tags: ["ENGINE", "INFERENCE"]
language: "EN"
document_type: "Manual"
version: "1.7.0"
source_pdf: "https://download.beckhoff.com/download/Document/automation/twincat3/TF38x0_TC3_ML_NN_Inference_Engine_EN.pdf"
release_date: "2025-06-12"
---
Manual | EN TF3800, TF3810 Twin CAT 3 | Machine Learning- und Neural Network Inference Engine 2025-06-12 | Version: 1.7.0
## Page 3

Table of contents Table of contents 1 Foreword.................................................................................................................................................... 5 1.1 Notes on the documentation............................................................................................................. 5 1.2 For your safety.................................................................................................................................. 5 1.3 Notes on information security............................................................................................................ 7 1.4 Documentation issue status.............................................................................................................. 8 2 Overview.................................................................................................................................................... 9 3 Installation............................................................................................................................................... 11 3.1 System requirements...................................................................................................................... 11 3.2 Installation....................................................................................................................................... 11 3.3 Licensing......................................................................................................................................... 15 4 Quick start............................................................................................................................................... 18 5 Machine Learning Models and file formats.......................................................................................... 22 5.1 Machine learning models supported............................................................................................... 22 5.1.1 Multi-layer perceptron...................................................................................................... 22 5.1.2 Support vector machine................................................................................................... 29 5.1.3 k-Means........................................................................................................................... 35 5.1.4 Principal Component Analysis......................................................................................... 36 5.1.5 Decision Tree................................................................................................................... 38 5.1.6 Extra Tree......................................................................................................................... 40 5.1.7 Ensemble Tree methods.................................................................................................. 42 5.2 Machine Learning Cheat Sheet: selection of models...................................................................... 60 5.3 Creation and conversion of ONNX.................................................................................................. 62 5.3.1 Open Neural Network Exchange (ONNX)........................................................................ 62 5.3.2 Samples of ONNX export................................................................................................. 63 5.3.3 Conversion from ONNX to XML and BML........................................................................ 65 5.4 File management of the ML description files................................................................................... 76 6 API............................................................................................................................................................ 79 6.1 Tc COM............................................................................................................................................ 79 6.2 PLC API.......................................................................................................................................... 81 6.2.1 Datatypes......................................................................................................................... 81 6.2.2 Function Blocks................................................................................................................ 82 7 Samples................................................................................................................................................... 94 7.1 PLC API.......................................................................................................................................... 94 7.1.1 Quick start........................................................................................................................ 94 7.1.2 Detailed example............................................................................................................. 94 7.1.3 Parallel, non-blocking access to an inference module..................................................... 94 8 Support and Service............................................................................................................................... 96 9 Appendix.................................................................................................................................................. 97 9.1 Log files........................................................................................................................................... 97 9.2 Third-party components.................................................................................................................. 97 9.3 XML Exporter.................................................................................................................................. 98 9.3.1 XML Exporter - samples................................................................................................. 100 TF3800, TF3810 Version: 1.7.0 3
## Page 4

Table of contents 4 Version: 1.7.0 TF3800, TF3810
## Page 5

Foreword 1 Foreword 1.1 Notes on the documentation This description is intended exclusively for trained specialists in control and automation technology who are familiar with the applicable national standards. The documentation and the following notes and explanations must be complied with when installing and commissioning the components. The trained specialists must always use the current valid documentation. The trained specialists must ensure that the application and use of the products described is in line with all safety requirements, including all relevant laws, regulations, guidelines, and standards. Disclaimer The documentation has been compiled with care. The products described are, however, constantly under development. We reserve the right to revise and change the documentation at any time and without notice. Claims to modify products that have already been supplied may not be made on the basis of the data, diagrams, and descriptions in this documentation. Trademarks Beckhoff®, ATRO® , Ether CAT®, Ether CAT G®, Ether CAT G10®, Ether CAT P®, MX-System®, Safety over Ether CAT®, TC/BSD®, Twin CAT®, Twin CAT/BSD®, Twin SAFE®, XFC®, XPlanar®, and XTS® are registered and licensed trademarks of Beckhoff Automation Gmb H. If third parties make use of the designations or trademarks contained in this publication for their own purposes, this could infringe upon the rights of the owners of the said designations. Ether CAT® is a registered trademark and patented technology, licensed by Beckhoff Automation Gmb H, Germany. Copyright © Beckhoff Automation Gmb H & Co. KG, Germany. The distribution and reproduction of this document, as well as the use and communication of its contents without express authorization, are prohibited. Offenders will be held liable for the payment of damages. All rights reserved in the event that a patent, utility model, or design are registered. Third-party trademarks Trademarks of third parties may be used in this documentation. You can find the trademark notices here: https://www.beckhoff.com/trademarks. 1.2 For your safety Safety regulations Read the following explanations for your safety. Always observe and follow product-specific safety instructions, which you may find at the appropriate places in this document. Exclusion of liability All the components are supplied in particular hardware and software configurations which are appropriate for the application. Modifications to hardware or software configurations other than those described in the documentation are not permitted, and nullify the liability of Beckhoff Automation Gmb H & Co. KG. TF3800, TF3810 Version: 1.7.0 5
## Page 6

Foreword Personnel qualification This description is only intended for trained specialists in control, automation, and drive technology who are familiar with the applicable national standards. Signal words The signal words used in the documentation are classified below. In order to prevent injury and damage to persons and property, read and follow the safety and warning notices. Personal injury warnings DANGER Hazard with high risk of death or serious injury. WARNING Hazard with medium risk of death or serious injury. CAUTION There is a low-risk hazard that could result in medium or minor injury. Warning of damage to property or environment NOTICE The environment, equipment, or data may be damaged. Information on handling the product This information includes, for example: recommendations for action, assistance or further information on the product. 6 Version: 1.7.0 TF3800, TF3810
## Page 7

Foreword 1.3 Notes on information security The products of Beckhoff Automation Gmb H & Co. KG (Beckhoff), insofar as they can be accessed online, are equipped with security functions that support the secure operation of plants, systems, machines and networks. Despite the security functions, the creation, implementation and constant updating of a holistic security concept for the operation are necessary to protect the respective plant, system, machine and networks against cyber threats. The products sold by Beckhoff are only part of the overall security concept. The customer is responsible for preventing unauthorized access by third parties to its equipment, systems, machines and networks. The latter should be connected to the corporate network or the Internet only if appropriate protective measures have been set up. In addition, the recommendations from Beckhoff regarding appropriate protective measures should be observed. Further information regarding information security and industrial security can be found in our https://www.beckhoff.com/secguide. Beckhoff products and solutions undergo continuous further development. This also applies to security functions. In light of this continuous further development, Beckhoff expressly recommends that the products are kept up to date at all times and that updates are installed for the products once they have been made available. Using outdated or unsupported product versions can increase the risk of cyber threats. To stay informed about information security for Beckhoff products, subscribe to the RSS feed at https:// www.beckhoff.com/secinfo. TF3800, TF3810 Version: 1.7.0 7
## Page 8

Foreword 1.4 Documentation issue status Version Changes 1.7.x • Extension of the graphical user interface [} 66] of the Twin CAT 3 Machine Learning Model Manager • Adjustments to the installation of the Python package [} 72] • Twin CAT 3.1 Build 4026: Brief information on installation [} 11] 8 Version: 1.7.0 TF3800, TF3810
## Page 9

Overview 2 Overview Introduction The idea behind machine learning is to learn a generalized correlation between inputs and outputs on the basis of example data. Accordingly, a certain amount of training data is required, on the basis of which a model is trained. In the training of the model, parameters of the model are adapted automatically to the training data by means of a mathematical process. In machine learning, the user has a large number of different models at his disposal. The selection and design of the models is part of the engineering process. Different types or designs of models fulfill different tasks, wherein the most important subdivisions are the classification and regression. Classification: The model receives an input (an image, one or more vectors, etc.) and assigns it to a class. The output is correspondingly a categorized variable. These classes could be, for example, good part or bad part. Distinction can also be made between several classes, for example quality classes A, B, C, D. Regression: The model receives an input and generates a continuous output. Not only are directly learned inputs assigned to directly learned outputs (as with a lookup table), but in addition the model is able to interpolate or, respectively, extrapolate non-learned inputs, provided it generalizes well. A functional correlation is learned. Once a model has been trained, it can be used for the learned task, i.e. the model is used for inference. TC3 Machine Learning Runtime Beckhoff supplies components for the inference of models in the Twin CAT XAR with its products TF3800 Twin CAT 3 Machine Learning Inference Engine and TF3810 Neural Network Inference Engine. A common software basis is used for both products, which is referred to in the following as the Machine Learning Runtime (ML Runtime for short). The ML Runtime is a module (Tc COM) integrated in Twin CAT 3 and executed in the Twin CAT XAR. It is thus possible to access the model interface (model inputs and model outputs) as well as to execute the model loaded in the ML Runtime in hard real-time. The TF3800 and TF3810 have different licenses. The license required depends on the ML model to be loaded. In principle, the TF3800 is required for loading and executing classic ML models. The TF3810 license is required for the loading of neural networks. The TF3810 includes the TF3800 license. Additional information on supported models and required licenses. [} 22] Workflow Basically, the process of machine learning and integration into Twin CAT 3 consists of three phases: 1. The collection of data 2. The training of a model 3. The deployment in the Twin CAT XAR A large number of Twin CAT products are available for the collection of data from the controller: see Twin CAT Scope, Twin CAT Database Server, Twin CAT Analytics Logger, Twin CAT Io T, etc. ML models can be trained in a large number of software tools. ML models are usually created in programming environments such as Python or R. Various open source and free tools exist that are suitable for the creation of ML models, such as Py Torch, Keras and Scikit-learn. Trained models can be exported from these tools in a standardized format as an ONNX file. The ONNX file is a standardized description of the trained ML model. This file is first converted into a format that is conditioned for Twin CAT (XML or BML file). Further Information: • Creation of ONNX files [} 63] • Conversion of ONNX files [} 65] Twin CAT offers two methods for deploying the model: TF3800, TF3810 Version: 1.7.0 9
## Page 10

Overview 1. The library TC3_MLL is provided for use in the PLC environment. ML models can be loaded asynchronously via a method call and subsequently executed cyclically in the PLC program by calling a further method. Additional information on the PLC API [} 81] 2. A simple method of machine learning without programming effort: a Tc COM object that can be inserted in the Twin CAT object tree in the development environment and configured. On starting the system, the Tc COM loads the configured model and executes it in the assigned cycle time. Additional information on the ML Tc COM [} 79] The picture below illustrates the deep integration of the Machine Learning Runtime in the Twin CAT XAR. Like all Twin CAT Runtime objects, the module is a Tc COM and is accordingly anchored deep in the hard real-time. Integration of machine learning into Twin CAT Analytics The products Machine Learning Inference Engine and Neural Network Inference Engine can also be integrated into the Twin CAT Analytics workflow. Refer to the Twin CAT Analytics documentation for detailed information. 10 Version: 1.7.0 TF3800, TF3810
## Page 11

Installation 3 Installation 3.1 System requirements Runtime (XAR) Technical data Description Operating system Windows 10 (64-bit), Twin CAT/BSD Target platform PC architecture (x64) Twin CAT version Twin CAT 3.1 Build 4024 | Build 4026 Required Twin CAT license TF3800 TC3 Machine Learning Inference Engine or TF3810 TC3 Neural Network Inference Engine The required license depends on the loaded ML model, see Machine learning models supported [} 22]. Engineering (XAE) Technical data Description Twin CAT version Twin CAT 3.1 Build 4024 | Build 4026 The setup is to be executed both on the engineering PC and on the runtime PC. 7-day trial licenses can be generated for the runtime. 3.2 Installation Twin CAT Package Manager: Installation (Twin CAT 3.1 Build 4026) Detailed instructions on installing products can be found in the chapter Installing workloads in the Twin CAT 3.1 Build 4026 installation instructions. Install the following workload to be able to use the product: Command line: tcpkg install TF3800. Machine Learning Inference Engine. XAE tcpkg install TF3800. Machine Learning Inference Engine. XAR Command line: tcpkg install TF3810. Neural Network Inference Engine. XAE tcpkg install TF3810. Neural Network Inference Engine. XAR TF3800, TF3810 Version: 1.7.0 11
## Page 12

Installation Command line: tcpkg install TF38xx. Machine Learning Model Manager. XAE Setup installation (Twin CAT 3.1 Build 4024) The following section describes how to install the Twin CAT 3 function for Windows-based operating systems. ü The Twin CAT 3 function setup file was downloaded from the Beckhoff website. 1. Run the setup file as administrator. To do this, select the Run As Admin command in the context menu of the file. ð The installation dialog opens. 2. Accept the end user licensing agreement and click Next. 12 Version: 1.7.0 TF3800, TF3810
## Page 13

Installation 3. Enter your user data. 4. If you want to install the full version of the Twin CAT 3 function, select Complete as the installation type. If you want to install the Twin CAT 3 function components separately, select Custom. TF3800, TF3810 Version: 1.7.0 13
## Page 14

Installation 5. Click Next, then Install to start the installation. ð A dialog box informs you that the Twin CAT system must be stopped to proceed with the installation. 6. Confirm the dialog with Yes. 14 Version: 1.7.0 TF3800, TF3810
## Page 15

Installation 7. Click Finish to exit the setup. ð The Twin CAT 3 function has been installed successfully. 3.3 Licensing The Twin CAT 3 function can be activated as a full version or as a 7-day test version. Both license types can be activated via the Twin CAT 3 development environment (XAE). Licensing the full version of a Twin CAT 3 Function A description of the procedure to license a full version can be found in the Beckhoff Information System in the documentation "Twin CAT 3 Licensing". Licensing the 7-day test version of a Twin CAT 3 Function A 7-day test version cannot be enabled for a Twin CAT 3 license dongle. 1. Start the Twin CAT 3 development environment (XAE). 2. Open an existing Twin CAT 3 project or create a new project. 3. If you want to activate the license for a remote device, set the desired target system. To do this, select the target system from the Choose Target System drop-down list in the toolbar. ð The licensing settings always refer to the selected target system. When the project is activated on the target system, the corresponding Twin CAT 3 licenses are automatically copied to this system. TF3800, TF3810 Version: 1.7.0 15
## Page 16

Installation 4. In the Solution Explorer, double-click License in the SYSTEM subtree. ð The Twin CAT 3 license manager opens. 5. Open the Manage Licenses tab. In the Add License column, check the check box for the license you want to add to your project (e.g. "TF4100 TC3 Controller Toolbox"). 6. Open the Order Information (Runtime) tab. ð In the tabular overview of licenses, the previously selected license is displayed with the status “missing”. 16 Version: 1.7.0 TF3800, TF3810
## Page 17

Installation 7. Click 7-Day Trial License... to activate the 7-day trial license. ð A dialog box opens, prompting you to enter the security code displayed in the dialog. 8. Enter the code exactly as it is displayed and confirm the entry. 9. Confirm the subsequent dialog, which indicates the successful activation. ð In the tabular overview of licenses, the license status now indicates the expiry date of the license. 10. Restart the Twin CAT system. ð The 7-day trial version is enabled. TF3800, TF3810 Version: 1.7.0 17
## Page 18

Quick start 4 Quick start Convert model (optional) An ML description file (Keras MLPExample_cos. XML) is provided in the ZIP of the https:// infosys.beckhoff.com/content/1033/tf38x0_tc3_ML_NN_Inference_Engine/Resources/8746884875.zip. This file can be used directly for integration into Twin CAT, or optionally converted to a binary format (BML). The method of converting a description file is shown below by way of an example. The same procedure also applies to the more usual case where an ONNX file is to be converted. ü The Machine Learning Model Manager [} 66] is open. Menu bar: (Extensions)* > Twin CAT > Machine Learning > Machine Learning Model Manager * Only with Visual Studio 2019 1. Use the Convert tool. 2. Click Select file and select the file Keras MLPExample_cos. XML. 3. In the drop-down menu, select Convert to *.bml 4. Click Convert files. ð The corresponding BML appears under <Twin CATpath>\Functions\TF38xx-Machine- Learning\Convert Tool Files You can change the default path for saving converted ML description files in the Machine Learning Model Manager with Select Target Path. The change is persistent. You can also convert several files at once by means of multi-selection. The converted files are saved by default on the XAE system in the folder <Twin CATpath>\Functions\TF38xx-Machine-Learning\Convert Tool Files. Integration in Twin CAT via the PLC API The procedure to load the ML description file into Twin CAT and to run it cyclically is described below. The PLC API [} 81] is dealt with first. • First of all, create a Twin CAT project and a PLC project • Add the PLC library Tc3_MLL under the References node In the Declaration, please create an instance of FB_Mll Prediction. In this simple case, the description file contains an MLP with one input and one output of the type FP32; accordingly, variables for input and output are created as REAL. A more generally accepted possibility to handle the inputs and outputs can be found in the Samples for PLC API [} 94]. In addition, create a string variable that contains the file name incl. path to the ML description file (path on the target system). Copy the corresponding file to this location on the target system (FTP, RDP, Shared Folder, …). Further information on this step can be found here [} 76]. NOTICE Path of the description file on the target system Pay attention to the settings of the File Writer and the writing rights on the target system. PROGRAM MAIN VAR fb Predict : FB_Mll Prediction; // ML Interface n Input Dim, n Output Dim : UDINT := 1; f Data In, f Data Out : REAL; s Model Name : T_Max String := 'C:/Twin CAT/3.1/Boot/ML_Boot/Keras MLPExample_cos.xml'; hr Error Code : HRESULT; b Load Model : BOOL; n State : INT := 0; END_VAR 18 Version: 1.7.0 TF3800, TF3810
## Page 19

Quick start In the Implementation part you create a state machine, for example, which enables you to switch between the Idle state, Config state, Predict state and Error state. In the first state you initially wait for the command to load a description file. Subsequently, the Configure method [} 85] of the fb Predict function block is called until TRUE is returned. This is present for one cycle and means that the configuration has been completed. A check is then done to establish whether an error occurred. If no error occurred, the state is switched to the next state, which is the Predict state. The state machine remains in the Predict state as long as no error occurs or a (new) model is to be loaded. CASE n State OF 0: // idle state IF b Load Model THEN b Load Model := FALSE; n State := 10; END_IF 10: // Config state fb Predict.st Prediction Parameter. Ml Model Filepath := s Model Name; // provide model path and name IF fb Predict. Configure() THEN // load model IF fb Predict.b Error THEN n State := 999; hr Error Code := fb Predict.hr Error Code; ELSE // no error -> proceed to predict state n State := 20; END_IF END_IF 20: // Predict state fb Predict. Predict( p Data Inp := ADR(f Data In), n Data Inp Dim := n Input Dim, fmt Data Inp Type := ETc Mll Data Type. E_MLLDT_FP32_REAL, p Data Out := ADR(f Data Out), n Data Out Dim := n Output Dim, fmt Data Out Type := ETc Mll Data Type. E_MLLDT_FP32_REAL, n Engine Id := 0, n Concurrency Id := 0); IF fb Predict.b Error THEN // error handling n State := 999; hr Error Code := fb Predict.hr Error Code; ELSIF b Load Model THEN // load (updated) model b Load Model := FALSE; n State := 10; END_IF; 999: // Error state // add error handling here END_CASE The Predict method [} 90] of fb Predict is used in the Predict state. This runs the loaded model. The method is informed of the input variables via the first three parameters of the method – pointer to a PLC variable, number of inputs and associated data type. The same is to be specified for the output variables (parameters 4 to 6). n Engine Id and n Concurrency Id are not required in this simple example and are always transferred with the value zero. Details for these parameters can be found in the samples Detailed example [} 94] and Parallel, non-blocking access to an inference module [} 94]. Before activating the project on a target, you must select the TF3810 license manually on the Manage Licenses tab under System>License in the project tree, as you wish to load a multi-layer perceptron (MLP). You can now activate the configuration. Log into the PLC and start the program. By setting the b Load Model variable in the online view to TRUE, the model is now loaded and begins with the prediction. You can manipulate the input variable f Data In and view the results in the output f Data Out. The multi-layer perceptron loaded approximately maps a cosine function in the input range of [-pi, pi] to the value range [-1,1]. Outside of the range [-pi, pi] the function increasingly diverges from the cosine function. TF3800, TF3810 Version: 1.7.0 19
## Page 20

Quick start You can download the sample described above here [} 94]. Incorporation of a model by means of Tc COM object This section deals with the execution of machine learning models by means of a prepared Tc COM object. A detailed description can be found here [} 79]. This interface offers a simple and clear way of loading models, executing them in real-time and generating appropriate links in your own application by means of the process image. Generate a prepared Tc COM object Tc Machine Learning Model Cycal 1. To do this, select the node Tc COM Objects with the right mouse button and select Add New Item… Under Tasks, generate a new Twin CAT task and assign this task context to the newly generated instance of Tc Machine Learning Model Cycal 20 Version: 1.7.0 TF3800, TF3810
## Page 21

Quick start 2. To do this, open the Context tab of the generated object. 3. Select your generated task in the drop-down menu. ð The instance of Tc Machine Learning Model Cycal has a tab called ML model configuration where you can load the description file of the ML algorithm (XML or BML) and the available data types for the inputs and outputs of the selected model are then displayed. • The file does not have to be on the target system. It can be selected from the development system and is then loaded to the target system on activating the configuration. ◦ A distinction is made between preferred and supported data types. The only difference is that a conversion of the data type takes place at runtime if a non-preferred type is selected. This may lead to slight losses in performance when using non-preferred data types. • The data types for inputs and outputs are initially set automatically to the preferred data types. The process image of the selected model is created by clicking Generate IO. Accordingly, by loading Keras MLPExample_cos.xml, you get a process image with an input of the type REAL and an output of the type REAL. Activating the project on the target 1. Before activating the project on a target, you must select the TF3810 license manually on the Manage Licenses tab under System>License in the project tree, as you wish to load a multi-layer perceptron (MLP). 2. Activate the configuration. ð You can now test the model by manually writing at the input. TF3800, TF3810 Version: 1.7.0 21
## Page 22

Machine Learning Models and file formats 5 Machine Learning Models and file formats This chapter provides an overview of supported Machine Learning models [} 22] and lists guiding principles for the selection of a suitable model [} 60]. In addition, it contains for each algorithm an example-based description of how you can export trained models from a Python environment as an ONNX file [} 62]. The exported ONNX file must be converted into a Twin CAT-specific XML or BML file [} 65]. For this purpose, several interfaces to a converter are available (CLI, API and GUI), with which the file management process can be integrated into your existing software landscapes. 5.1 Machine learning models supported The table below lists all supported model types, including the required license and software version. Selecting a model An introduction explaining which criteria you should consider when selecting a model can be found here: Machine Learning Cheat Sheet: selection of models [} 60]. ONNX export for supported model types Python examples of the ONNX export from different frameworks are given for all supported model types in the section Samples of ONNX export [} 63]. Required license for supported model types The required Twin CAT license differs depending on the model type that is loaded into the Machine Learning Runtime. Note that the TF3810 license contains the TF3800 license, which means that if the TF3810 license is valid, all models that require a TF3800 or TF3810 license can be loaded. Supported models For licensing, refer also to: System requirements [} 11]. Model type License Available from setup version Support vector machine [} 29] TF3800 3.1.42.0 (SVM) Principal Component Analysis TF3800 3.1.57.0 [} 36] (PCA) k-Means [} 35] TF3800 3.1.57.0 Random Forest [} 45] TF3800 3.1.58.0 Multi-layer perceptron [} 22] TF3810 3.1.42.0 (MLP) Decision Tree [} 38] TF3800 3.1.62.0 Extra Tree [} 40] TF3800 3.1.62.0 Extra Trees [} 43] TF3800 3.1.62.0 Gradient Boosting [} 48] TF3800 3.1.62.0 Hist Gradient Boosting [} 50] TF3800 3.1.62.0 XGBoost [} 52] TF3800 3.1.62.0 Light GBM [} 56] TF3800 3.1.62.0 5.1.1 Multi-layer perceptron A multi-layer perceptron (MLP) can be used both for classification and for regression [} 62]. The basic idea of an MLP is the linking of the smallest units – so-called neurons – in a network. Each neuron takes up information from previous neurons or directly via model inputs and processes it. A directional flow of information takes place through this network from inputs to outputs. 22 Version: 1.7.0 TF3800, TF3810
## Page 23

Machine Learning Models and file formats A neuron processes its inputs x as a weighted sum plus an ordinate value and transforms the intermediate result with an activation function. Neurons are usually arranged in layers, which are then linked one after the other. If a network has more than one layer between inputs and outputs, then it is referred to as a multi-layer perceptron. The structure is illustrated in the figure below. • Input layer: Has no neurons of its own. Serves as an input layer and defines the number and data type of the inputs. • Hidden layer: Layer with its own neurons. The layer is characterized by the number of neurons as well as the selected activation function. Any number of hidden layers can be layered one after the other. • Output layer: Layer with its own neurons. The number of neurons and their activation function are oriented to the application to be implemented. Input Output signals signals Input layer Hidden layer Hidden layer Output layer Supported properties ONNX support The following ONNX operators are supported: • Mat Mul (matrix multiplication) followed by ADD (add) • GEMM (general matrix multiplication) In addition, the following activation functions are supported: Activation function Description tanh Hyperbolic tangent (-1.1) sigmoid Sigmoid function – an exponential function (0.1) softmax Softmax – a normalized exponential function – often used for classification (0.1) sine Sine function (-1.1) cosine Cosine function (-1.1) relu "Rectifier" – positive portion is linear – good learning properties in case of deep networks (0, inf) abs Absolute value of the input (0, inf) linear/id Linear identity – simple linear function f(x) = x (-inf, inf) exp A simple exponential function e^(x) (0, inf) logsoftmax Logarithm of softmax – sometimes more efficient than softmax in the calculation (-inf, inf) sign Sign function (-1.1) softplus Sometimes better than relu due to the differentiability (0, inf) softsign Conditionally better convergence behavior than tanh (-1.1) Samples of the ONNX support for MLPs from Pytorch, Keras and Scikit-learn can be found here: ONNX export of an MLP [} 24]. Supported data types TF3800, TF3810 Version: 1.7.0 23
## Page 24

Machine Learning Models and file formats A distinction must be made between "supported datatype" and "preferred datatype". The preferred datatype corresponds to the precision of the execution engine. The preferred datatype is floating point 32 (E_MLLDT_FP32-REAL). When using a supported datatype, an efficient type conversion automatically takes place in the library. Slight losses of performance can occur due to the type conversion. A list of the supported datatypes can be found in ETc Mll Data Type [} 81]. Further comments There are no limits on the software side with regard to the number of layers or the number of neurons. With regard to the calculation duration and memory requirement, the limits of the employed hardware are to be observed. 5.1.1.1 ONNX export of an MLP Download Python samples A Zip archive containing all samples can be found here: Samples of ONNX export [} 63] MLP Regressor with Py Torch import torch import numpy as np input_dim = 3 output_dim = 5 n_samples = 100 dummy_input = np.random.random((n_samples, input_dim)) dummy_output = np.random.random((n_samples, output_dim)) tensor_in = torch. Float Tensor(dummy_input) tensor_out = torch. Float Tensor(dummy_output) train_dataset = torch.utils.data. Tensor Dataset(tensor_in, tensor_out) train_loader = torch.utils.data. Data Loader(train_dataset, shuffle=True, batch_size=32) class MLP_Net(torch.nn. Module): def __init__(self): super().__init__() self.fc1 = torch.nn. Linear(input_dim, 10) self.fc2 = torch.nn. Linear(10, output_dim) def forward(self, x): x = torch.nn.functional.relu(self.fc1(x)) x = torch.sigmoid(self.fc2(x)) return x mlp_net = MLP_Net() optimizer = torch.optim. Adam(mlp_net.parameters(), lr =0.001) n_epochs = 5 for epoch in range(n_epochs): for batch in train_loader: input, output = batch mlp_net.zero_grad() pred_out = mlp_net(input) criterion = torch.nn. MSELoss() loss = criterion(pred_out, output) loss.backward() optimizer.step() onnx_file = 'pytorch_mlp.onnx' tensor_input_size = torch. Float Tensor(np.random.random((1,input_dim))) # First dimension must be 1 torch.onnx.export(mlp_net, tensor_input_size, onnx_file, verbose=True) 24 Version: 1.7.0 TF3800, TF3810
## Page 25

Machine Learning Models and file formats MLP Regressor with Keras import tensorflow as tf import numpy as np import tf2onnx input_dim = 3 output_dim = 5 n_samples = 100 dummy_input = np.random.random((n_samples, input_dim,)) dummy_output = np.random.random((n_samples, output_dim)) model = tf.keras. Sequential() model.add(tf.keras.layers. Dense(5, input_shape = (input_dim,), activation=tf.keras.activations.relu, use_bias = True)) model.add(tf.keras.layers. Dense(output_dim, activation='sigmoid')) model.build() model.summary() learning_rate = 0.001 loss = 'mean_squared_error' optimizer = tf.keras.optimizers. Adamax(lr=learning_rate) model.compile(optimizer=optimizer, loss=loss) model.fit(x=dummy_input, y=dummy_output, batch_size=32 ,epochs=5,verbose=1, shuffle=True) filename = 'tf_keras_mlp' onnx_model, _ = tf2onnx.convert.from_keras(model) with open(filename+'.onnx','wb') as f: f.write(onnx_model. Serialize To String()) TF3800, TF3810 Version: 1.7.0 25
## Page 26

Machine Learning Models and file formats MLP Regressor with Scikit-learn import sklearn.neural_network as skl import numpy as np input_dim = 3 output_dim = 5 n_samples = 100 dummy_input = np.random.random((n_samples,input_dim)) dummy_output = np.random.random((n_samples,output_dim)) model = skl. MLPRegressor(hidden_layer_sizes =(5), activation ='relu') model.fit(dummy_input,dummy_output) filename = 'skl_relu_reg' from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type initial_type = [('float_input',Float Tensor Type([None,input_dim]))] onx = convert_sklearn(model,initial_types=initial_type) with open(filename+'.onnx','wb') as f: f.write(onx. Serialize To String()) 26 Version: 1.7.0 TF3800, TF3810
## Page 27

Machine Learning Models and file formats MLP Classifier with Scikit-learn import sklearn.neural_network as skl import numpy as np import onnx from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type def modify_onnx_MLPClassifier(onnx_MLPCLassifier): """ Function to modify onnx model from MLPClassifier to make it suitable for Twin CAT Machine Lea rning The function removes unsupported nodes and uses the probability estimates of the classes as outp ut for the model. The output of the modified onnx model is the same as the output of the predict_proba() method fr om sklearn.neural_network. MLPClassifier. To get the class as an integer output either a binarization or an argmax function must be applie d to the model output. """ # Delete nodes after last sigmoid/softmax layer output_node_types = {'Sigmoid', 'Softmax'} len_onx_init = len(onnx_MLPCLassifier.graph.node) erased_node_inputs = [] for idx, node in enumerate(reversed(onnx_MLPCLassifier.graph.node)): if node.op_type in output_node_types: idx_last_node = len_onx_init - idx -1 break else: for idx, input in enumerate(node.input): TF3800, TF3810 Version: 1.7.0 27
## Page 28

Machine Learning Models and file formats erased_node_inputs.append(input) onnx_MLPCLassifier.graph.node.remove(node) # Get output dimension and clean initializers second_last_node = onnx_MLPCLassifier.graph.node[idx_last_node-1] for initializer in onnx_MLPCLassifier.graph.initializer: if initializer.name in second_last_node.input: output_dim = initializer.dims[1] if initializer.name in erased_node_inputs: onnx_MLPCLassifier.graph.initializer.remove(initializer) # Erase original outputs and create new output for output in reversed(onnx_MLPCLassifier.graph.output): onnx_MLPCLassifier.graph.output.remove(output) name_new_output = "probabilities" new Output = onnx.helper.make_tensor_value_info(name_new_output, onnx. Tensor Proto. FLOAT, shape=(N one, output_dim)) onnx_MLPCLassifier.graph.output.append(new Output) # Create new node and connect to new output last_node_output = onnx_MLPCLassifier.graph.node[idx_last_node].output new_node = onnx.helper.make_node("Identity", last_node_output, [name_new_output], "Identity_Out" ) onnx_MLPCLassifier.graph.node.append(new_node) return onnx_MLPCLassifier input_dim = 3 output_dim = 2 n_samples = 100 dummy_input = np.random.random((n_samples,input_dim)) dummy_output = np.random.randint(2, size=(n_samples, output_dim)) model = skl. MLPClassifier(activation ='relu', hidden_layer_sizes=(5)) model.fit(dummy_input,dummy_output) filename = 'skl_mlp_clf' initial_type = [('float_input',Float Tensor Type([None,input_dim]))] onx = convert_sklearn(model,initial_types=initial_type, options={type(model): {'zipmap': False}}) onx = modify_onnx_MLPClassifier(onx) with open(filename+'.onnx','wb') as f: f.write(onx. Serialize To String()) 28 Version: 1.7.0 TF3800, TF3810
## Page 29

Machine Learning Models and file formats Observe the function modify_onnx_MLPClassifier. This modifies the lower part of the ONNX graph so that only operators supported by the Twin CAT Neural Network Inference Engine are used. Without this modification, the operators shown here will be generated (depending on the dimensionality of the problem). Only the area with the green border is supported. 5.1.2 Support vector machine A support vector machine (SVM) can be used both for classification and for regression [} 62]. The SVM is a frequently used tool in particular with regard to classification tasks. The fundamental goal of an SVM is to find a hyperplane in an N-dimensional space, wherein the distance between the closest data point and the plane is maximized. A hyperplane can only separate the space linearly (also called linear SVM). A non-linear separation is also possible by means of a so-called kernel trick (also called kernel SVM). The N-dimensional space is transformed into a higher-dimensional space here. A linear separation with a hyperplane is possible in an accordingly higher-dimensional space. If a distinction needs to be made between several classes, several support vector machines are generated internally and classification takes place by means of comparisons. A one-class SVM can also be trained and used for anomaly detection. Supported properties ONNX support The following ONNX operators are supported: • SVMRegressor • SVMClassifier TF3800, TF3810 Version: 1.7.0 29
## Page 30

Machine Learning Models and file formats Supported kernel functions are listed in the following table: Kernel function Description Linear Radial Basis Function (RBF) Sigmoid Polynomial For samples of the export of SVMs as ONNX, see ONNX export of an SVM [} 30]. Classification limitation With classification models, only the output of the labels is mapped in the PLC. The scores/ probabilities are not available in the PLC. Supported data types A distinction must be made between "supported datatype" and "preferred datatype". The preferred datatype corresponds to the precision of the execution engine. The preferred datatype is floating point 64 (E_MLLDT_FP64-LREAL). When using a supported datatype, an efficient type conversion automatically takes place in the library. Slight losses of performance can occur due to the type conversion. A list of the supported datatypes can be found in ETc Mll Data Type [} 81]. 5.1.2.1 ONNX export of a SVM Download Python samples A Zip archive containing all samples can be found here: Samples of ONNX export [} 63] SVM Regressor with Scikit-learn from sklearn.svm import SVR from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type X = [[13,3],[1,16],[1,2]] Y = [1.0,2.0,3.0] model = SVR(kernel='rbf',gamma=10) model.fit(X,Y) out = model.predict(X) input_type = [('float_input', Float Tensor Type([None,len(X[0])]))] onnx_filename = 'svr-rbf.onnx' onx = convert_sklearn(model,initial_types=input_type) with open(onnx_filename,"wb") as f: f.write(onx. Serialize To String()) 30 Version: 1.7.0 TF3800, TF3810
## Page 31

Machine Learning Models and file formats SVM Classifier with Scikit-learn One-class SVM from sklearn.datasets import make_blobs from sklearn.svm import One Class SVM from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type n_samples = 150 input_dim = 3 X, _ = make_blobs(n_samples=n_samples, n_features=input_dim, centers=1, cluster_std=0.3, shuffle=Tru e, random_state=42, ) svm = One Class SVM(kernel='rbf', nu=0.3) svm.fit(X) initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_sklearn(svm, initial_types=initial_type) filename = 'skl_oneclass_svm' with open(filename + '.onnx', "wb") as f: f.write( onnx_model. Serialize To String()) f.close() TF3800, TF3810 Version: 1.7.0 31
## Page 32

Machine Learning Models and file formats Binary classification from sklearn.svm import SVC import numpy as np # random dataset n_samples = 150 input_dim = 4 n_classes = 2 # binary classification rand_in = np.random.random((n_samples, input_dim,)) rand_singleout_multiclass = np.random.randint(n_classes, size=(n_samples, 1)) X = rand_in y = rand_singleout_multiclass # train SVC clr = SVC(kernel='linear', gamma=10) # decision_function_shape option is ignored for binary classifi cation clr.fit(X, y) # Convert into ONNX format from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] # # Zipmap should be always turned off as it's not implemented in TF3800 onx = convert_sklearn(clr, initial_types=initial_type, options={type(clr): {'zipmap':False}}) with open("svc_random.onnx", "wb") as f: f.write(onx. Serialize To String()) 32 Version: 1.7.0 TF3800, TF3810
## Page 33

Machine Learning Models and file formats Multi-class classification from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.svm import SVC # data set iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y) # train SVC, note that decision_function_shape ovo is mandatory clr = SVC(kernel='linear', gamma=10,decision_function_shape='ovo') clr.fit(X_train, y_train) # Convert into ONNX format from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type initial_type = [('float_input', Float Tensor Type([None, 4]))] onx = convert_sklearn(clr, initial_types=initial_type) with open("svc_iris.onnx", "wb") as f: f.write(onx. Serialize To String()) Observe the shape of the ONNX graph! The decision_function_shape “ovo” option must be used with multi-class SVC models so that an ONNX graph that is compatible with TF3800 is generated. Invalid ONNX graph The following example shows the invalid ONNX graph: decision_function_shape “ovr”: TF3800, TF3810 Version: 1.7.0 33
## Page 34

Machine Learning Models and file formats Valid ONNX graph The following example shows the valid ONNX graph: decision_function_shape “ovo”: 34 Version: 1.7.0 TF3800, TF3810
## Page 35

Machine Learning Models and file formats 5.1.3 k-Means The k-Means algorithm is one of the unsupervised learning methods and is used for cluster analysis [} 62]. k-Means attempts to divide a random sample into k-clusters of the same variance; however, the number of clusters k must be known in advance. The algorithm scales well to a large number of samples and is one of the most widely used clustering methods. Unsupervised means that the k-Means does not need to be trained with annotated (labeled) data. This property makes the algorithm very popular. As soon as the training has been executed and the clusters have been defined, new data can be assigned to the already known clusters in the inference. Supported properties ONNX support So far, only export from Scikit-learn is supported. The specification of the ONNX Custom Attributes Key: “sklearn_model” value: “KMeans” is necessary for k-Means models so that the conversion step works in XML and BML. Restriction With classification models, only the output of the labels is mapped in the PLC. The scores/ probabilities are not available in the PLC. An example of the export of an ONNX file from Scikit-learn for use in Twin CAT can be found here: ONNX export of a k-Means [} 35]. Supported data types A distinction must be made between "supported datatype" and "preferred datatype". The preferred datatype corresponds to the precision of the execution engine. The preferred datatype is floating point 64 (E_MLLDT_FP64-LREAL). When using a supported datatype, an efficient type conversion automatically takes place in the library. Slight losses of performance can occur due to the type conversion. A list of the supported datatypes can be found in ETc Mll Data Type [} 81]. 5.1.3.1 ONNX export of a k-Means Download Python samples A Zip archive containing all samples can be found here: Samples of ONNX export [} 63] k-Means with Scikit-learn from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type # Generate data for clustering: num of samples, dimensions, num of clusters X, y, centers = make_blobs(n_samples=50, n_features=3, centers=5, cluster_std=0.5, shuffle=True, ran dom_state=42, return_centers=True) num_features = X.shape[1] num_clusters = centers.shape[0] # Define and train K-Means model km = KMeans(n_clusters=num_clusters, init='k-means+ +', n_init=10, max_iter=500, tol=1e-04, random_state=42) km.fit(X) y_km = km.predict(X) # Export model as ONNX out_onnx = "kmeans.onnx" initial_type = [('float_input', Float Tensor Type([None, num_features]))] TF3800, TF3810 Version: 1.7.0 35
## Page 36

Machine Learning Models and file formats onnx_model = convert_sklearn(km, initial_types=initial_type) # for k-means models this meta info is mandatory. Otherwise convert process to Twin CAT- specific format will fail! meta = onnx_model.metadata_props.add() meta.key = "sklearn_model" meta.value = "KMeans" with open(out_onnx, "wb") as f: f.write( onnx_model. Serialize To String()) According to our level of knowledge, only Scikit-learn with skl2onnx is currently capable of converting a k- Means to ONNX. For this reason, the description is limited to that. ONNX Custom Attributes necessary The specification of the Custom Attribute "sklearn_model" and "KMeans" is necessary for k-means models, so that the conversion step in Beckhoff XML and Beckhoff BML works. 5.1.4 Principal Component Analysis Principal Component Analysis (PCA) calculates so-called principal components with the help of which one can rotate the coordinate system of a given data set in such a way that the variance of the data, i.e. its information content, is maximized along the new principal axes. The covariance matrix of the transformed data is diagonalized and the order of principal components is sorted so that the first principal component carries the largest information portion of the data set, the second then carries the second largest information portion, and so on. The latter principal components often contribute little information to the data set, which means that they can be ignored. As a result, the parameter space is reduced with the least possible loss of information (Dimension reduction [} 62]). The dimension reduction with PCA is often used for preprocessing prior to the cluster analysis or a classification. The PCA can also be used for anomaly detection [} 62] by reducing a large space to a few principal components and determining limit values for the important principal components “by hand”. Supported properties ONNX support 36 Version: 1.7.0 TF3800, TF3810
## Page 37

Machine Learning Models and file formats Supported ONNX operators: • Sub • Mat Mul A sample showing how a PCA for dimension reduction can be exported from Scikit-learn and used in Twin CAT can be found here: ONNX export of a PCA [} 37]. Supported data types A distinction must be made between "supported datatype" and "preferred datatype". The preferred datatype corresponds to the precision of the execution engine. The preferred datatype is floating point 64 (E_MLLDT_FP64-LREAL). When using a supported datatype, an efficient type conversion automatically takes place in the library. Slight losses of performance can occur due to the type conversion. A list of the supported datatypes can be found in ETc Mll Data Type [} 81]. 5.1.4.1 ONNX export of a PCA Download Python samples A Zip archive containing all samples can be found here: Samples of ONNX export [} 63] PCA with Scikit-learn import numpy as np from sklearn.decomposition import PCA from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type, Int64Tensor Type #Generate imput data_types rng = np.random. Random State(1) X = np.dot(rng.rand(4, 3), rng.randn(3, 300)). T # Dimensionality reduction with PCA pca = PCA(n_components=2) pca.fit(X) X_pca = pca.transform(X) print("original shape: ", X.shape) print("transformed shape:", X_pca.shape) #Convert model to ONNX initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] model_onnx = convert_sklearn(pca, initial_types=initial_type) meta = model_onnx.metadata_props.add() with open("pca.onnx", "wb") as f: f.write( model_onnx. Serialize To String()) According to our level of knowledge, only Scikit-learn with skl2onnx is currently capable of converting a PCA to ONNX. For this reason, the description is limited to that. TF3800, TF3810 Version: 1.7.0 37
## Page 38

Machine Learning Models and file formats 5.1.5 Decision Tree A Decision Tree is an ML model that uses a tree-like structure to make predictions. It is a simple, but powerful tool for the prediction of values (regression) or classes (classification) [} 62] on the basis of several inputs, which works by dividing the data into smaller and smaller subsets until a final decision is made. The structure of the tree enables a simple interpretation and visualization of the model. Supported properties ONNX support • Tree Ensamble Classifier • Tree Ensamble Regressor Samples of the export of Decision Tree models can be found here: ONNX export of a Decision Tree [} 38]. Classification limitation With classification models, only the output of the labels is mapped in the PLC. The scores/ probabilities are not available in the PLC. Supported data types A distinction must be made between "supported datatype" and "preferred datatype". The preferred datatype corresponds to the precision of the execution engine. The preferred datatype is floating point 64 (E_MLLDT_FP64-LREAL). When using a supported datatype, an efficient type conversion automatically takes place in the library. Slight losses of performance can occur due to the type conversion. A list of the supported datatypes can be found in ETc Mll Data Type [} 81]. 5.1.5.1 ONNX export of a Decision Tree Download Python samples A Zip archive containing all samples can be found here: Samples of ONNX export [} 63] 38 Version: 1.7.0 TF3800, TF3810
## Page 39

Machine Learning Models and file formats Decision Tree Regressor with Scikit-learn from sklearn.datasets import make_regression from sklearn.tree import Decision Tree Regressor from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type # # Generate data for regression X, y = make_regression(n_samples=300, n_features=10, n_informative=10, n_targets=1) # # Construct Decision Tree-Model model = Decision Tree Regressor(criterion='squared_error', splitter='best', max_depth=None, min_sample s_split=2, min_samples_leaf=1) model.fit(X,y) # # Convert model to ONNX onnxfile = 'decisiontree-regressor.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_sklearn(model, initial_types=initial_type, target_opset=12) # # Export to ONNX file with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() Decision Tree Classifier with Scikit-learn from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import Decision Tree Classifier from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type # # Load dataset X, y = load_iris(return_X_y = True) # # Construct Decision Tree-Model X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) model = Decision Tree Classifier(criterion='gini', splitter='best', max_depth=None, min_samples_split= 2, min_samples_leaf=1) model.fit(X_train,y_train) # # Convert model to ONNX onnxfile = 'decisiontree-classifier.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] # # Zipmap should be always turned off as it's not implemented in TF3800 onnx_model = convert_sklearn(model, initial_types=initial_type, options={type(model): {'zipmap':Fals e}}, target_opset=12) # # Export to ONNX file with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) TF3800, TF3810 Version: 1.7.0 39
## Page 40

Machine Learning Models and file formats f.close() exit() 5.1.6 Extra Tree An Extra Tree is the randomized variant of a Decision Tree [} 38]. It can also be used for the prediction of values (regression) or classes (classification) [} 62]. Supported properties ONNX support • Tree Ensamble Classifier • Tree Ensamble Regressor Examples of the export of Extra Tree models can be found here: ONNX export of an Extra Tree [} 41] Classification limitation With classification models, only the output of the labels is mapped in the PLC. The scores/ probabilities are not available in the PLC. Supported data types A distinction must be made between "supported datatype" and "preferred datatype". The preferred datatype corresponds to the precision of the execution engine. The preferred datatype is floating point 64 (E_MLLDT_FP64-LREAL). When using a supported datatype, an efficient type conversion automatically takes place in the library. Slight losses of performance can occur due to the type conversion. A list of the supported datatypes can be found in ETc Mll Data Type [} 81]. 40 Version: 1.7.0 TF3800, TF3810
## Page 41

Machine Learning Models and file formats 5.1.6.1 ONNX export of an Extra Tree Download Python samples A Zip archive containing all samples can be found here: Samples of ONNX export [} 63] Extra Tree Regressor with Scikit-learn from sklearn.datasets import make_regression from sklearn.tree import Extra Tree Regressor from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type # # Generate data for regression X, y = make_regression(n_samples=300, n_features=10, n_informative=10, n_targets=1) # # Construct Extra Tree-Model model = Extra Tree Regressor(criterion='squared_error', splitter='random', max_depth=None, min_samples _split=2, min_samples_leaf=1) model.fit(X,y) # # Convert model to ONNX onnxfile = 'extratree-regressor.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_sklearn(model, initial_types=initial_type, target_opset=12) # # Export to ONNX file with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() Extra Tree Classifier with Scikit-learn from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import Extra Tree Classifier from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type # # Load dataset X, y = load_iris(return_X_y = True) # # Construct Decision Tree-Model X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) model = Extra Tree Classifier(criterion='gini', splitter='random', max_depth=None, min_samples_split=2 , min_samples_leaf=1) model.fit(X_train,y_train) TF3800, TF3810 Version: 1.7.0 41
## Page 42

Machine Learning Models and file formats # # Convert model to ONNX onnxfile = 'extratree-classifier.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] # # Zipmap should be always turned off as it's not implemented in TF3800 onnx_model = convert_sklearn(model, initial_types=initial_type, options={type(model): {'zipmap':Fals e}}, target_opset=12) # # Export to ONNX file with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() 5.1.7 Ensemble Tree methods Ensemble methods combine several Decision Trees in order to achieve a better prediction performance. The basic principle is to train not just one model (one tree), but several trees – a forest of trees – and to combine the individual results into a common result. There are basically two technologies with which an ensemble of trees can be created. Bagging The bagging methods include: • Random Forest [} 45] • Extra Trees [} 43] Boosting The boosting methods include: • Gradient Boosting [} 48] • Histogram Gradient Boosting [} 50] • XGBoost [} 52] • Light GBM [} 56] 42 Version: 1.7.0 TF3800, TF3810
## Page 43

Machine Learning Models and file formats Unsupported additional Ensemble Tree methods The models Bagging Classifier, Bagging Regressor, Ada Boost Classifier and Ada Boost Regressor are also available in Scikit-learn. During an ONNX export they currently generate a graph that is incompatible with Twin CAT libraries, which means they cannot be supported. 5.1.7.1 Extra Trees Extra Trees creates an ensemble of randomized Decision Trees [} 40]. Each tree is trained to a subset of the data set and the partial results are averaged. This increases the accuracy of the prediction in comparison with the Decision Tree [} 38] and the tendency toward overfitting is reduced. Supported properties ONNX support • Tree Ensamble Classifier • Tree Ensamble Regressor Samples of the export of Extra Trees can be found here: ONNX export of Extra Trees [} 43]. Classification limitation With classification models, only the output of the labels is mapped in the PLC. The scores/ probabilities are not available in the PLC. Supported data types A distinction must be made between "supported datatype" and "preferred datatype". The preferred datatype corresponds to the precision of the execution engine. The preferred datatype is floating point 64 (E_MLLDT_FP64-LREAL). When using a supported datatype, an efficient type conversion automatically takes place in the library. Slight losses of performance can occur due to the type conversion. A list of the supported datatypes can be found in ETc Mll Data Type [} 81]. 5.1.7.1.1 ONNX export of Extra Trees Download Python samples A Zip archive containing all samples can be found here: Samples of ONNX export [} 63] Extra Trees Regressor with Scikit-learn from sklearn.ensemble import Extra Trees Regressor from sklearn.datasets import make_regression from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type # import onnx # # Generate data for regression X, y = make_regression(n_samples=300, n_features=10, n_informative=10, n_targets=1) # # Construct the model model = Extra Trees Regressor(max_depth=None, n_estimators=100) model.fit(X,y) # # Convert model to ONNX onnxfile = 'extratrees-regressor.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_sklearn(model, initial_types=initial_type, target_opset=12) # # Export to ONNX file # onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) TF3800, TF3810 Version: 1.7.0 43
## Page 44

Machine Learning Models and file formats f.close() exit() Extra Trees Classifier with Scikit-learn from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import Extra Trees Classifier from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type # # Load data for classification X, y = load_iris(return_X_y = True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # # Construct Extra Trees-Model X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) model = Extra Trees Classifier(criterion ='entropy', n_estimators=100, max_features=None) model.fit(X_train,y_train) # # Export model into ONNX format onnxfile = 'extratrees-classifier.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] # # Zipmap should be always turned off as it's not implemented in TF3800 onnx_model = convert_sklearn(model, initial_types=initial_type, options={type(model): {'zipmap':Fals e}}) # # Export to ONNX file with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() 44 Version: 1.7.0 TF3800, TF3810
## Page 45

Machine Learning Models and file formats 5.1.7.2 Random Forest A Random Forest can be used both for classification and for regression [} 62]. The algorithm belongs to the ensemble methods, since a user-defined number of uncorrelated decision trees is built and trained. In the Random Forest, the prediction of the ensemble results from the averaged prediction of the individual trees. Compared to individual Decision Trees [} 38], a Random Forest often has a better accuracy at the cost of the Random Forest is not being transparent with regard to the predictions made. Compared to an SVM, a Random Forest is more efficient in terms of computing time, especially for high- dimensional data. Supported properties ONNX support • Tree Ensamble Classifier • Tree Ensamble Regressor Samples of the export of Random Forest models can be found here: ONNX export of a Random Forest [} 46] Classification limitation With classification models, only the output of the labels is mapped in the PLC. The scores/ probabilities are not available in the PLC. Supported data types A distinction must be made between "supported datatype" and "preferred datatype". The preferred datatype corresponds to the precision of the execution engine. The preferred datatype is floating point 64 (E_MLLDT_FP64-LREAL). When using a supported datatype, an efficient type conversion automatically takes place in the library. Slight losses of performance can occur due to the type conversion. A list of the supported datatypes can be found in ETc Mll Data Type [} 81]. TF3800, TF3810 Version: 1.7.0 45
## Page 46

Machine Learning Models and file formats 5.1.7.2.1 ONNX export of a Random Forest Download Python samples A Zip archive containing all samples can be found here: Samples of ONNX export [} 63] Scikit-learn: Random Forest Classifier from sklearn.ensemble import Random Forest Classifier from sklearn.datasets import make_classification from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type X, y = make_classification(n_samples=1000, n_features=3, n_informative=3, n_redundant=0, random_stat e=1) clf = Random Forest Classifier(n_estimators=100) clf.fit(X, y) initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] # # Zipmap should be always turned off as it's not implemented in TF3800 onnx_model = convert_sklearn(clf, initial_types=initial_type, options={type(clf): {'zipmap':False}}) with open("rf_classifier.onnx", "wb") as f: f.write( onnx_model. Serialize To String()) exit() Scikit-learn: Random Forest Regressor from sklearn.ensemble import Random Forest Regressor from sklearn.datasets import make_regression from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type X, y = make_regression(n_samples=1000, n_features=5, n_informative=5, random_state=2) clf = Random Forest Regressor(n_estimators=100) clf.fit(X, y) initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_sklearn(clf, initial_types=initial_type) with open("rf_regressor.onnx", "wb") as f: f.write( onnx_model. Serialize To String()) 46 Version: 1.7.0 TF3800, TF3810
## Page 47

Machine Learning Models and file formats Light GBM: Random Forest Regressor import numpy as np import onnx_graphsurgeon as gs import lightgbm as lgb from lightgbm import LGBMRegressor from skl2onnx.common.data_types import Float Tensor Type from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from onnxmltools.convert import convert_lightgbm import onnxmltools.convert.common.data_types import onnx # # Generate data for regression X, y = make_regression(n_samples=300, n_features=10, n_informative=10, n_targets=1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # # Construct Light GBM-Random Forest-Model model = LGBMRegressor(boosting_type='rf', class_weight=None, colsample_bynode=0.3, colsample_bytree= 1.0, importance_type='split', learning_rate=0.05, max_depth=-1, min_child_samples=2, min_child_weigh t=0.001, min_split_gain=0.0, n_estimators=150, n_jobs=-1, num_class=1, num_leaves=500, objective='re gression', random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=0.632, subsample _for_bin=200000, subsample_freq=1) model.fit(X_train, y_train,eval_set=[(X_test,y_test), (X_train,y_train)],eval_metric='rmse', verbose=20) # # Convert model to ONNX onnxfile = 'lgbm-regressor-randomforest.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_lightgbm(model, initial_types=initial_type, target_opset=12) # Manipulate ONNX graph # # Import model to graph object graph = gs.import_onnx(onnx_model) graph.name = "LGBM-Random Forest" # # Modify Tree Ensemble output shape (necessary to meet Twin CAT requirement, working on an update to make this step obsolete) tree_node = [node for node in graph.nodes if node.op == "Tree Ensemble Regressor"][0] tree_node.outputs[0].shape = [None, 1] tree_node.outputs[0].dtype = np.float32 # # Modify DIV Node inputs to provide correct averaging (necessary to correct a bug in onnxmltools v ersion 1.11.1) div_node = [node for node in graph.nodes if node.op == "Div"][0] div_node.inputs[1].to_constant(values=np.asarray([[model.n_estimators]], dtype=np.float32)) # # Export graph object to ONNX Proto Model graph.cleanup().toposort() onnx_model = gs.export_onnx(graph) # # Add ONNX domain tag to Tree Ensemble Node for proper node recognition (only a reset of the tag as it gets lost during onnx manipulation) TF3800, TF3810 Version: 1.7.0 47
## Page 48

Machine Learning Models and file formats tree_node = [node for node in onnx_model.graph.node if node.op_type == "Tree Ensemble Regressor"][0] tree_node.domain = "ai.onnx.ml" tree_node.doc_string = "Converted from LGBMRegressor() model with explicit shaping" # # Export ONNX model to file onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() 5.1.7.3 Gradient Boosting A Gradient Boosting model can be used both for classification and for regression [} 62]. Like the Random Forest [} 45], for example, the model is one of the Ensemble Tree [} 42] methods. Compared to an individual Decision Tree, the accuracy of the prediction can be improved with the Gradient Boosting model at the cost of the model no longer being simple to explain. Random Forest and Gradient Boosting differ from each other in the way the individual trees are generated. Supported properties ONNX support • Tree Ensamble Classifier • Tree Ensamble Regressor Samples of the export of Gradient Boosting models can be found here: ONNX export of Gradient Boosting [} 49]. Classification limitation With classification models, only the output of the labels is mapped in the PLC. The scores/ probabilities are not available in the PLC. Supported data types A distinction must be made between "supported datatype" and "preferred datatype". The preferred datatype corresponds to the precision of the execution engine. 48 Version: 1.7.0 TF3800, TF3810
## Page 49

Machine Learning Models and file formats The preferred datatype is floating point 64 (E_MLLDT_FP64-LREAL). When using a supported datatype, an efficient type conversion automatically takes place in the library. Slight losses of performance can occur due to the type conversion. A list of the supported datatypes can be found in ETc Mll Data Type [} 81]. 5.1.7.3.1 ONNX export of Gradient Boosting Download Python samples A Zip archive containing all samples can be found here: Samples of ONNX export [} 63] Gradient Boosting Regressor with Scikit-learn from sklearn.ensemble import Gradient Boosting Regressor from sklearn.datasets import make_regression from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type # import onnx # # Generate data for regression X, y = make_regression(n_samples=300, n_features=10, n_informative=10, n_targets=1) # # Construct the model model = Gradient Boosting Regressor(learning_rate=0.08, max_depth=3, n_estimators=300) model.fit(X,y) # # Convert model to ONNX onnxfile = 'gdb-regressor.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_sklearn(model, initial_types=initial_type, target_opset=12) # # Export to ONNX file # onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() Gradient Boosting Classifier with Scikit-learn from sklearn.ensemble import Gradient Boosting Classifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type # import onnx # # Load data for classification X, y = load_iris(return_X_y = True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) model = Gradient Boosting Classifier(n_estimators=100, learning_rate=0.1, max_depth=3) TF3800, TF3810 Version: 1.7.0 49
## Page 50

Machine Learning Models and file formats model.fit(X_train,y_train) # # Convert model to ONNX onnxfile = 'gdb-classifier.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] # # Zipmap should be always turned off as it's not implemented in TF3800 onnx_model = convert_sklearn(model, initial_types=initial_type, options={type(model): {'zipmap':Fals e}}, target_opset=12) # # Export to ONNX file # onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() 5.1.7.4 Histogram-based Gradient Boosting A histogram-based Gradient Boosting model can be used both for classification and for regression [} 62]. The model is based on the Gradient Boosting [} 48]; here, however, the continual inputs are discretized in bins with the help of a histogram. This hugely accelerates the training of the model, in particular with very large data sets. Supported properties ONNX support • Tree Ensamble Classifier • Tree Ensamble Regressor Samples of the export of Hist Gradient Boosting models can be found here: ONNX export of Hist Gradient Boosting [} 51]. Classification limitation With classification models, only the output of the labels is mapped in the PLC. The scores/ probabilities are not available in the PLC. Supported data types A distinction must be made between "supported datatype" and "preferred datatype". The preferred datatype corresponds to the precision of the execution engine. 50 Version: 1.7.0 TF3800, TF3810
## Page 51

Machine Learning Models and file formats The preferred datatype is floating point 64 (E_MLLDT_FP64-LREAL). When using a supported datatype, an efficient type conversion automatically takes place in the library. Slight losses of performance can occur due to the type conversion. A list of the supported datatypes can be found in ETc Mll Data Type [} 81]. 5.1.7.4.1 ONNX export of Hist Gradient Boosting Download Python samples A Zip archive containing all samples can be found here: Samples of ONNX export [} 63] Hist Gradient Boosting Regressor with Scikit-learn from sklearn.ensemble import Hist Gradient Boosting Regressor from sklearn.datasets import make_regression from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type # import onnx # # Generate data for regression X, y = make_regression(n_samples=300, n_features=10, n_informative=10, n_targets=1) # # Construct the model model = Hist Gradient Boosting Regressor(learning_rate=0.08, max_depth=3, max_iter=300) model.fit(X,y) # # Convert model to ONNX onnxfile = 'histgdb-regressor.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_sklearn(model, initial_types=initial_type, target_opset=12) # # Export to ONNX file # onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() Hist Gradient Boosting Classifier with Scikit-learn from sklearn.ensemble import Hist Gradient Boosting Classifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split TF3800, TF3810 Version: 1.7.0 51
## Page 52

Machine Learning Models and file formats from skl2onnx import convert_sklearn from skl2onnx.common.data_types import Float Tensor Type # import onnx # # Load data for classification X, y = load_iris(return_X_y = True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # # Construct the model model = Hist Gradient Boosting Classifier(learning_rate=0.08, max_depth=3, max_iter=300) model.fit(X_train,y_train) # # Convert model to ONNX onnxfile = 'histgdb-iris.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] # # Zipmap should be always turned off as it's not implemented in TF3800 onnx_model = convert_sklearn(model, initial_types=initial_type, options={type(model): {'zipmap':Fals e}}, target_opset=12) # # Export to ONNX file # onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() 5.1.7.5 XGBoost An XGBoost model can be used both for classification and for regression [} 62]. Compared to Gradient Boosting [} 48], the XGBoost has advantages with regard to the generalization of the model. The training data set should be large – considerably more samples compared to the number of features used. Supported properties ONNX support • Tree Ensamble Classifier • Tree Ensamble Regressor Samples of the export of XGBoost models can be found here: ONNX export of XGBoost [} 53] 52 Version: 1.7.0 TF3800, TF3810
## Page 53

Machine Learning Models and file formats Classification limitation With classification models, only the output of the labels is mapped in the PLC. The scores/ probabilities are not available in the PLC. Supported data types A distinction must be made between "supported datatype" and "preferred datatype". The preferred datatype corresponds to the precision of the execution engine. The preferred datatype is floating point 64 (E_MLLDT_FP64-LREAL). When using a supported datatype, an efficient type conversion automatically takes place in the library. Slight losses of performance can occur due to the type conversion. A list of the supported datatypes can be found in ETc Mll Data Type [} 81]. 5.1.7.5.1 ONNX export of XGBoost Download Python samples A Zip archive containing all samples can be found here: Samples of ONNX export [} 63] Limitation of the version of XGBoost The supported versions of XGBoost are limited due to the ONNX export behavior: version <= 1.5.2 or >= 1.7.4. XGB Regressor # # Important Requirement: XGBoost version must be 1.7.4 or higher (otherwise onnxmltools 1.11.1 does not match) import xgboost as xgb from xgboost import XGBRegressor from sklearn.datasets import make_regression from skl2onnx.common.data_types import Float Tensor Type from onnxmltools.convert import convert_xgboost import onnx # # Generate data for regression X, y = make_regression(n_samples=300, n_features=10, n_informative=10, n_targets=1) # # Construct XGB-Model model = XGBRegressor(objective='reg:squarederror',booster='gbtree', max_depth=3, learning_rate=0.08, n_estimators=300) model.fit(X,y) # # Convert model to ONNX onnxfile = 'xgb-regressor.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_xgboost(model, initial_types=initial_type, target_opset=12) onnx_model.graph.doc_string = "Converted from XGBoost ver."+xgb.__version__ # # Export to ONNX file onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() TF3800, TF3810 Version: 1.7.0 53
## Page 54

Machine Learning Models and file formats XGB Classifier import onnx from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import xgboost as xgb # # Important Requirement: XGBoost version must be 1.7.4 or higher (otherwise onnxmltools 1.11.1 does not match) from xgboost import XGBClassifier from skl2onnx.common.data_types import Float Tensor Type from onnxmltools.convert import convert_xgboost X, y = load_iris(return_X_y = True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) model = XGBClassifier(objective= 'multi:softmax', learning_rate=0.03, max_depth=3, n_estimators=300, eval_metric='mlogloss', early_stopping_rounds=20, verbosity=1, use_label_encoder=False) model.fit(X_train,y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=True) onnxfile = 'xgb-iris.onnx' # # Convert model to ONNX initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_xgboost(model, initial_types=initial_type, target_opset=12) onnx_model.graph.doc_string = "Converted from XGBoost ver."+xgb.__version__ # # Export to ONNX file onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() 54 Version: 1.7.0 TF3800, TF3810
## Page 55

Machine Learning Models and file formats XGB binary classifier from skl2onnx.common.data_types import Float Tensor Type from sklearn.model_selection import train_test_split from sklearn.datasets import make_moons, make_circles, make_classification import onnx # # Important Requirement: XGBoost version must be 1.7.4 or higher (otherwise onnxmltools 1.11.1 does not match) import xgboost as xgb from xgboost import XGBClassifier from onnxmltools.convert import convert_xgboost # # Generate data for binary classification X, y = make_moons(n_samples=300, noise=0.3, random_state=1) # X, y = make_circles(n_samples=300, shuffle=True, noise=0.3, random_state=1, factor=0.8) # # Construct XGB-Model X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) model = XGBClassifier(objective= 'binary:logistic', learning_rate=0.03, max_depth=3, n_estimators=30 0, eval_metric='auc', early_stopping_rounds=20, verbosity=1, use_label_encoder=False) model.fit(X_train,y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=True) # # Convert model to ONNX onnxfile = 'xgb-binary.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_xgboost(model, initial_types=initial_type, target_opset=12) onnx_model.graph.doc_string = "Converted from XGBoost ver."+xgb.__version__ # # Export to ONNX file onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() TF3800, TF3810 Version: 1.7.0 55
## Page 56

Machine Learning Models and file formats 5.1.7.6 Light GBM A Light GBM model can be used both for classification and for regression [} 62]. Light GBM is one of the histogram-based Gradient Boosting [} 50] methods. This makes training efficient, in particular with large data sets. Supported properties ONNX support • Tree Ensamble Classifier • Tree Ensamble Regressor Samples of the export of Light GBM models can be found here: ONNX export of Light GBM [} 56]. Classification limitation With classification models, only the output of the labels is mapped in the PLC. The scores/ probabilities are not available in the PLC. Supported data types A distinction must be made between "supported datatype" and "preferred datatype". The preferred datatype corresponds to the precision of the execution engine. The preferred datatype is floating point 64 (E_MLLDT_FP64-LREAL). When using a supported datatype, an efficient type conversion automatically takes place in the library. Slight losses of performance can occur due to the type conversion. A list of the supported datatypes can be found in ETc Mll Data Type [} 81]. 5.1.7.6.1 ONNX export of Light GBM Download Python samples A Zip archive containing all samples can be found here: Samples of ONNX export [} 63] 56 Version: 1.7.0 TF3800, TF3810
## Page 57

Machine Learning Models and file formats LGBM Regressor from lightgbm import LGBMRegressor from skl2onnx.common.data_types import Float Tensor Type from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from onnxmltools.convert import convert_lightgbm import onnx # # Generate data for regression X, y = make_regression(n_samples=300, n_features=10, n_informative=10, n_targets=1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # # Construct Light GBM-Model model = LGBMRegressor(objective='regression', max_depth=31, learning_rate=0.05, n_estimators=300) model.fit(X_train, y_train,eval_set=[(X_test,y_test), (X_train,y_train)],eval_metric='rmse', verbose=20) # # Convert model to ONNX onnxfile = 'lgbm-regressor.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_lightgbm(model, initial_types=initial_type, target_opset=12) onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() LGBM Regressor (gamma objective) import numpy as np from lightgbm import LGBMRegressor from skl2onnx.common.data_types import Float Tensor Type from sklearn.model_selection import train_test_split from onnxmltools.convert import convert_lightgbm import onnx # # Generate data for regression N_ROWS = 1000 N_COLS = 4 X = np.random.randn(N_ROWS, N_COLS) # # For 'poisson' and 'gamma' objective, all target values need to be non-negative y = abs(np.random.randn(N_ROWS)) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # # Construct Light GBM-Model model = LGBMRegressor(objective='gamma', max_depth=-1, learning_rate=0.05, n_estimators=300) model.fit(X_train, y_train,eval_set=[(X_test,y_test), (X_train,y_train)],eval_metric='rmse', verbose=20) TF3800, TF3810 Version: 1.7.0 57
## Page 58

Machine Learning Models and file formats # # Convert model to ONNX onnxfile = 'lgbm-regressor-gamma.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_lightgbm(model, initial_types=initial_type, target_opset=12) onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() LGBM Regressor (poisson objective) import numpy as np from lightgbm import LGBMRegressor from skl2onnx.common.data_types import Float Tensor Type from sklearn.model_selection import train_test_split from onnxmltools.convert import convert_lightgbm import onnx # # Generate data for regression N_ROWS = 1000 N_COLS = 4 X = np.random.randn(N_ROWS, N_COLS) # # For 'poisson' and 'gamma' objective, all target values need to be non-negative y = abs(np.random.randn(N_ROWS)) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # # Construct Light GBM-Model model = LGBMRegressor(objective='poisson', max_depth=-1, learning_rate=0.05, n_estimators=300) model.fit(X_train, y_train,eval_set=[(X_test,y_test), (X_train,y_train)],eval_metric='rmse', verbose=20) # # Convert model to ONNX onnxfile = 'lgbm-regressor-poisson.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] onnx_model = convert_lightgbm(model, initial_types=initial_type, target_opset=12) onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() 58 Version: 1.7.0 TF3800, TF3810
## Page 59

Machine Learning Models and file formats LGBM Classifier import onnx import onnx_graphsurgeon as gs from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from lightgbm import LGBMClassifier from skl2onnx.common.data_types import Float Tensor Type from onnxmltools.convert import convert_lightgbm # # Load data for classification X, y = load_iris(return_X_y = True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # # Construct Light GBM Model model = LGBMClassifier(objective='multiclass', learning_rate=0.05, max_depth=-1, n_estimators=100, r andom_state=42) model.fit(X_train,y_train,eval_set=[(X_test,y_test), (X_train,y_train)],verbose=20,eval_metric='logloss') # # Convert model to ONNX onnxfile = 'lgbm-iris.onnx' initial_type = [('float_input', Float Tensor Type([None, X.shape[1]]))] # # Zipmap should be always turned off as it's not implemented in TF3800 onnx_model = convert_lightgbm(model, initial_types=initial_type, zipmap=False) # # Manipulate graph to force its ONNX conformity (necessary to correct a bug in onnxmltools version 1.11.1) graph = gs.import_onnx(onnx_model) graph.name = "LGBMClassifier" tree_node = [node for node in graph.nodes if node.op == "Tree Ensemble Classifier"][0] tree_node.name = "Tree Ensemble Classifier" tree_node.outputs = graph.outputs tree_node.outputs[0].shape = [None] tree_node.outputs[1].shape = [None, model.n_classes_] # # Collect 2 artifacts of the converter cast_node = [node for node in graph.nodes if node.op == "Cast"][0] mul_node = [node for node in graph.nodes if node.op == "Mul"][0] # # Clear outputs of these two nodes mul_node.outputs.clear() cast_node.outputs.clear() # # Remove these nodes from the graph graph.cleanup().toposort() onnx_model = gs.export_onnx(graph) nodes = onnx_model.graph.node for node in nodes: # # Modify node attributes. TF3800, TF3810 Version: 1.7.0 59
## Page 60

Machine Learning Models and file formats if node.op_type == "Tree Ensemble Classifier": node.domain = "ai.onnx.ml" # # Domain info is required for ONNX conformity node.doc_string = "Converted from LGBMClassifier() model" # # Export to ONNX file onnx.checker.check_model(onnx_model) with open(onnxfile, "wb") as f: f.write( onnx_model. Serialize To String()) f.close() exit() 5.2 Machine Learning Cheat Sheet: selection of models Which of the supported models is suitable for my problem? This question is frequently asked. The following information is intended to assist you in selecting suitable algorithms. Type of input data of the model The first essential question concerns the type of input data of the model: image data, time series or tabular data? The supported models are mainly suitable for tabular data. This means that the input of the model forms an array of values. Image data Convolutional Neural Networks (CNNs) are usually used for the direct processing of image data. These are expected to be supported starting from Q3/2023. MLPs can also deliver adequate results for a restricted application area. For this purpose, an image pixel is input into the model as a vector. In addition, it is expedient to extract features from the image data first and to use these features as the input data of an AI model. Twin CAT Vision provides a powerful library for image capture, preprocessing and feature generation. The features can then be combined as an array and the problem interpreted as a tabular data problem. Time series 60 Version: 1.7.0 TF3800, TF3810
## Page 61

Machine Learning Models and file formats Recurrent neural networks such as the LSTM are usually used for the direct processing of time series, i.e. series of data points in which the temporal sequence of the samples carries essential information. These are expected to be supported starting from Q3/2023. MLPs can also deliver adequate results for a restricted application area. N-samples are input into the model as vectors. In addition, it is expedient to extract signal features from the time series and to use these features as the input data of an AI model. PLC libraries such as the Condition Monitoring library or the Analytics library are suitable for the feature generation of time series. In practice it has proven to be very efficient, for example, to form static variables over a defined time segment, such as mean value, standard deviation, maximum and minimum value, etc. The time segment might be the length of a process step, for example from the start to the end of a cut, or from the start to the end of a bending process. In addition to static variables, frequency- based features such as the signal power in defined frequency bands have proven to be useful, especially with rotary processes. The generated features are then combined in an array and used as the input for the AI model. Accordingly, the problem can be interpreted as a tabular data problem. Tabular Data Tabular Data can be used directly as the input for most AI models. Situations where an array of input data are directly available could be: the length, width and mass as well as its optical components in R, G and B values are measured with different measuring instruments. The values can be directly combined as an array of 6 elements and used as the input for an AI model – for example, for classification as OK or NOK. Description of the goal Once the type of input data has been defined, the question arises as to what exactly the AI should do, or what it can do under given conditions. Do annotated (labeled) data exist and what type are the labels? Clustering In clustering, the inner structure of the input data are analyzed. No annotated data are necessary for the cluster analysis; however, the number k of the expected clusters must be known. Anomaly detection A popular application, likewise for the case that no annotated data exist. In the training phase, only data that can be described as “normal” are presented to the model. In the inference phase, the model can distinguish between a known input data structure and an unknown input data structure. In the latter case an anomaly is assumed. The challenge in anomaly detection is the preprocessing of the training data, so that if possible only the normal case is used in the training, as well as the limited meaningfulness of the result. Dimensionality reduction Human beings are good at visualizing the 2- and 3-dimensional space. Point clouds in a 3D plot are easy to handle and can improve the understanding of processes. However, it quickly becomes confusing if several dimensions are involved. The purpose of dimension reduction is to map an N-dimensional input vector to a smaller vector while losing the least possible amount of information: for example, a 10-dimensional input is reduced to 3 dimensions while retaining 95% of the information. Redundant information of the input data is exploited. The dimension reduction is well suited for use as a feature generation step, e.g. before a classifier. Regression A regression problem requires the existence of an annotated data set. As a rule, a problem is described with N REAL or LREAL as the input of the model and M REAL or LREAL as the output. Example: N features are created during a forming process (e.g.: maximum, standard deviation, skew of the servo motor current). For each of these features, the resulting diameter of the formed product in the longitudinal and transverse direction is known. From the 3 features, 2 values are estimated accordingly. If the curve of a time series is to be modeled, the N past time values can be used as the input vector and the N+1 value as the label. The manual labeling of the data is thus unnecessary. Classification A classification problem requires an annotated data set. As a rule, N REAL or LREAL values are mapped here to a category that is usually represented as INT in Twin CAT. For example, whether a finished product corresponds to quality class A, B or C is calculated from N features. TF3800, TF3810 Version: 1.7.0 61
## Page 62

Machine Learning Models and file formats Explainability of an AI model In some situations it is of great importance to be able to explain the results of an AI model, i.e. to answer the question, for example, as to why a model has classified a product as defective. Unfortunately, most algorithms work like black boxes, and the results can only be explained with difficulty if at all – even if they are very precise. Decision Trees are models that can be explained very well, because the path through the tree can be retraced with the individual limit values of the branches. However, the accuracy of these models is often not as convincing as with other models that cannot be explained. AI model Cheat Sheet The following figure provides a simple guide to the selection of a suitable AI model. It illustrates the classification of AI models for different application purposes, provided the model input is tabular data. 5.3 Creation and conversion of ONNX The learning process of a Machine Learning model takes place outside of the real-time, usually in a script language such as Python or R. The learned model is to be exported from the selected ML framework as an ONNX file. In order to load the description of the model in the Twin CAT runtime, the ONNX file must first be converted into a proprietary, Twin CAT 3-specific format. An overview of the description formats for machine learning models: • Open Neural Network Exchange Format (ONNX) [} 62] • Beckhoff ML XML [} 74] • Beckhoff ML BML [} 76] Beckhoff's proprietary formats in XML and BML are directly readable from the Machine Learning Runtime. The ONNX data format must be converted to a Beckhoff proprietary format using provided converters [} 65]. Whereas ONNX and XML are openly visible formats, BML is a binary format and thus characterized above all by a small file size and an efficient loading behavior (execution time of the Configure method) in the XAR. 5.3.1 Open Neural Network Exchange (ONNX) What is ONNX? ONNX is an open file format for the representation of Machine Learning Models and is managed as a community project. Homepage of the ONNX community: onnx.ai 62 Version: 1.7.0 TF3800, TF3810
## Page 63

Machine Learning Models and file formats The ONNX format defines groups of operators in a standardized format, allowing learned models to be used interoperably with various frameworks, runtimes and further tools. ONNX supports descriptions of neural networks as well as classic machine learning algorithms and is therefore the suitable format for both the Twin CAT Machine Learning Inference Engine and the Twin CAT Neural Network Inference Engine. Why ONNX? Through support for ONNX, Beckhoff integrates the Twin CAT Machine Learning products in an open manner and thus guarantees flexible workflows. While the automation specialist can work in Twin CAT 3, the data scientist can work with his usual tools (Py Torch, Scikit-Learn, ...). The use of ONNX facilitates cross-workgroup working, both internally and cross-company with partners. The automation specialist provides the data scientist with recorded data. The data scientist creates an ML model and hands over his work as an ONNX file to the automation specialist. This file already contains all information to execute the created model in Twin CAT. The offline testing of models is simplified, because all common AI frameworks can load and also execute the ONNX file. Which software supports ONNX? Supported tools of the ONNX community can be viewed here: onnx.ai/supported-tools. Including, for example, the frameworks: • Py Torch • Keras/Tensor Flow • MXNet • Scikit-learn • … Graph Optimizer • ONNX Optimizer (https://github.com/onnx/onnx/blob/enable_noexpception_build/docs/Optimizer.md) Graph Visualizer • Netron (https://github.com/lutzroeder/Netron) • … 5.3.2 Samples of ONNX export How do I create ONNX files? Below, several ways of exporting certain models as ONNX from different frameworks are shown using examples. The samples do not claim to be complete and only serve to provide a primary overview. For more detailed documentation, refer to the documentation for the respective framework. The listed examples are limited to the creation of an ONNX file. Examples for conversion to make the file available in Twin CAT can be found here: Converting ONNX to XML and BML [} 65] as well as in the ZIP archive for the linked samples (see below) in the Python API_mllib folder. TF3800, TF3810 Version: 1.7.0 63
## Page 64

Machine Learning Models and file formats Overview of available samples Python package Model type Option Comment Sample link Py Torch MLP Regressor Go To Page [} 24] Keras MLP Regressor Go To Page [} 25] Scikit-learn MLP Regressor Go To Page [} 26] Scikit-learn MLP Classifier ONNX graph must Go To Page [} 27] be adapted Scikit-learn SVR Go To Page [} 30] Scikit-learn SVC decision_function_s Go To Page [} 31] hape='ovo' Scikit-learn k-means Meta Key must be Go To Page [} 35] entered in ONNX. Scikit-learn PCA Go To Page [} 37] Scikit-learn Decision Tree Go To Page [} 39] Classifier Scikit-learn Decision Tree Go To Page [} 39] Regressor Scikit-learn Extra Tree Classifier Go To Page [} 41] Scikit-learn Extra Tree Go To Page [} 41] Regressor Scikit-learn Extra Trees Go To Page [} 44] Classifier Scikit-learn Extra Trees Go To Page [} 43] Regressor Scikit-learn Random Forest Go To Page [} 46] Classifier Scikit-learn Random Forest Go To Page [} 46] Regressor Light GBM Random Forest ONNX graph must Go To Page [} 47] Regressor be adapted Scikit-learn Gradient Boosting Go To Page [} 49] Classifier Scikit-learn Gradient Boosting Go To Page [} 49] Regressor Scikit-learn Hist Gradient Go To Page [} 51] Boosting Classifier Scikit-learn Hist Gradient Go To Page [} 51] Boosting Regressor XGBoost XGBClassifier Not all Package version <= Go To Page [} 54] configurations allow 1.5.2 or >= 1.7.4 an ONNX export required XGBoost XCBRegressor Not all Package version <= Go To Page [} 53] configurations allow 1.5.2 or >= 1.7.4 an ONNX export required Light GBM LGBMRegressor Not all Go To Page [} 57] configurations allow an ONNX export Light GBM LGBMClassifier ONNX graph must Go To Page [} 59] be adapted All samples can be downloaded here as a ZIP archive: https://infosys.beckhoff.com/content/1033/ tf38x0_tc3_ML_NN_Inference_Engine/Resources/13668699915.zip 64 Version: 1.7.0 TF3800, TF3810
## Page 65

Machine Learning Models and file formats 5.3.3 Conversion from ONNX to XML and BML The method for converting one or more ONNX files to the Beckhoff specific XML or BML format is described below. Why convert ONNX to XML or BML? The ONNX format cannot be directly loaded to the target platform by the ML Runtime. Therefore, a conversion step is necessary in the engineering before the ML description file can be transferred to the target system. The creation of an XML file has the advantage that it is openly readable. This allows all information in the XML to be viewed during the engineering phase using a simple XML editor. The BML file is binary and the contents are therefore no longer simply viewable. In addition, a BML file is loaded considerably faster by the ML Runtime, therefore the BML file is recommended for the delivery. Which Twin CAT-specific information can be entered in the XML or BML file? Creation of multi-engines It is possible to load more than one ML model into an instance of the FB_Mll Prediction [} 82]. Switching between the models, referred to in the following as engines in this context, is possible without latency. This way, for example, a set of models can be trained for different work areas of the machine and the correct engine can be addressed without latency during the inference phase. The condition for merging several models is that the model structure – the <Configuration> [} 75] section in Beckhoff XML – is identical for all models. For example, this means that MLPs can only be merged if their structure (number of layers, number of neurons per layer and activation functions) is the same. Only the model parameters, i.e. the weights in the example of the MLPs, may be different. Cf. the Beckhoff ML XML description: XML Tag Parameters [} 75]. During merging, therefore, several AI models with an identical structure but different parameters are combined in a description file. The individual models (= engines) are loaded via a single description file in the ML Runtime. The engine ID that is to be addressed is to be transferred in each case to the Predict method [} 90] when calling in the PLC. The engine can be addressed via a string using the Predict Ref method [} 91]. A Get Engine Id From Ref method [} 88] is also available for finding the associated ID from the reference. Multi-engines should be regarded as an organization unit. Of course, it is also possible to instantiate several instances of a FB_Mll Prediction [} 82] in the PLC and to load a dedicated description file into each FB. Minimum version of the ML Runtime driver When converting the description file, two entries are automatically set in the XML or BML file. One is the version of the converting component and the other is a “required version” of the ML Runtime driver. On loading the model file into Twin CAT, the “required version” is checked and a warning is output if the result of the query is false. Cf. the Beckhoff ML XML description: XML Tag Auxiliary Specifications [} 74]. Which application-specific information can be entered in the XML or BML file? Input and output scalings The inputs of the AI model are often scaled for the training process. This scaling must then also be performed for the inference. This can either be implemented by hand in the PLC or entered directly as information in the XML or BML file. If the scaling entries are set, the scaling is performed automatically. A scaling and an offset must be specified for the scaling. The following applies: y = x * Scaling + Offset If scaled inputs are used for a model, back-scaling of the model output is usually also necessary. Therefore, an output scaling is available in addition to the input scaling. Output transformation for selected models only Whereas an input transformation is possible for all AI models, output transformations can only be used for models of the regression type. TF3800, TF3810 Version: 1.7.0 65
## Page 66

Machine Learning Models and file formats Cf. the Beckhoff ML XML description: XML Tag Auxiliary Specifications [} 74]. Model name, model version, model description, etc. For the unambiguous identification of a model description file, it is possible to add various descriptions of a model. These can be used as a free string: • A model version • A model name • A model description • A model author • Further optional tags Cf. the Beckhoff ML XML description: XML Tag Auxiliary Specifications [} 74]. Free Custom Attributes section Custom Attributes are optional and may be freely used by the user. The number of attributes and the number of XML tags are not limited. The attributes are typed in the XML/BML so that the entries can be read again in the PLC. The methods Get Custom Attribute_array, Get Custom Attribute_fp64, Get Custom Attribute_int64 and Get Custom Attribute_str are available for this. See also Detailed sample [} 94]. Examples of the use of Custom Attributes could be: • Specification of an internal version or identification number • Specification of the input range of the individual features • Description of the inputs and outputs • … Cf. the Beckhoff ML XML description: XML Tag Custom Attributes [} 74]. How can I convert files and add information? Different interfaces are offered for the simple conversion and information modeling step in your work process: • A GUI in the Twin CAT XAE “Machine Learning Model Manager [} 66]” • A CLI “mllib_toolbox.exe [} 69]” • An API as a Python package “beckhoff.toolbox [} 72]” Not every interface offers the actions described above. The CLI is limited to basic applications – mainly conversion. The Python API and the GUI offer the largest functional scope. 5.3.3.1 GUI The Twin CAT 3 Machine Learning Model Manager is the central UI for editing ONNX files. The tool is integrated in Visual Studio and can be opened via the menu bar under Twin CAT > Machine Learning > Machine Learning Model Manager. Required Visual Studio version The graphic interface of the Twin CAT 3 Machine Learning Model Manager is compatible with Visual Studio 2017, 2019, 2022 and the Tc Xae Shell. As an alternative to the editing of ML model description files via the interface of the Twin CAT 3 Machine Learning Model Manager, you can also use a command line tool, see CLI [} 69], or a Python library, see Python API [} 72]. Conversion of ML model files A conversion tool for ML model description files is located on the Convert tool tab. XML [} 74] and ONNX [} 62] files can be selected and converted to XML or BML [} 76] format. 66 Version: 1.7.0 TF3800, TF3810
## Page 67

Machine Learning Models and file formats NOTICE Conversion of Beckhoff BML back to XML is not provided for The objective of Beckhoff BML is to represent the content as a not freely readable binary file. Therefore, the conversion process from Beckhoff BML to Beckhoff XML is not provided for. The File Browser is opened via Select files and ML model description files can be selected (multi-selection is possible by Ctrl + click). Selected ML model files are listed on the left-hand side with their path and file name. Files can be removed from the list again with Remove selected from list. Listed ML model description files can be selected in the left-hand list (multi-selection is possible with Ctrl + click here, too) and converted with Convert files into the format selected in the drop-down menu. The converted files are saved in the converted file path. The default path is <Twin CATPath>\Functions\TF38xx-Machine-Learning\Convert Tool Files. The converted file path can be opened in the File Browser by clicking Open converted file path. The path can be changed with Select converted files path. The change is retained even after restarting the PC. The Model Manager is the central tool for managing ONNX files for the Twin CAT Machine Learning Server. Description files are generated for executing the AI model with the Machine Learning Server. A pop-up window states the Twin CAT product that the ONNX is compatible with for each converted ONNX. TF3800, TF3810 Version: 1.7.0 67
## Page 68

Machine Learning Models and file formats Creating metadata for the model The Configuration tool tab displays the configurator for: • Custom attributes [} 74] • Product Version and Target Version Information [} 74] • Input and Output Scaling [} 74] An ML description file can be selected using Select file and then edited. After editing, the original file is overwritten using Save changes. The Custom Attributes are edited using the buttons: • Add attribute entry: Adds an attribute to the selected tree item. The tree item must be selected in the left-hand list. ◦ If an attribute is created, then the name, the data type and the value or values respectively must be specified. ◦ Attributes are deleted by selecting the attribute and pressing the Delete button. • Add tree entry: Adds a tree item under the selected tree item (as a subtree item). • Rename tree entry: The selected tree item can be renamed. • Remove tree entry: The selected tree item incl. the subtree items is deleted. The editing area for Input and Output Transformations can be enabled by activating the Trafo active checkbox. Depending on the number of inputs and outputs, a corresponding number of rows is offered, in each of which scaling and offset are to be entered, cf. XML Tag Auxiliary Specifications [} 74]. A value from a list of numbers from the clipboard can be entered as an offset or scaling using the Paste Scaling and Paste Offset buttons. The number sequence can be separated by comma, semicolon or space. Only the number of numbers in the list must match the number of inputs or outputs respectively. 68 Version: 1.7.0 TF3800, TF3810
## Page 69

Machine Learning Models and file formats The Producer and Target Version Information is set automatically by the Twin CAT 3 ML Model Manager. The Target Version is determined automatically on the basis of the feature set of the model description file used. If an older ML Runtime version is used to load this model file, a warning message appears when executing the Configure method. The number of inputs and outputs of the model as well as the model type are displayed in the lower right area of the window. This cannot be edited and is only for information. 5.3.3.2 CLI In addition to the GUI and the Python package, a command line tool is available for the programmatic processing of ML description files, e.g. conversion from ONNX to XML or BML: mllib_toolbox.exe The executable is located in <Twin Cat Install Dir>\Functions\TF38xx-Machine- Learning\Utilities\Model Manager API. Use of the executable mllib_toolbox.exe can be used, for example, from the command prompt. The built-in help is displayed by running the exe without arguments. You can move/copy the mllib_toolbox.exe to any other place you like on your PC. The exe uses the path environment variable, which points to the mllib_um.dll (default <Twin Cat Install Dir>\3.1\Components\Base\Addins\Tc MLExtension). Conversion of ONNX files ONNX files are converted using the method onnximport mllib_toolbox.exe onnximport ".\decision tree\decisiontree-classifier.onnx" ". \decision tree\decisiontree-classifier.bml" Alternatively, an argument can also be used (--xml or –bml). Mllib_toolbox onnximport my Onnx File.onnx --xml Mllib_toolbox onnximport my Onnx File.onnx -bml The conversion command automatically writes the “required version” (minimum version of the ML Runtime driver) in the generated XML or BML. Merging and extracting engines TF3800, TF3810 Version: 1.7.0 69
## Page 70

Machine Learning Models and file formats Use the merge command to create multi-engines. The following command merges the two named XML files to form a multi-engine with 2 engines. The last-named file is overwritten in the process. mllib_toolbox.exe merge Keras MLPExample_cos.xml Keras MLPExample_sin.xml A new target file can also be specified. This may be both an XML and a BML file. mllib_toolbox.exe merge Keras MLPExample_cos.xml Keras MLPExample_sin.xml Multi Engine.bml The number of files to be combined with a command is not limited. Use the Extract method to extract engines from a description file with multiple engines. Using the Info command, first check how many engines exist in the file – and what they are called. In the figure above you can see two engines with the designations mlp_fp32_engine::merge0 or merge1. To extract the first engine into the target file Extracted.xml, call: mllib_toolbox.exe extract Multi Engine.xml?eng='mlp_fp32_engine::merge0' Extract.xml Displaying information from an ML model description file The info command can be used to quickly check what model is described in a description file. This can analyze ONNX files as well as XML and BML files. mllib_toolbox.exe info decisiontree-classifier.xml mllib_toolbox.exe info decisiontree-classifier.bml mllib_toolbox.exe info decisiontree-classifier.onnx 70 Version: 1.7.0 TF3800, TF3810
## Page 71

Machine Learning Models and file formats Execution of an instruction list You can perform operations graphically in the Machine Learning Model Manager. Each operation that you perform in a session is written as an XML command on the “Automation tool” tab. In the figure above, for example, a conversion command and a command to enter Custom Attributes can be seen. Mark the commands that you wish to export and select Create XML file. Pay attention to the order in which you select the commands in the Machine Learning Model Manager. The order of selection determines the order of the commands in the exported file. You can use the generated XML file to reproduce the command order via: mllib_toolbox.exe rawxml Automation Tool Export.xml Custom Attributes, Scalings and Model description These properties are not available in the CLI. Use the Python package [} 72] or the Twin CAT Machine Learning Model Manager [} 66] for that. The CLI is limited to basic functions. Also see about this 2 Python API [} 72] TF3800, TF3810 Version: 1.7.0 71
## Page 72

Machine Learning Models and file formats 2 GUI [} 66] 5.3.3.3 Python API Installation of the Python package The Python package is stored as a whl file in the folder <Twin Cat Install Dir>\Functions\TF38xx-Machine- Learning\Utilities\Model Manager API\Python Package. The package is compatible with: • Windows 10 • Windows 11 • Debian 12 • Ubuntu 24.04 The Clang C++ libraries are required for use under Linux® distributions. Install libc++1-XX with XX>=14 via the Advanced Package Tool (apt). >># apt install libc++1-19 -y To install the package, use : pip install <Twin Cat Install Dir>\Functions\TF38xx-Machine- Learning\Utilities\Model Manager API\Python Package\<whl-file-name> The Twin Cat Install Dir folder may contain different versions of the package (if you have installed a new setup on top of an old Twin CAT Machine Learning Setup). Make sure you always use the current version. Use of the Beckhoff toolbox For a description of the individual points such as Custom Attributes and Model Description, see: Conversion from ONNX to XML and BML [} 65]. The following source code is also available as a py file. See: https://infosys.beckhoff.com/content/1033/ tf38x0_tc3_ML_NN_Inference_Engine/Resources/13668699915.zip in the Python API_mllib directory. The package is loaded with import beckhoff.toolbox as tb Conversion of an ONNX file to XML tb.onnximport("../decision tree/decisiontree-regressor.onnx", "../decision tree/decisiontree- regressor.xml") tb.onnximport("../decision tree/decisiontree-regressor.onnx", "../decision tree/decisiontree- regressor.bml") Display of model information tb.info("../decision tree/decisiontree-regressor.onnx") tb.info("../decision tree/decisiontree-regressor.xml") Add Custom Attributes new_ca = { 'n ID' : -34234, 'b Tested' : True, 'f Num' : 324.3E-12, 'Another Tree Item' : { 'f Pi' : 3.134 12, 'b False Flag' : False }} tb.modify_ca("../decision tree/decisiontree-regressor.xml","../decision tree/decisiontree-regressor- custom.xml", new_ca) Add Model Description (name, version, author, etc. of a model) model_description = { "new_version" : "2.3.1.0", "new_name" : "Current Pre Control Axis42", "new_desc":"This is the most awesome model to control Axis42", "new_author":"Max", "new_tags":"awesome, ingenious, astounding", } 72 Version: 1.7.0 TF3800, TF3810
## Page 73

Machine Learning Models and file formats tb.modify_md("../decision tree/decisiontree-regressor-custom.xml", "../decision tree/decisiontree- regressor-md2.xml", **model_description) Add Input and Output Transformations Output Transformations only for selected models Whereas an input transformation is possible for all AI models, output transformations can only be used for models of the regression type. # add input / output scalings (input-output-transformations) tb.modify_iot("../decision tree/decisiontree-regressor-md.xml", "../decision tree/decisiontree- regressor-iot.xml", tb.iot_scaled_offset([1.2,3.4,1.0,1.1,1.0,1.0,1.0,1.0,1.0,1.0], [1.2,3.4,1.0,1.1,1.0,1.0,1.0,1.0,1.0,1.0]), None) # no output transformation The function iot_scaled_offset expects a list of offsets and scalings. A value must be specified for each input or output. def iot_scaled_offset(offsets : list, scalings : list): The number of inputs in the example above is 10, therefore a list of 10 elements is transferred. No output transformation is available for decision trees, therefore a None is transferred here. Generate multi-engines Merge several models into one XML file. # # Create a Multi-Engine # merge two XML files - output file is Merged.xml tb.merge(['Keras MLPExample_sin.xml','Keras MLPExample_cos.xml'],'Merged.xml') # merge two specific engines from two XML files tb.merge([tb.input_engine('Merged.xml','mlp_fp32_engine::merge1'), tb.input_engine('Keras MLPExample_cos.xml','mlp_fp32_engine')], 'Double Merged.xml') # merge two specific engines from two XML files and provide a reference name for both engines in the target file tb.merge([tb.input_engine('Keras MLPExample_sin.xml','mlp_fp32_engine', 'sine'), tb.input_engine('Keras MLPExample_cos.xml','mlp_fp32_engine', 'cosine')], 'Merged Ref.xml') # extract a specific engine from a Multi-Engine file tb.extract(tb.input_reference('Merged Ref.xml', 'sine'),'Extract.xml') Test-Predict for the ML Runtime in Python Generate an XML and subsequently predict-call the ML Runtime. The ML Runtime is compiled as a DLL and is called from Python as a user-mode process. No Twin CAT Runtime is required. tb.onnximport("../decision tree/decisiontree- regressor.onnx", "../decision tree/decisiontree- regressor.xml") inp_file_xml = "../decision tree/decisiontree-regressor.xml" # define input and output format and input values (10 in, 1 out) prediction = [{'input_type': 'fp64', 'output_type': 'fp64', 'input': [-1.0,-1.0,1.0,1.4,-1.0,-1.0,1. 0,1.4,4.2,4.2]}] # call predict method of Python-ML-Runtime out = tb.predict(inp_file_xml,prediction) print(out) Further sample of a Classifier. tb.onnximport("../decision tree/decisiontree-classifier.onnx", "../decision tree/decisiontree- classifier.xml") inp_file_xml = "../decision tree/decisiontree-classifier.xml" # define input and output format and input values (4 in, 1 out) prediction = [{'input_type': 'fp64', 'output_type': 'int32', 'input': [-1.0,-1.0,1.0,1.4]}] # call predict method of Python-ML-Runtime out = tb.predict(inp_file_xml,prediction) print(out) TF3800, TF3810 Version: 1.7.0 73
## Page 74

Machine Learning Models and file formats 5.3.3.4 Description of the Beckhoff-specific XML and BML format 5.3.3.4.1 Beckhoff ML XML Introduction to Beckhoff ML XML The Beckhoff-specific XML format for the representation of trained Machine Learning Models forms a core component of the Twin CAT Machine Learning Inference Engine and Twin CAT Neural Network Inference Engine. The file is created from an ONNX file [} 62] using the TC3 Machine Learning Model Manager [} 66] or the Machine Learning Toolbox [} 69] or the provided Python package [} 72]. As opposed to ONNX, the XML-based description file can map Twin CAT-specific properties. The XML guarantees an extended functional scope of the Twin CAT Machine Learning product – see for example the concept of the Multi-engines [} 75]. On the other hand, it ensures seamless cooperation between the creator and user of the description file - compare Input and output transformations [} 74] and Custom Attributes [} 74]. Essential areas of the Beckhoff ML XML are described below. This helps you to understand the functions it provides. XML Tag <Machine Learning Model> Obligatory tag with 2 obligatory attributes. The tag is generated automatically and may not be manipulated. Sample: <Machine Learning Model model Name="Support_Vector_Machine" default Engine="svm_fp64_engine"> The attribute model Name can be read in the PLC via the method Get Model Name [} 89]. The model type that is to be loaded is identified by the model name. For example, the attribute can take the values support_vector_machine or mlp_neural_network . The attribute model Name in this tag should not be confused with the attribute str_model Name from <Model Description>. XML Tag <Custom Attributes> The tag Custom Attributes is optional and may be freely used by the user. The depth of the tree and the number of attributes are not limited. Creation can take place via the TC3 Machine Learning Model Manager. The XML can also be manually edited in this area. Attributes can be read in the PLC via the methods Get Custom Attribute_array [} 85], Get Custom Attribute_fp64 [} 86], Get Custom Attribute_int64 [} 87] und Get Custom Attribute_str [} 87]. In the XML the typification is given by the prefixes str_, int64_, fp64_ and so on. Sample: <Custom Attributes> <Model str_Name="Temp Estimator" str_Version="1.2.11.0" /> <Meta Info arrfp64_Input Range="0.10000000000000001,0.90000000000000002" int64_The Answer="42" /> </Custom Attributes> Here, a model with the name "Temp Estimator" is created in the version 1.2.11.0. Thus, an array and an integer value are provided as further information. Sample code for reading the Custom Attributes can be downloaded from the Samples [} 94] section. XML Tag <Auxilliary Specifications> The Auxilliary Specifications area is optional and is subdivided into the children <PTI> and <IOModification>. Sample: <Auxiliary Specifications> <PTI str_producer="Beckhoff MLlib Keras Exporter" str_producer Version="3.0.200525.0 str_required V ersion="3.0.200517.0d"/> <Model Description str_model Version="2.3.1.0" str_model Name="Current Pre Control Axis42" str_model Des c="This is the most awesome model to control Axis42" str_model Author="Max" str_model Tags="awesome,in genious,astounding" /> <IOModification> 74 Version: 1.7.0 TF3800, TF3810
## Page 75

Machine Learning Models and file formats <Output Transformation str_type="SCALED_OFFSET" fp64_offsets="0.48288949404162623" fp64_scaling s="1.4183887951105305"/> </IOModification> </Auxiliary Specifications> <PTI> PTI stands for "Product Version and Target Version Information". The tool with which the XML was created and version of the tool at the time of the XML generation are specified here. A minimum version of the executive ML Runtime can also be specified via the attribute str_required Version. The query is regarded as passed if the attribute is not set. If the attribute is set, the query is regarded as passed if the ML Runtime Version is higher than or equal to the required version. If the query is not passed, i.e. if the version of the ML Runtime used is lower than the required version, then a warning is displayed when executing the Configure method. <IOModification> If inputs or outputs of the learned model are scaled in the training environment, the scaling parameters used can be integrated directly in the XML file so that Twin CAT automatically performs the scaling in the ML Runtime. The scaling takes place by means of y = x * Scaling + Offset. <Model Description> Write attributes to this optional tag • the model version str_model Version • the model name str_model Name • the model description str_model Desc • the author of the model str_model Author • further optional tags str_model Tags XML Tag <Configuration> The obligatory area Configuration describes the structure of the loaded model. Example - SVM <Configuration str_operation Type="SVM_TYPE_NU_REGRESSION" fp64_cost="0.1" fp64_nu="0.3" str_kernel Fu nction="KERNEL_FN_RBF" fp64_gamma="1.0" int64_num Input Attributes="1"/> Example - MLP <Configuration int_num Input Neurons="1" int_num Layers="2" bool_uses Bias="true"> <Mlp Layer1 int_num Neurons="3" str_activation Function="ACT_FN_TANH"/> <Mlp Layer2 int_num Neurons="1" str_activation Function="ACT_FN_IDENTITY"/> </Configuration> A configuration exists once only and is generated automatically. XML Tag <Paramaters> The obligatory area Parameters substantiates the loaded model with the described <Configuration>. The learned parameters of the model are stored here, e.g. the weights of the neurons. In the standard case, i.e. a learned model is described in an XML, the <Parameters> tag exists only once in the XML. <Parameters str_engine="mlp_fp32_engine" int_num Layers="2" bool_uses Bias="true"> Several models with identical <Configuration> can be merged via the Machine Learning Model Manager so that both models are described in a single XML. Distinction can then be made between the parameter sets by Engines, which is specified as an attribute for each parameter tag. Sample: <Parameters str_engine="mlp_fp32_engine::merge0" int64_num Layers="2" bool_uses Bias="true"> … </Parameters> <Parameters str_engine="mlp_fp32_engine::merge1" int64_num Layers="2" bool_uses Bias="true"> TF3800, TF3810 Version: 1.7.0 75
## Page 76

Machine Learning Models and file formats … </Parameters> <IODistributor str_distributor="multi_engine_io_distributor::mlp_fp32_engine- merge" str_engine_type="mlp_fp32_engine" int64_engine_count="2"> <Engine0 str_engine_name="merge0" str_reference="sin_engine" /> <Engine1 str_engine_name="merge1" str_reference="cos_engine" /> </IODistributor> Two MLPs with an identical Configuration were merged here. The first engine bears the ID 0 and the internal name "mlp_fp32_engine::merge0" and can be addressed by the user via the reference "sin_engine". The second engine bears the ID 1 and the internal name "mlp_fp32_engine::merge1" and the reference "cos_engine". The ID of the engine is sequentially incremented by the value one, starting from zero. The reference is a string that can be specified in the Model Manager during Merge. If several engines are merged in an XML, all engines are loaded in the ML Runtime and are available for inference. The Predict method [} 90] is to be transferred when calling the engine ID that is to be used. The reference for the engine can be transferred via the Predict Ref method [} 91]. A Get Engine Id From Ref method [} 88] is also available for finding the associated ID from the reference. Switching between the engines is possible without latency. There is an example of the use of multi-engines in the PLC in the Samples area. 5.3.3.4.2 Beckhoff ML BML The BML format is a binary representation of the XML-based ML description file. As a result, the format is not openly visible and the file size is smaller in comparison with ONNX and XML. This also makes the charging process of the model much faster than charging an XML. A BML file can be generated via the TC3 Machine Learning Model Manager [} 66] from an XML or an ONNX file. The way back from a BML file to an XML file is not provided for. 5.4 File management of the ML description files File management on the Engineering PC (XAE) Conversion, editing of ONNX, XML and BML The Machine Learning Model Manager [} 66] serves as a central tool for the processing and conversion of Machine Learning models. If a file is loaded and edited, the resulting file is saved in the standard case in the following folders – depending on the action performed: • \Functions\TF38xx-Machine-Learning\Convert Tool Files • \Functions\TF38xx-Machine-Learning\Extract Tool Files • \Functions\TF38xx-Machine-Learning\Merge Tool Files Each default folder can be adapted in the Model Manager. If the Machine Learning Toolbox [} 69] is used instead of the GUI-based Machine Learning Model Manager, the newly created file is stored in the active path as long as no other path has been specifically named. Integrating a model into a Twin CAT solution If a BML or XML description file is used in a Twin CAT solution, a distinction must be made between Tc COM API [} 79] and the PLC API [} 81] with regard to the file management. Tc COM API After the integration of a description file in the Tc COM Tc Machine Learning Model Cycal, the corresponding description file is copied into the Visual Studio project directory and is thus part of the project: <VS Projekt>\_MLInstall. On activating the configuration, the file is copied from the Visual Studio project directory into the boot folder on the target system: \Twin CAT\3.1\Boot\ML_Boot. 76 Version: 1.7.0 TF3800, TF3810
## Page 77

Machine Learning Models and file formats PLC API When using the PLC API, the file name and path of the Machine Learning model file are specified in the PLC code as T_Max String – accordingly, the user must ensure that a corresponding file exists coming from the target system. This means that the description file does not become part of the Visual Studio project directory, nor is it transferred automatically to the target system. Transfer of the ML description files to the target system on activating the configuration Tc COM API When using the Tc COM object Tc Machine Learning Model Cycal, the ML description file is transferred automatically from the XAE system to the XAR system. The file is transferred from the Visual Studio project folder <VS Project>\_MLInstall to the Boot folder Twin CAT\3.1\Boot\ML_Boot on the XAR. PLC API If the PLC API is used, the user is responsible for the transfer of the ML description file. As a result, the flexibility of the application is increased on the one hand, while corresponding steps have to be implemented by the user on the other. The ML description file can be transferred to the target system in many ways. One of them is named below by way of example. • Via the properties of the PLC project under "Deployment" it is possible to specify which files are to be transferred to the target system in the case of a certain event, for example the activation of the configuration. NOTICE Writing rights on the target system The writing rights on the operating system side and the Write Filter settings must be observed. Updating ML description files in the field An important scenario in machine learning is the updating of data-based algorithms in the field during the running time of a machine. Here too, distinction must be made between use of the Tc COM [} 79] API and use of the PLC API [} 81]. Tc COM API In the case of the Tc COM API, the update behavior is the same as with other changes in the Tc COM area. An XAE system is necessary with an ADS route to the target system. In the XAE, a new ML description file can be integrated in the Twin CAT project. The new project is then transferred to the target system by activating the configuration. Therefore, it is necessary to restart the Twin CAT runtime here. PLC API If the PLC API is used, it is possible to update the ML description file on the target system without restarting the Twin CAT runtime. To do this it is merely necessary to update the ML description file on the target system and to retrigger the Configure method. TF3800, TF3810 Version: 1.7.0 77
## Page 78

Machine Learning Models and file formats In addition, for example using ADS, you can set the PLC variable containing the full path of the ML description file to the value of the new file that was transferred beforehand to the file system and then set the state of the state machine back to "load". 78 Version: 1.7.0 TF3800, TF3810
## Page 79

API 6 API Two ways of programming are available to the user in Twin CAT 3. Static Tc COM objects can be created and triggered via a cyclic task, or an instance can be created from and configured via the PLC. The programming interface of the PLC offers far greater flexibility than the use of a Tc COM instance; conversely, the latter is very simple and in many cases adequate. 6.1 Tc COM The use of a Tc COM for the inference of an ML model in Twin CAT provides a very simple possibility to execute trained models in the Twin CAT XAR. In principle, the entire procedure is documented in the quick start, so that the steps described there are initially repeated and a few further details are given below. Incorporation of a model by means of Tc COM object This section deals with the execution of machine learning models by means of a prepared Tc COM object. This interface offers a simple and clear way of loading models, executing them in real-time and generating appropriate links in your own application by means of the process image. Generate a prepared Tc COM object Tc Machine Learning Model Cycal 1. To do this, select the node Tc COM Objects with the right mouse button and select Add New Item… Under Tasks, generate a new Twin CAT task and assign this task context to the newly generated instance of Tc Machine Learning Model Cycal 2. To do this, open the Context tab of the generated object. 3. Select your generated task in the drop-down menu. TF3800, TF3810 Version: 1.7.0 79
## Page 80

API ð The instance of Tc Machine Learning Model Cycal has a tab called ML model configuration where you can load the description file of the ML algorithm (XML or BML) and the available data types for the inputs and outputs of the selected model are then displayed. • The file does not have to be on the target system. It can be selected from the development system and is then loaded to the target system on activating the configuration. ◦ A distinction is made between preferred and supported data types. The only difference is that a conversion of the data type takes place at runtime if a non-preferred type is selected. This may lead to slight losses in performance when using non-preferred data types. • The data types for inputs and outputs are initially set automatically to the preferred data types. The process image of the selected model is created by clicking Generate IO. Accordingly, by loading Keras MLPExample_cos.xml, you get a process image with an input of the type REAL and an output of the type REAL. Activating the project on the target 1. Before activating the project on a target, you must select the TF3810 license manually on the Manage Licenses tab under System>License in the project tree, as you wish to load a multi-layer perceptron (MLP). 2. Activate the configuration. ð You can now test the model by manually writing at the input. If the process image is larger, i.e. many inputs or outputs exist, it may be helpful not to generate each input individually as a PDO, but to define an input or output as an array type. To do this, check the checkbox Generate IO as array and click Generate IO. Models with several engines, cf. XML Tag parameters [} 75], can be loaded, but only Engine Id = 0 is used. Switching between the Engine Ids with the Tc COM API is not provided for. The ML description file used is automatically transferred from the Engineering system to the Runtime system on activating the configuration. File management details are described in the section File management of the ML description files [} 76]. 80 Version: 1.7.0 TF3800, TF3810
## Page 81

API 6.2 PLC API 6.2.1 Datatypes 6.2.1.1 ETc Mll Data Type Syntax Definition: TYPE ETc Mll Data Type : ( E_MLLDT_UNDEFINED := 0, E_MLLDT_INT8_SINT := 10, E_MLLDT_INT16_INT := 20, E_MLLDT_INT32_DINT := 30, E_MLLDT_INT64_LINT := 40, E_MLLDT_FP16 := 50, E_MLLDT_FP16B := 55, E_MLLDT_FP32_REAL := 60, E_MLLDT_FP64_LREAL := 70, E_MLLDT_SPECIAL := 99 )BYTE; END_TYPE Values Name Description E_MLLDT_UNDEFINED invalid / undefined data type E_MLLDT_INT8_SINT 8-bit signed integer number (SINT / char) E_MLLDT_INT16_INT 16-bit signed integer number (INT / short) E_MLLDT_INT32_DINT 32-bit signed integer number (DINT / long) E_MLLDT_INT64_LINT 64-bit signed integer number (LINT / long long) E_MLLDT_FP16 16-bit IEEE floating point number (future usage) E_MLLDT_FP16B 16-bit "bfloat16" floating point number (future usage) E_MLLDT_FP32_REAL 32-bit IEEE floating point number (REAL / float) E_MLLDT_FP64_LREAL 64-bit IEEE floating point number (LREAL / double) E_MLLDT_SPECIAL Function-specific byte stream 6.2.1.2 ST_Mll Prediction Parameters Syntax Definition: TYPE ST_Mll Prediction Parameters : STRUCT Ml Model Filepath : STRING(255); Max Concurrency : UINT; END_STRUCT END_TYPE Parameters Name Type Default Description Ml Model Filepath STRING(255) File path of the loaded model. Either *.xml or *.bml file Max Concurrency UINT 1 Maxium number of threads calling predict on the FB in the same time. TF3800, TF3810 Version: 1.7.0 81
## Page 82

API 6.2.2 Function Blocks 6.2.2.1 FB_Mll Prediction FB_Mll Prediction st Prediction Parameter ST_Mll Prediction Parameters BOOL b Busy e Trace Level Tc Event Severity BOOL b Error I_Tc Message ip Result Message HRESULT hr Error Code Syntax Definition: FUNCTION_BLOCK FB_Mll Prediction VAR_INPUT st Prediction Parameter : ST_Mll Prediction Parameters; e Trace Level : Tc Event Severity; END_VAR VAR_OUTPUT b Busy : BOOL; b Error : BOOL; ip Result Message : I_Tc Message; hr Error Code : HRESULT; END_VAR Inputs Name Type Description st Prediction Par General Prediction parameters ameter ST_Mll Prediction Paramete rs [} 81] e Trace Level Tc Event Severity Eventlogger trace level. Default = Critical. Must be set before Configure method is called Outputs Name Type Description b Busy BOOL True if a asynchronous action is taking place b Error BOOL Indicates error in method ip Result Messa I_Tc Message Contains the last invoked error ge hr Error Code HRESULT Unique error code 82 Version: 1.7.0 TF3800, TF3810
## Page 83

API Methods Name Description Check Preferred IOData Types [} 84] Check Supported IOData Types [} 84] Configure [} 85] Get Custom Attribute_array [} 85] Get Custom Attribute_fp64 [} 86] Get Custom Attribute_int64 [} 87] Get Custom Attribute_str [} 87] Get Engine Id From Ref [} 88] Get Input Dim [} 88] Get Max Concurrency [} 89] Get Model Name [} 89] Get Output Dim [} 90] Predict [} 90] Predict Ref [} 91] Reset [} 92] Set Active Engine Options [} 93] General information The FB_Mll Prediction is a central Function Block for the usage of TC3 Machine Learning in the PLC. The Function Block offers a variety of Methods as described above. Basically, the FB_Mll Prediction offers the functionality to load and to execute ML models. Hence, it is an interface to the Twin CAT 3 integrated inference engine (ML Runtime). Error handling Note that all methods of FB_Mll Prediction return a BOOL which indicates if the execution on the method caused any error, e.g. b Failed := fbprediction. Get Input Dim(n Input Dim); The evaluation of the return value of the methods is equivalent to the evaluation of the Output b Error of FB_Mll Prediction. Further, FB_Mll Prediction references the Twin CAT 3 Event Logger and thus ensures that information (events) is provided via the standardized interface I_Tc Message. The trace level can be adjusted using Tc Event Severity. Sample Code Sample code for the usage of the Function Block is available here [} 94]. TF3800, TF3810 Version: 1.7.0 83
## Page 84

API System Requirements 6.2.2.1.1 Check Preferred IOData Types Check Preferred IOData Types fmt Input Type ETc Mll Data Type BOOL Check Preferred IOData Types fmt Output Type ETc Mll Data Type Is Preferred Reference To BOOL Syntax Definition: METHOD Check Preferred IOData Types : BOOL VAR_INPUT fmt Input Type : ETc Mll Data Type; fmt Output Type : ETc Mll Data Type; Is Preferred : Reference To BOOL; END_VAR Inputs Name Type Description fmt Input Type ETc Mll Data Type [} 81] Input type to check fmt Output Type ETc Mll Data Type [} 81] Output type to check Is Preferred Reference To BOOL Return true if preferred typo Return value BOOL A distinction is made between preferred and supported data types. The only difference is that a conversion of the data type takes place at runtime if a non-preferred type is selected. This may lead to slight losses in performance when using non-preferred data types. 6.2.2.1.2 Check Supported IOData Types Check Supported IOData Types fmt Input Type ETc Mll Data Type BOOL Check Supported IOData Types fmt Output Type ETc Mll Data Type Is Supported Reference To BOOL Syntax Definition: METHOD Check Supported IOData Types : BOOL VAR_INPUT fmt Input Type : ETc Mll Data Type; fmt Output Type : ETc Mll Data Type; Is Supported : Reference To BOOL; END_VAR Inputs Name Type Description fmt Input Type ETc Mll Data Type [} 81] Input type to check fmt Output Type ETc Mll Data Type [} 81] Output type to check Is Supported Reference To BOOL returns true if supported 84 Version: 1.7.0 TF3800, TF3810
## Page 85

API Return value BOOL 6.2.2.1.3 Configure Configure BOOL Configure Syntax Definition: METHOD Configure : BOOL Return value BOOL The method loads the specified ML model description file and configures the inference engine. Specify all settings using the st Prediction Parameter before calling the configure method. fb Predict.st Prediction Parameter. Ml Model Filepath := 'C:/my Model.xml'; fb Predict.st Prediction Parameter. Max Concurrency := 1; b Configured := fb Predict. Configure(); 6.2.2.1.4 Get Custom Attribute_array Get Custom Attribute_array s Custom Attribute Name T_Max String BOOL Get Custom Attribute_array fmt Attribute Data Type Reference To ETc Mll Data Type p Data Buffer PVOID n Data Buffer Len UDINT n Array Length Reference To UDINT pn Bytes Written Pointer To UDINT Syntax Definition: METHOD Get Custom Attribute_array : BOOL VAR_INPUT s Custom Attribute Name : T_Max String; fmt Attribute Data Type : Reference To ETc Mll Data Type; p Data Buffer : PVOID; n Data Buffer Len : UDINT; n Array Length : Reference To UDINT; pn Bytes Written : Pointer To UDINT; END_VAR TF3800, TF3810 Version: 1.7.0 85
## Page 86

API Inputs Name Type Description s Custom Attribu T_Max String Name of the custom attribute te Name fmt Attribute Dat Reference To Data format of the custom attribute a Type ETc Mll Data Type [} 81] p Data Buffer PVOID Destination data buffer, where the custom attribute is copied into n Data Buffer Le UDINT Length of the destination data buffer in bytes n n Array Length Reference To UDINT Number of data type elements (i.e. number of fp32 values) found in the custom attribute pn Bytes Written Pointer To UDINT Returns the number of bytes that have been written to the destination buffer Return value BOOL Methods reads a custom attribute specified by s Custom Attribute Name of type Array. Refer to this sample code [} 94] showing how to read an array of LREAL. 6.2.2.1.5 Get Custom Attribute_fp64 Get Custom Attribute_fp64 s Custom Attribute Name T_Max String BOOL Get Custom Attribute_fp64 lr Data Out Reference To LREAL Syntax Definition: METHOD Get Custom Attribute_fp64 : BOOL VAR_INPUT s Custom Attribute Name : T_Max String; lr Data Out : Reference To LREAL; END_VAR Inputs Name Type Description s Custom Attribu T_Max String Name of the custom fp64 attribute te Name lr Data Out Reference To LREAL Output value of the custom fp64 attribute Return value BOOL Methods reads a custom attribute specified by s Custom Attribute Name of type LREAL. s Custom Key : T_Max String := Xml Tree Item/Xml Attribute'; f Value : LREAL; fbprediction. Get Custom Attribute_fp64(s Custom Key, f Value); 86 Version: 1.7.0 TF3800, TF3810
## Page 87

API 6.2.2.1.6 Get Custom Attribute_int64 Get Custom Attribute_int64 s Custom Attribute Name T_Max String BOOL Get Custom Attribute_int64 n Data Out Reference To LINT Syntax Definition: METHOD Get Custom Attribute_int64 : BOOL VAR_INPUT s Custom Attribute Name : T_Max String; n Data Out : Reference To LINT; END_VAR Inputs Name Type Description s Custom Attribu T_Max String Name of the custom int64 attribute te Name n Data Out Reference To LINT Output value of the custom int64 attribute Return value BOOL Methods reads a custom attribute specified by s Custom Attribute Name of type LINT. s Custom Key : T_Max String := Xml Tree Item/Xml Attribute'; i Value : LINT; fbprediction. Get Custom Attribute_int64(s Custom Key, i Value); 6.2.2.1.7 Get Custom Attribute_str Get Custom Attribute_str s Custom Attribute Name T_Max String BOOL Get Custom Attribute_str s Dst Attribute String Buffer Reference To T_Max String pn String Len Pointer To UDINT Syntax Definition: METHOD Get Custom Attribute_str : BOOL VAR_INPUT s Custom Attribute Name : T_Max String; s Dst Attribute String Buffer : Reference To T_Max String; pn String Len : Pointer To UDINT; END_VAR Inputs Name Type Description s Custom Attribu T_Max String Name of the custom string attribute te Name s Dst Attribute St Reference To Pointer to a string buffer to write the custom string attribute into ring Buffer T_Max String pn String Len Pointer To UDINT (Optional) Actual length of the string attribute TF3800, TF3810 Version: 1.7.0 87
## Page 88

API Return value BOOL Methods reads a custom attribute specified by s Custom Attribute Name of type T_Max String. s Custom Key : T_Max String := Xml Tree Item/Xml Attribute'; s Value : T_Max String ; fbprediction. Get Custom Attribute_str(s Custom Key, s Value); 6.2.2.1.8 Get Engine Id From Ref Get Engine Id From Ref s Engine Ref T_Max String BOOL Get Engine Id From Ref n Engine Id Reference To UDINT Syntax Definition: METHOD Get Engine Id From Ref : BOOL VAR_INPUT s Engine Ref : T_Max String; n Engine Id : Reference To UDINT; END_VAR Inputs Name Type Description s Engine Ref T_Max String Reference string of model engine (or parameter set) used for prediction, use default value 0 if there are no multi-engines used n Engine Id Reference To UDINT Id of model engine (or parameter set) Return value BOOL In case of multi engines in an ML model description file, engines can be selected via ID or reference name. While the Predict-method expects an ID as input, the Predict Ref method expects a reference name as input. Get Engine Id From Ref can convert a reference name into an engine ID. Reference names can be found in the Beckhoff ML XML file inside the <IODistributor> section, see XML attribute str_reference, and can be set via the TC3 Machine Learning Model Manager, see Merge Tool. 6.2.2.1.9 Get Input Dim Get Input Dim n Input Dim Reference To UDINT BOOL Get Input Dim Syntax Definition: METHOD Get Input Dim : BOOL VAR_INPUT n Input Dim : Reference To UDINT; END_VAR 88 Version: 1.7.0 TF3800, TF3810
## Page 89

API Inputs Name Type Description n Input Dim Reference To UDINT Size of the input data array Return value BOOL 6.2.2.1.10 Get Max Concurrency Get Max Concurrency n Concurrency Reference To UDINT BOOL Get Max Concurrency Syntax Definition: METHOD Get Max Concurrency : BOOL VAR_INPUT n Concurrency : Reference To UDINT; END_VAR Inputs Name Type Description n Concurrency Reference To UDINT Maximum supported number of concurrently processing threads Return value BOOL Methods reads out current configuration of inference engine regarding possible number of concurrently processing threads. This value is set by the user using st Prediction Parameter before calling Configure method. 6.2.2.1.11 Get Model Name Get Model Name s Dst Model Name Buffer Reference To T_Max String BOOL Get Model Name pn String Len Pointer To UDINT Syntax Definition: METHOD Get Model Name : BOOL VAR_INPUT s Dst Model Name Buffer : Reference To T_Max String; pn String Len : Pointer To UDINT; END_VAR Inputs Name Type Description s Dst Model Nam Reference To Name of the loaded model e Buffer T_Max String pn String Len Pointer To UDINT (Optional) Actual length of the string attribute TF3800, TF3810 Version: 1.7.0 89
## Page 90

API Return value BOOL The method reads the model name of the loaded ML description file. The model name is an identifier of the machine learning model type. Returned strings can be for example ‘support_vector_machine’ or ‘mlp_neural_network’. 6.2.2.1.12 Get Output Dim Get Output Dim n Output Dim Reference To UDINT BOOL Get Output Dim Syntax Definition: METHOD Get Output Dim : BOOL VAR_INPUT n Output Dim : Reference To UDINT; END_VAR Inputs Name Type Description n Output Dim Reference To UDINT Size of the output data array Return value BOOL 6.2.2.1.13 Predict Predict p Data Inp PVOID BOOL Predict n Data Inp Dim UDINT fmt Data Inp Type ETc Mll Data Type p Data Out PVOID n Data Out Dim UDINT fmt Data Out Type ETc Mll Data Type n Engine Id UDINT n Concurrency Id UDINT Syntax Definition: METHOD Predict : BOOL VAR_INPUT p Data Inp : PVOID; n Data Inp Dim : UDINT; fmt Data Inp Type : ETc Mll Data Type; p Data Out : PVOID; n Data Out Dim : UDINT; fmt Data Out Type : ETc Mll Data Type; n Engine Id : UDINT; n Concurrency Id : UDINT; END_VAR 90 Version: 1.7.0 TF3800, TF3810
## Page 91

API Inputs Name Type Description p Data Inp PVOID Pointer to the input data array (e.g. ARRAY[0..10] OF REAL in PLC) n Data Inp Dim UDINT Number of inputs in the current vector fmt Data Inp Typ ETc Mll Data Type [} 81] ETc Mll Data Type data type of input array e p Data Out PVOID Pointer to the output data array (e.g. ARRAY[0..10] OF REAL in PLC) n Data Out Dim UDINT Number of outputs in the current vector fmt Data Out Typ ETc Mll Data Type [} 81] ETc Mll Data Type data type of output array e n Engine Id UDINT Id of model engine (or parameter set) used for prediction, use default value 0 if there are no multi-engines used n Concurrency I UDINT Id of the processing thread. Important: Never have two d concurrently processing threads use the same id. Return value BOOL The method performs the inference of the loaded model with the given input data and stores the result in the output data. Use configure method to load a ML model description file before calling Predict method. For the inputs and outputs a pointer, the number of inputs/outputs and the data type are needed. Sample call: dtype : ETc Mll Data Type. E_MLLDT_FP32_REAL; n Input Dim : UDINT := 3; n Output Dim : UDINT := 2; n Input : ARRAY[1..3] OF REAL; n Output : ARRAY[1..2] OF REAL; n Current Engine ID : UDINT := 0; n Concurrency Id : UDINT := 0; fbprediction. Predict( p Data Inp:=ADR(n Input) , n Data Inp Dim:= n Input Dim, fmt Data Inp Type:= dtype, p Data Out:=ADR(n Output) , n Data Out Dim:= n Output Dim, fmt Data Out Type:= dtype, n Engine Id:= n Current Engine ID, n Concurrency Id:= n Concurrency Id ); 6.2.2.1.14 Predict Ref Predict Ref p Data Inp PVOID BOOL Predict Ref n Data Inp Dim UDINT fmt Data Inp Type ETc Mll Data Type p Data Out PVOID n Data Out Dim UDINT fmt Data Out Type ETc Mll Data Type s Engine Ref T_Max String n Concurrency Id UDINT Syntax Definition: TF3800, TF3810 Version: 1.7.0 91
## Page 92

API METHOD Predict Ref : BOOL VAR_INPUT p Data Inp : PVOID; n Data Inp Dim : UDINT; fmt Data Inp Type : ETc Mll Data Type; p Data Out : PVOID; n Data Out Dim : UDINT; fmt Data Out Type : ETc Mll Data Type; s Engine Ref : T_Max String; n Concurrency Id : UDINT; END_VAR Inputs Name Type Description p Data Inp PVOID Pointer to the input data array (e.g. ARRAY[0..10] OF REAL in PLC) n Data Inp Dim UDINT Number of inputs in the current vector fmt Data Inp Typ ETc Mll Data Type [} 81] ETc Mll Data Type data type of input array e p Data Out PVOID Pointer to the output data array (e.g. ARRAY[0..10] OF REAL in PLC) n Data Out Dim UDINT Number of outputs in the current vector fmt Data Out Typ ETc Mll Data Type [} 81] ETc Mll Data Type data type of output array e s Engine Ref T_Max String Reference string of model engine (or parameter set) used for prediction, use default value 0 if there are no multi-engines used n Concurrency I UDINT Id of the processing thread. Important: Never have two d concurrently processing threads use the same id. Return value BOOL The method performs the inference of the loaded model with the given input data and stores the result in the output data. Use configure method to load a ML model description file before calling Predict Ref method. For the inputs and outputs a pointer, the number of inputs/outputs and the data type are needed. Sample call: dtype : ETc Mll Data Type. E_MLLDT_FP32_REAL; n Input Dim : UDINT := 3; n Output Dim : UDINT := 2; n Input : ARRAY[1..3] OF REAL; n Output : ARRAY[1..2] OF REAL; s Current Engine Ref: T_Max String := 'Engine Ref'; n Concurrency Id : UDINT := 0; fbprediction. Predict( p Data Inp:=ADR(n Input) , n Data Inp Dim:= n Input Dim, fmt Data Inp Type:= dtype, p Data Out:=ADR(n Output) , n Data Out Dim:= n Output Dim, fmt Data Out Type:= dtype, s Engine Ref:= s Current Engine Ref, n Concurrency Id:= n Concurrency Id ); 6.2.2.1.15 Reset Reset BOOL Reset 92 Version: 1.7.0 TF3800, TF3810
## Page 93

API Syntax Definition: METHOD Reset : BOOL Return value BOOL This Methods resets the FB’s error state. 6.2.2.1.16 Set Active Engine Options Set Active Engine Options s Engine Options T_Max String BOOL Set Active Engine Options Syntax Definition: METHOD Set Active Engine Options : BOOL VAR_INPUT s Engine Options : T_Max String; END_VAR Inputs Name Type Description s Engine Option T_Max String s Return value BOOL Input is a JSON-String with the following Key-Value-Pairs: Key Value Defaut-Value* allow_FPU TRUE / FALSE TRUE Allow_SSE3 TRUE / FALSE TRUE Allow_AVX TRUE / FALSE TRUE Allow_FMA TRUE / FALSE TRUE Allow_AVX_512F TRUE / FALSE TRUE *Default-Values: The library uses as default the maximum performance. Hence, all available SIMD- extensions provided by the target PC’s CPU are set to TRUE by default. Sample Code to disable FMA and allow AVX (all others will be left unaltered): fb Predict : FB_Mll Prediction; Engine Opts : T_Max String := '{ "allow_AVX":"true", "allow_FMA":"false" }'; fb Predict. Set Active Engine Options(Engine Opts); TF3800, TF3810 Version: 1.7.0 93
## Page 94

Samples 7 Samples 7.1 PLC API 7.1.1 Quick start The sample from the section Quick start [} 18] can be downloaded here: https://infosys.beckhoff.com/ content/1033/tf38x0_tc3_ML_NN_Inference_Engine/Resources/8746884875.zip. The ZIP contains a tszip archive (see PLC documentation, tszip) and a Beckhoff ML XML file (Keras MLPExample_cos. XML). Copy the XML file to the place defined in the PLC as the destination, or change the string to a different path. 7.1.2 Detailed example The sample can be downloaded here: https://infosys.beckhoff.com/content/1033/ tf38x0_tc3_ML_NN_Inference_Engine/Resources/8763219467.zip. The ZIP contains a tszip archive (see PLC documentation, tszip) and a Beckhoff ML XML file (Trigonometry MLP. XML). Copy the XML file to the place defined in the PLC as the destination, or change the string to a different path. The ML model description file contains an MLP with an input and an output, cf. XML tag <Configuration> with int64_num Input Neurons = 1 as well as the second (last) layer with int64_num Neurons = 1. Two parameter tags exist, i.e. the file contains two MLPs that are trained differently but are identical in structure (<Configuration>). One of them is an MLP that was trained to approximate a sine function, while the other is an MLP that is intended to approximate a cosine function. In the <IODistributor> area it can be seen that one engine is reachable with the reference "sin_engine" and the other with the reference "cos_engine". Some metadata are stored in the <Custom Attributes> area, e.g. the name of the model, the version and the validity range of the input variables. As in the quick start sample, a simple state machine is run through in the PLC source code. It differs from the quick start sample in the executability of the "Configure" state and the use of several engines. The "Configure" state shows by way of example how flexibly you can handle the number of inputs and outputs and how you can read as much information as possible from the description file and put it to use directly in the PLC. You can switch between the two engines manually in the online view by setting the Engine Id to 0 or 1. 7.1.3 Parallel, non-blocking access to an inference module This sample shows how an instance of FB_Mll Prediction can be accessed from two tasks running concurrently. The sample can be downloaded here: https://infosys.beckhoff.com/content/1033/ tf38x0_tc3_ML_NN_Inference_Engine/Resources/8775872011.zip. The instance fbpredict is declared in the GVL_ML. All programs in the PLC thus have access to the instance. The following are created as programs: • P_Init ML: The step sequence for initializing/loading an ML model is described here. • P_Predict_Task1: The Predict method of the fbpredict is called, wherein PRG is executed on Core 1. • P_Predict_Task2: The Predict method of the fbpredict is called, wherein PRG is executed on Core 2. The essential components for the concurrent execution of 2 Predict calls are: 94 Version: 1.7.0 TF3800, TF3810
## Page 95

Samples • The maximum number of concurrent accesses must be specified with the Configure method: GVL_ML.fbpredict.st Prediction Parameter. Max Concurrency := n Max Concurrency; with n Max Concurrency = 2. The instance then keeps this number of independent inference machines available. • A unique ID of the calling context must be specified when calling the Predict method. These are declared as constants in P_Predict_Task1 and P_Predict_Task2, see n Concurrency Id. The user must ensure that each calling context transfers a unique ID with the Predict call. • The remainder of the source code is largely identical to the Quick start sample [} 94]. TF3800, TF3810 Version: 1.7.0 95
## Page 96

Support and Service 8 Support and Service Beckhoff and their partners around the world offer comprehensive support and service, making available fast and competent assistance with all questions related to Beckhoff products and system solutions. Download finder Our download finder contains all the files that we offer you for downloading. You will find application reports, technical documentation, technical drawings, configuration files and much more. The downloads are available in various formats. Beckhoff's branch offices and representatives Please contact your Beckhoff branch office or representative for local support and service on Beckhoff products! The addresses of Beckhoff's branch offices and representatives round the world can be found on our internet page: www.beckhoff.com You will also find further documentation for Beckhoff components there. Beckhoff Support Support offers you comprehensive technical assistance, helping you not only with the application of individual Beckhoff products, but also with other, wide-ranging services: • support • design, programming and commissioning of complex automation systems • and extensive training program for Beckhoff system components Hotline: +49 5246 963-157 e-mail: support@beckhoff.com Beckhoff Service The Beckhoff Service Center supports you in all matters of after-sales service: • on-site service • repair service • spare parts service • hotline service Hotline: +49 5246 963-460 e-mail: service@beckhoff.com Beckhoff Headquarters Beckhoff Automation Gmb H & Co. KG Huelshorstweg 20 33415 Verl Germany Phone: +49 5246 963-0 e-mail: info@beckhoff.com web: www.beckhoff.com 96 Version: 1.7.0 TF3800, TF3810
## Page 97

Appendix 9 Appendix 9.1 Log files The log files are important for Support. They can be found under <Twin CATInstall Path>\Functions\TF38xx- Machine-Learning\Logs. The files Cycal Log.txt and Model Manager Log.txt are created on the XAE system. These log the behavior during the engineering of components. Cycal Log.txt contains logs regarding the configuration of Tc Machine Learning Model Cycal Tc COM. The TC3 Machine Learning Model Manager writes to the file Model Manager Log.txt. On the runtime system, the file mllib.log is created for logging the behavior during the machine runtime. You can access Report an Issue via the Visual Studio menu bar under Twin CAT > Machine Learning. This dialog opens the Support Information Report, which assists you in sending a report to Beckhoff Support. 9.2 Third-party components This software contains third-party components. TF3800, TF3810 Version: 1.7.0 97
## Page 98

Appendix Please refer to the license file provided in the following folder for further information: C:\Program Data\Beckhoff\Twin CAT\Functions\TF38xx-Machine-Learning\Legal 9.3 XML Exporter Classification of the XML Exporters The XML Exporters provided by Beckhoff can be freely used and changed. They are open source and under MIT license. This offers customers the option of adapting the XML Exporter according to their needs, for example by adding company-specific or project-specific Custom Attributes, cf. XML Tag Custom Attributes [} 74]. The XML exporters are provided after installation of the product in the folder <Twin CATPath>\Functions\TF38xx-Machine-Learning\Utilities\exporter. The XML Exporters are based on special versions of the libraries It is recommended to export created ML models via the ONNX format as well as to convert correspondingly to XML or BML. In the early phase of Twin CAT Machine Learning, the XML Exporters were intended as a transitional solution until all relevant libraries offered comprehensive ONNX support. This is the case today, so XML Exporters are no longer tested and updated with newer libraries. Direct XML export of an MLP Only network architectures that have a sequential structure in the following sense are supported: each neuron of one layer is exclusively linked to each neuron of the following layer. It is possible to export layers without bias. Export from Keras / Tensor Flow • File: Keras Mlp2Xml.py • Sample call: https://infosys.beckhoff.com/content/1033/tf38x0_tc3_ML_NN_Inference_Engine/ Resources/8746685963.zip • Requirements for the Python environment: ◦ Keras with Tensor Flow backend (Tensor Flow Version 1.15.0) ◦ Numpy (Version 1.17.4) ◦ Matplotlib • Supported activation functions: tanh, sigmoid, softmax, relu, linear/identity, exp, softplus, softsign • Only sequential models can be exported with the XML Exporter, no functional models. The model is to be generated accordingly with: from tensorflow.keras.models import Sequential model = Sequential() • Dropout layers are supported (only relevant for training, ignored when exporting) • Dense layers are supported. The activation functions must be transferred to the layer as an argument and not as a discrete activation layer. from keras.layers import Activation, Dense # this will not work !!! model.add(Dense(64)) model.add(Activation(‘tanh’)) # this will work model.add(Dense(64,activation=’tanh’)) • The API is described in detail in the header of the file Keras Mlp2Xml.py. net2xml(net,output_scaling_bias=None,output_scaling_scal=None) ◦ net, obligatory, class of the trained model ◦ output_scaling_bias, optional, list (in case of several features), otherwise float or int ◦ output_scaling_scal, optional, list (in case of several features), otherwise float or int ◦ A string document is returned that can be saved as an XML. 98 Version: 1.7.0 TF3800, TF3810
## Page 99

Appendix Export from MATLAB® • File: Matlab Mlp2Xml.m • Sample call: https://infosys.beckhoff.com/content/1033/tf38x0_tc3_ML_NN_Inference_Engine/ Resources/8746880267.zip • Requirements for the MATLAB® environment: ◦ MATLAB® ◦ Deep Learning Toolbox • Supported activation functions: tanh, sigmoid, softmax, relu, linear/identity • Supported models of the Deep Learning Toolbox: fitnet, patternnet • Process Fcns are supported by the types mapminmax and mapstd • The use of Process Fcns is optional • If a Process Fcn is used in the output layer, its activation function must be purelin • The API is described in detail in the header of the file Matlab Mlp2Xml.m Matlab Mlp2Xml(net, fnstr, varargin) • net, obligatory, class of the trained model • fnstr, obligatory, string with path and file name • output_scaling_bias, optional, vector • output_scaling_scal, optional, vector Direct XML export of an SVM Export from Scikit-learn • File: Sci Kit Learn Svm2Xml.py • Sample call: https://infosys.beckhoff.com/content/1033/tf38x0_tc3_ML_NN_Inference_Engine/ Resources/8746882571.zip • Requirements for the Python environment: ◦ Python Interpreter: 3.6 or higher ◦ Scikit-learn: Version 0.22.0 or higher ◦ Matplotlib ◦ Numpy • Only numerical class labels can be exported. • Supported classes or models: SVC, Nu SVC, One Class SVM, SVR, Nu SVR ◦ Linear SVR and Linear SVC are not supported by the Exporter, but can alternatively be implemented via the classes SVR and SVC, each with a linear kernel. • Supported kernel functions: linear, rbf, sigmoid, polynomial ◦ Neither individual kernel functions nor precomputed functions are supported • Remarks about model parameters: ◦ Gamma = scale is not supported ◦ Gamma = auto_deprecated: it is exported gamma = 0.0 ◦ Gamma = auto: it is exported gamma = 1/n_features. ◦ C = inf is not supported ◦ decision_function_shape = ovr is not supported. decision_function_shape = ovo must be used. Default in Scikit-learn is ovr! ◦ break_ties is ignored because decision_function_shape = ovr is not supported. • The API is described in detail in the header of the file Sci Kit Learn Svm2Xml.py. svm2xml(svm, input_scaling_bias=None, input_scaling_scal=None) ◦ net, obligatory, class of the trained model ◦ input_scaling_bias, optional, list (in case of several features), otherwise float or int TF3800, TF3810 Version: 1.7.0 99
## Page 100

Appendix ◦ input_scaling_scal, optional, list (in case of several features), otherwise float or int ◦ A string document is returned that can be saved as an XML. 9.3.1 XML Exporter - samples Small samples of the use of the XML Exporter [} 98] provided by Beckhoff can be downloaded here. • Export of an MLP from Keras/Tensor Flow: https://infosys.beckhoff.com/content/1033/ tf38x0_tc3_ML_NN_Inference_Engine/Resources/8746685963.zip • Export of an MLP from MATLAB®: https://infosys.beckhoff.com/content/1033/ tf38x0_tc3_ML_NN_Inference_Engine/Resources/8746880267.zip • Export of an SVM from Scikit-learn: https://infosys.beckhoff.com/content/1033/ tf38x0_tc3_ML_NN_Inference_Engine/Resources/8746882571.zip 100 Version: 1.7.0 TF3800, TF3810
## Page 101

Trademark statements Beckhoff®, Twin CAT®, Twin CAT/BSD®, TC/BSD®, Ether CAT®, Ether CAT G®, Ether CAT G10®, Ether CAT P®, Safety over Ether CAT®, Twin SAFE®, XFC®, XTS® and XPlanar® are registered trademarks of and licensed by Beckhoff Automation Gmb H. Third-party trademark statements Debian is a registered trademark owned by Software in the Public Interest, Inc. DSP System Toolbox, Embedded Coder, MATLAB, MATLAB Coder, MATLAB Compiler, Math Works, Predictive Maintenance Toolbox, Simscape, Simscape™ Multibody™, Simulink, Simulink Coder, Stateflow and Thing Speak are registered trademarks of The Math Works, Inc. The registered trademark Linux® is used pursuant to a sublicense from the Linux Foundation, the exclusive licensee of Linus Torvalds, owner of the mark on a worldwide basis. Microsoft, Microsoft Azure, Microsoft Edge, Power Shell, Visual Studio, Windows and Xbox are trademarks of the Microsoft group of companies.
## Page 102

More Information: www.beckhoff.com/tf3800 Beckhoff Automation Gmb H & Co. KG Hülshorstweg 20 33415 Verl Germany Phone: +49 5246 9630 info@beckhoff.com www.beckhoff.com
