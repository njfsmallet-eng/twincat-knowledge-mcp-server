---
title: "TF3520_TC3_Analytics_Storage_Provider_EN"
product: "TF3520"
category: "Analytics"
tags: ["ANALYTICS", "PROVIDER", "STORAGE"]
language: "EN"
document_type: "Manual"
version: "1.7.1"
source_pdf: "https://download.beckhoff.com/download/Document/automation/twincat3/TF3520_TC3_Analytics_Storage_Provider_EN.pdf"
release_date: "2025-10-23"
---
Manual | EN TF3520 Twin CAT 3 | Analytics Storage Provider 2025-10-23 | Version: 1.7.1
## Page 3

Table of contents Table of contents 1 Foreword.................................................................................................................................................... 5 1.1 Notes on the documentation............................................................................................................. 5 1.2 For your safety.................................................................................................................................. 5 1.3 Notes on information security............................................................................................................ 7 2 Overview.................................................................................................................................................... 8 3 Installation............................................................................................................................................... 10 3.1 System requirements...................................................................................................................... 10 3.2 Installation....................................................................................................................................... 10 3.3 Installation under Twin CAT4026..................................................................................................... 13 3.4 Licensing......................................................................................................................................... 13 3.5 Installing the Twin CAT/BSD............................................................................................................ 15 4 Analytics Workflow - First Steps........................................................................................................... 17 4.1 Recording data from the machine................................................................................................... 17 4.2 Communication............................................................................................................................... 20 4.3 Historicize data................................................................................................................................ 21 4.4 Importing/converting Analytics Files................................................................................................ 28 4.5 Analyse data................................................................................................................................... 30 4.6 24h Analytics application................................................................................................................. 34 5 Technical introduction............................................................................................................................ 43 6 Configuration........................................................................................................................................... 45 6.1 Configurator.................................................................................................................................... 45 6.1.1 Generic Configurations.................................................................................................... 45 6.1.2 Additional Configurations................................................................................................. 46 6.1.3 Importing/converting Analytics Files................................................................................. 55 6.2 Manager.......................................................................................................................................... 56 6.2.1 Manager ("Recorder")...................................................................................................... 57 6.2.2 Data handling with Rule Engine....................................................................................... 62 6.3 Working with Historical Data........................................................................................................... 95 6.4 Console Configurator/Client............................................................................................................ 99 6.4.1 Configurator................................................................................................................... 100 6.4.2 Client.............................................................................................................................. 104 6.4.3 Batch files for control...................................................................................................... 107 7 PLC API.................................................................................................................................................. 113 7.1 Function blocks............................................................................................................................. 113 7.1.1 Topic Architecture.......................................................................................................... 113 7.1.2 FB_ALY_Storage Provider.............................................................................................. 131 7.1.3 FB_ALY_Active Recordings............................................................................................ 141 7.2 Data types..................................................................................................................................... 144 7.2.1 ST_ALY_SP_Config....................................................................................................... 144 7.2.2 ST_Connection Settings................................................................................................. 144 7.2.3 ST_Recording Status...................................................................................................... 145 7.2.4 E_Cancel Type................................................................................................................ 145 7.2.5 E_Control Mode.............................................................................................................. 146 TF3520 Version: 1.7.1 3
## Page 4

Table of contents 7.2.6 E_Data Storage Type....................................................................................................... 146 7.2.7 E_Raw Data Format......................................................................................................... 147 7.2.8 E_Recording State.......................................................................................................... 147 7.2.9 E_Record Mode.............................................................................................................. 147 7.2.10 E_Reload Type............................................................................................................... 148 7.2.11 E_Ring Buffer Mode......................................................................................................... 148 7.2.12 E_Time Mode.................................................................................................................. 148 7.2.13 E_Set Get Historical Data State......................................................................................... 149 7.2.14 E_Storage State.............................................................................................................. 149 7.2.15 E_Symbol Mode.............................................................................................................. 150 7.2.16 E_Pipeline Cmd Type...................................................................................................... 150 8 Samples................................................................................................................................................. 151 8.1 PLC Client..................................................................................................................................... 151 9 Appendix................................................................................................................................................ 159 9.1 Glossary........................................................................................................................................ 159 9.2 FAQ - frequently asked questions and answers........................................................................... 161 9.3 Third-party components................................................................................................................ 162 10 Support and Service............................................................................................................................. 163 4 Version: 1.7.1 TF3520
## Page 5

Foreword 1 Foreword 1.1 Notes on the documentation This description is intended exclusively for trained specialists in control and automation technology who are familiar with the applicable national standards. The documentation and the following notes and explanations must be complied with when installing and commissioning the components. The trained specialists must always use the current valid documentation. The trained specialists must ensure that the application and use of the products described is in line with all safety requirements, including all relevant laws, regulations, guidelines, and standards. Disclaimer The documentation has been compiled with care. The products described are, however, constantly under development. We reserve the right to revise and change the documentation at any time and without notice. Claims to modify products that have already been supplied may not be made on the basis of the data, diagrams, and descriptions in this documentation. Trademarks Beckhoff®, ATRO® , Ether CAT®, Ether CAT G®, Ether CAT G10®, Ether CAT P®, MX-System®, Safety over Ether CAT®, TC/BSD®, Twin CAT®, Twin CAT/BSD®, Twin SAFE®, XFC®, XPlanar®, and XTS® are registered and licensed trademarks of Beckhoff Automation Gmb H. If third parties make use of the designations or trademarks contained in this publication for their own purposes, this could infringe upon the rights of the owners of the said designations. Ether CAT® is a registered trademark and patented technology, licensed by Beckhoff Automation Gmb H, Germany. Copyright © Beckhoff Automation Gmb H & Co. KG, Germany. The distribution and reproduction of this document, as well as the use and communication of its contents without express authorization, are prohibited. Offenders will be held liable for the payment of damages. All rights reserved in the event that a patent, utility model, or design are registered. Third-party trademarks Trademarks of third parties may be used in this documentation. You can find the trademark notices here: https://www.beckhoff.com/trademarks. 1.2 For your safety Safety regulations Read the following explanations for your safety. Always observe and follow product-specific safety instructions, which you may find at the appropriate places in this document. Exclusion of liability All the components are supplied in particular hardware and software configurations which are appropriate for the application. Modifications to hardware or software configurations other than those described in the documentation are not permitted, and nullify the liability of Beckhoff Automation Gmb H & Co. KG. TF3520 Version: 1.7.1 5
## Page 6

Foreword Personnel qualification This description is only intended for trained specialists in control, automation, and drive technology who are familiar with the applicable national standards. Signal words The signal words used in the documentation are classified below. In order to prevent injury and damage to persons and property, read and follow the safety and warning notices. Personal injury warnings DANGER Hazard with high risk of death or serious injury. WARNING Hazard with medium risk of death or serious injury. CAUTION There is a low-risk hazard that could result in medium or minor injury. Warning of damage to property or environment NOTICE The environment, equipment, or data may be damaged. Information on handling the product This information includes, for example: recommendations for action, assistance or further information on the product. 6 Version: 1.7.1 TF3520
## Page 7

Foreword 1.3 Notes on information security The products of Beckhoff Automation Gmb H & Co. KG (Beckhoff), insofar as they can be accessed online, are equipped with security functions that support the secure operation of plants, systems, machines and networks. Despite the security functions, the creation, implementation and constant updating of a holistic security concept for the operation are necessary to protect the respective plant, system, machine and networks against cyber threats. The products sold by Beckhoff are only part of the overall security concept. The customer is responsible for preventing unauthorized access by third parties to its equipment, systems, machines and networks. The latter should be connected to the corporate network or the Internet only if appropriate protective measures have been set up. In addition, the recommendations from Beckhoff regarding appropriate protective measures should be observed. Further information regarding information security and industrial security can be found in our https://www.beckhoff.com/secguide. Beckhoff products and solutions undergo continuous further development. This also applies to security functions. In light of this continuous further development, Beckhoff expressly recommends that the products are kept up to date at all times and that updates are installed for the products once they have been made available. Using outdated or unsupported product versions can increase the risk of cyber threats. To stay informed about information security for Beckhoff products, subscribe to the RSS feed at https:// www.beckhoff.com/secinfo. TF3520 Version: 1.7.1 7
## Page 8

Overview 2 Overview With the Twin CAT Analytics Storage Provider, Beckhoff offers a way to continuously store high-resolution data in a binary format or in plain text. The decisive factor is that the user does not have to worry about data storage. The storage provider takes care of this automatically. The configuration is done with a few clicks in engineering. Complex SQL commands are not necessary. Classic databases can be used, but also binary blob stores. Components • Twin CAT Analytics Storage Provider service: A Windows service that manages the communication. • Twin CAT Analytics Storage Provider PLC Library: A Twin CAT 3 PLC library with functions for controlling the storage provider from a PLC application. • Twin CAT Analytics Storage Provider Client: A console application with functions for controlling the Storage Provider via the command line. • Twin CAT Analytics Storage Provider Configurator: An engineering application for configuring the various data sinks and data sources. • Twin CAT Analytics Storage Provider Manager: An engineering application for managing the recorded data and controlling the Storage Provider. Principle of operation The Analytics Storage Provider receives and sends data via MQTT communication protocol. For this purpose, it is connected to a native MQTT message broker in the network and on the other side to the corresponding data sink. Supported databases/storage • Twin CAT Analytics Binary File [} 48] • Microsoft SQL (binary format / plain text [} 49]) • CSV file [} 51] • Influx DB [} 52] 8 Version: 1.7.1 TF3520
## Page 9

Overview • Microsoft Azure Blob [} 53] TF3520 Version: 1.7.1 9
## Page 10

Installation 3 Installation 3.1 System requirements The requirements of the Service and the PLC library of the Analytics Storage Provider can be found in the following tables. It is also possible to install both on one system as well. Technical data Service TF3520 Twin CAT 3 Analytics Storage Provider Target System Windows 10, Twin CAT/BSD . NET Framework . Net 4.5.1 or higher Min. Twin CAT version 3.1.4022.25 Min. Twin CAT level TC1000 | Twin CAT 3 ADS Technical data Library TF3520 Twin CAT 3 Analytics Storage Provider Target System Windows 10, Twin CAT/BSD Min. Twin CAT version 3.1.4022.29 Min. Twin CAT level TC1200 | Twin CAT 3 PLC 3.2 Installation Setup installation (Twin CAT 3.1 Build 4024) The following section describes how to install the Twin CAT 3 function for Windows-based operating systems. ü The Twin CAT 3 function setup file was downloaded from the Beckhoff website. 1. Run the setup file as administrator. To do this, select the Run As Admin command in the context menu of the file. ð The installation dialog opens. 2. Accept the end user licensing agreement and click Next. 10 Version: 1.7.1 TF3520
## Page 11

Installation 3. Enter your user data. 4. If you want to install the full version of the Twin CAT 3 function, select Complete as the installation type. If you want to install the Twin CAT 3 function components separately, select Custom. TF3520 Version: 1.7.1 11
## Page 12

Installation 5. Click Next, then Install to start the installation. ð A dialog box informs you that the Twin CAT system must be stopped to proceed with the installation. 6. Confirm the dialog with Yes. 12 Version: 1.7.1 TF3520
## Page 13

Installation 7. Click Finish to exit the setup. ð The Twin CAT 3 function has been installed successfully. 3.3 Installation under Twin CAT4026 Twin CAT Package Manager If you are using Twin CAT 3.1 Build 4026 (and higher) on the Microsoft Windows operating system, you can install this function via the Twin CAT Package Manager, see Installation documentation. Normally you install the function via the corresponding workload; however, you can also install the packages contained in the workload individually. This documentation briefly describes the installation process via the workload. Command line program Tc Pkg You can use the Tc Pkg Command Line Interface (CLI) to display the available workloads on the system: tcpkg list TF3520 You can use the following command to install the Workload of the TF3520 TC3 Analytics Storage Provider function. tcpkg install TF3520. Analytics Storage Provider. XAE tcpkg install TF3520. Analytics Storage Provider. XAR Twin CAT Package Manager UI You can use the User Interface (UI) to display all available workloads and install them if required. To do this, follow the corresponding instructions in the interface. 3.4 Licensing The Twin CAT 3 function can be activated as a full version or as a 7-day test version. Both license types can be activated via the Twin CAT 3 development environment (XAE). TF3520 Version: 1.7.1 13
## Page 14

Installation Licensing the full version of a Twin CAT 3 Function A description of the procedure to license a full version can be found in the Beckhoff Information System in the documentation "Twin CAT 3 Licensing". Licensing the 7-day test version of a Twin CAT 3 Function A 7-day test version cannot be enabled for a Twin CAT 3 license dongle. 1. Start the Twin CAT 3 development environment (XAE). 2. Open an existing Twin CAT 3 project or create a new project. 3. If you want to activate the license for a remote device, set the desired target system. To do this, select the target system from the Choose Target System drop-down list in the toolbar. ð The licensing settings always refer to the selected target system. When the project is activated on the target system, the corresponding Twin CAT 3 licenses are automatically copied to this system. 4. In the Solution Explorer, double-click License in the SYSTEM subtree. ð The Twin CAT 3 license manager opens. 5. Open the Manage Licenses tab. In the Add License column, check the check box for the license you want to add to your project (e.g. "TF4100 TC3 Controller Toolbox"). 6. Open the Order Information (Runtime) tab. ð In the tabular overview of licenses, the previously selected license is displayed with the status “missing”. 14 Version: 1.7.1 TF3520
## Page 15

Installation 7. Click 7-Day Trial License... to activate the 7-day trial license. ð A dialog box opens, prompting you to enter the security code displayed in the dialog. 8. Enter the code exactly as it is displayed and confirm the entry. 9. Confirm the subsequent dialog, which indicates the successful activation. ð In the tabular overview of licenses, the license status now indicates the expiry date of the license. 10. Restart the Twin CAT system. ð The 7-day trial version is enabled. 3.5 Installing the Twin CAT/BSD The Twin CAT 3 Analytics Storage Provider Server is available as a package for Twin CAT/BSD in the package repository. Under the package name "TF3520 Analytics Storage Provider" it can be installed via the following command: doas pkg install TF3520-Analytics-Storage-Provider Further information about the Package Server can be found in the Embedded PC section of the Twin CAT/ BSD manual. After a system restart or restart of Twin CAT, the Twin CAT 3 Analytics Storage Provider Server is also started and can be configured by a client via MQTT. MQTT port enabling To use the Analytics Storage Provider and Console Configurator/Client [} 99] under Twin CAT/ BSD, the corresponding MQTT port must be enabled for communication. For more info see: Port enabling under Twin CAT/BSD After installation, the Client.dll for the console is located under the path /usr/local/etc/Twin CAT/Functions/ TF3520-Analytics-Storage-Provider/Client. TF3520 Version: 1.7.1 15
## Page 16

Installation The Analytics Storage Provider service can be started by the following command if the license is activated: doas service Tc Analytics Storage Provider start 16 Version: 1.7.1 TF3520
## Page 17

Analytics Workflow - First Steps 4 Analytics Workflow - First Steps This step by step documentation presents the complete Twin CAT Analytics workflow. From the data acquisition over the communication and historizing up to the evaluation and analysis of the data and to the presentation of the data in web-based dashboard. 4.1 Recording data from the machine On the machine side is the Analytics Logger the recorder of process data from the machine image, PLC, NC and so on. The Logger is working in the real-time context of Twin CAT. The Twin CAT Analytics Logger is installed with Twin CAT XAE and XAR. The Logger can act as MQTT Client to communicate the recorded data to a native MQTT Message Broker or store the data in the same data format in a local binary file. By the usage as MQTT Client the Logger is able to bypass short disconnects to the Message Broker with a ring buffer functionality. You can configure a ring buffer as well for the local binary file storage. • To configure the Analytics Logger you have to navigate in your existing Twin CAT Project to the Analytics tree node TF3520 Version: 1.7.1 17
## Page 18

Analytics Workflow - First Steps • Right click on this node and click on “Add Data Logger” to add one new instance to your configuration • For configuring the base settings, please double click on the new tree item You can make your specific Analytics Logger settings -Data Format: Binary file or MQTT stream -FILE format: Analytics Logger stores the data in local binary files and all other settings are not necessary anymore. The files will be stored in C:\Twin CAT\3.1\Boot\Analytics. -BINARY: Data will be sent to the configured MQTT Message Broker. You can have multiple Logger in one Twin CAT project to communicate data to different MQTT Message Broker. -Data Compression: on (default) or off 18 Version: 1.7.1 TF3520
## Page 19

Analytics Workflow - First Steps -Max Compression: mode of the compression -MQTT host name -MQTT Tcp port -MQTT main topic for own hierarchical levels to keep the identification easy -MQTT Client ID should be unique in the network -MQTT username -MQTT password to make authentication at the message broker -At the TLS (Transport Layer Security) tab, security settings can be configured. TLS is a secure communication channel between client and server. By the usage of certificates, the TCP port 8883 is exclusively reserved for MQTT over TLS. Analytics Logger is supporting the modes CA Certificates, CA Certificates & Client Certificate and Preshared Key (PSK) mode. • If variables in your PLC application are marked in the declaration with the attribute {attribute 'Tc Analytics'} they will be shown automatically as a stream below the Data Logger tree node. An additional device stream will be shown if your configuration provides an Ether CAT Process Image. TF3520 Version: 1.7.1 19
## Page 20

Analytics Workflow - First Steps • In the stream a Selection tab is available to choose the variables that should be recorded • Finally it is possible to change the package size for the frames or to configure the ring buffer for disconnects and file in the Data Handling tab. 4.2 Communication Currently, the Analytics workflow is fully mappable via MQTT. The engineering tools can also access the data of the machines via ADS and carry out analyzes. 20 Version: 1.7.1 TF3520
## Page 21

Analytics Workflow - First Steps If you choose for the Io T communication protocol MQTT you have to setup a native MQTT Message Broker somewhere in the network (VM in a cloud system is also possible). This Message Broker provides a decoupling of the different applications in the Analytics Workflow. 4.3 Historicize data After the Twin CAT Analytics Storage Provider has been installed, the service running in the background can be configured. You will find the Twin CAT Analytics. Storage Provider. Configurator application in the folder C: \Twin CAT\Functions\TF3520-Analytics-Storage Provider\Tools. TF3520 Version: 1.7.1 21
## Page 22

Analytics Workflow - First Steps The main part of the topic can be defined in the configuration as well as the comment, which is used for identification if more than one Storage Provider is registered with the message broker. You can make the message broker settings and decide on a storage type: • Analytics File (binary file) • CSV file • Microsoft SQL (binary / plain text) • Inlfux DB (plain text) • Microsoft Azure Blob (Azure Cloud required) At last you can save the configuration and start the service. The next step is to configure the specific recording. For this you should select the Storage Provider Manager in your development environment. 22 Version: 1.7.1 TF3520
## Page 23

Analytics Workflow - First Steps With the Storage Provider Recorder recording definitions can be created, started and managed. In addition, it is possible to manage the data memories of individual Analytics Storage Providers. All important properties of the found Analytics Storage Providers and historized data are clearly displayed. TF3520 Version: 1.7.1 23
## Page 24

Analytics Workflow - First Steps Toolbar Manager window ("OVERVIEW") 1 Add new broker 2 Remove selected broker 3 Refresh display 4 Collapse all nodes 5 View switch between dark/light mode Function Manager window ("OVERVIEW") First assign a Recorder Alias. This helps to group the started recordings and to find its self started ones again. After that, one or more brokers can be set up. This is done via the already known input mask for MQTT connection properties. Once a connection to the broker could be established, all Analytics Storage Providers connected to it will be listed. "Storage" status 24 Version: 1.7.1 TF3520
## Page 25

Analytics Workflow - First Steps 1 Storage Online 2 Storage Offline 3 Storage starts 4 Storage starts with error. Still trying to start it 5 Storage is shut down 6 Storage is in the error state Toolbar Manager window ("CONFIGURATIONS") 1 Create a new pipeline 2 Create a new pipeline with Rule Engine 3 Open Target Browser for connecting simple pipelines 4 Edit a selected pipeline 5 Delete a selected pipeline 6 Start a selected pipeline Function Manager window ("CONFIGURATIONS") The window is divided into two tabs. Pipelines and Live Status. Under Pipelines you will find the configurations of your pipelines. You can define new pipelines from here. Edit existing. Delete or start. To create a new simple pipeline, click the "Create new pipeline" button. The following dialog opens. TF3520 Version: 1.7.1 25
## Page 26

Analytics Workflow - First Steps You can now drag and drop the symbols you want to record from the Target Browser into the dialog. You also assign a Recording Alias and a Record Name. Various placeholders are available for the Record Name: "{Auto ID}" "{Topic}" "{System ID}" "{Layout}" "{Cycle Time}" "{Sample Size}" "{Record Start}" You can also configure recording names and a duration (otherwise the recording will run endlessly until it is stopped manually). A ring buffer can be set according to storage space or time. The entries are confirmed with OK and a new local recording definition is created. It is now possible to start this definition directly via the toolbar or the context menu. However, it is also possible to make the definition globally accessible. This can be done via the context menu with the entry Publish Recording. The following dialog then opens: 26 Version: 1.7.1 TF3520
## Page 27

Analytics Workflow - First Steps Here you can now select the desired Analytics Storage Provider via which the definition is to be published. In addition, the definition is assigned a Storage and a Data Broker of the selected Analytics Storage Provider. After the selection, the recording definition is confirmed with OK and published to the selected Analytics Storage Provider. This means that it can be found by any Storage Provider Manager that is connected to the MQTT Broker. After starting a pipeline, the view automatically jumps to the second tab, the Live Status. All active recordings from all users are listed here. The recordings can be ended in this tab and it is also possible to jump to the resulting record. Use historized data After and also during recording, you can select the historical data as input for your analysis in Target Browser. In the Target Browser, you will find a new control on the right side for the historical data. There you can select the timespan for your data. TF3520 Version: 1.7.1 27
## Page 28

Analytics Workflow - First Steps 4.4 Importing/converting Analytics Files In the following it is assumed that you have installed Twin CAT under "C:/Twin CAT". Otherwise, you must adjust the specified paths accordingly. You can import recordings from the Analytics Logger stored in Analytics File Format (Analytics.tas, Analytics- <Date>.tay) into the Storage Provider. In general, you can convert data saved by the Storage Provider as an Analytics File into other formats. Analytics File is always the source format. To do this, perform the following steps: 1. Save the folder with the Analytics Files in your Storage Provider location By default, here: C:\Twin CAT\Functions\TF3520-Analytics-Storage Provider\Storage\Analytics Storage Provider (create the folders manually if they do not exist) 28 Version: 1.7.1 TF3520
## Page 29

Analytics Workflow - First Steps 2. Open Twin CAT. Analytics. Storage Provider. Configurator.exe. The program can be found under the path C:\Twin CAT\Functions\TF3520-Analytics-Storage Provider\Tools 3. Select the Storage into which the Analytics File is to be imported and press the Data Import button. 4. Then select the path to the Analytics File in the Data Import dialog and enter all other known parameters for this recording. 5. Press OK and the data import begins. ð Now you can see your imported data in the Twin CAT Target Browser [} 95]. TF3520 Version: 1.7.1 29
## Page 30

Analytics Workflow - First Steps ð You may have to wait a short time or restart your Storage Provider. 4.5 Analyse data ü Open your Twin CAT Engineering environment to start the data analysis. 1. Open Visual Studio® > File > New > Project… 2. Select the Analytics project template from Twin CAT Measurement. ð The new project is displayed in the Solution Explorer. After clicking the Analytics Project tree node element a start window opens where you can select your first action. From here you can add a network, open the Toolbox, open the Target Browser or open the Analytics Storage Provider Recorder. In the following steps you will perform all these actions. 30 Version: 1.7.1 TF3520
## Page 31

Analytics Workflow - First Steps 3. It makes sense to open the Toolbox of Visual Studio® first. There you will find all the algorithms supported by Twin CAT Analytics. Algorithms need to be grouped and organized into networks. Right- click Analytics Project to add a new network, or add a network using the start page. The first network is always generated by default. 4. When you click on the network, an editor opens. Now you can drag and drop the desired algorithm into the editor interface. 5. After selecting the algorithm, you need to connect input variables to the modules (algorithm). To do this, open the Target Browser. Twin CAT > Target Browser > Target Browser 6. Now select the Tc Analytics or Tc Analytics File tab in the Target Browser. Continue with the tab Tc Analytics (MQTT). TF3520 Version: 1.7.1 31
## Page 32

Analytics Workflow - First Steps 7. Click the icon highlighted in green in the toolbar of this Analytics extension. A window opens in which you can specify the connectivity data of your message broker. 8. Select your MQTT Analytics client (Twin CAT Analytics Logger, Twin CAT Io T Data Agent or Beckhoff EK9160). There is a unique ID for each control. This ID is displayed in the Target Browser. 9. Clicking on the gear icon, you will get to the Machine Administration page. Here you can assign a system alias name that will be displayed in the Target Browser instead of the ID. 10. In the next step, you can choose between live data and historical data for each MQTT Analytics client. In this case, the historical data is provided by the Twin CAT Analytics Storage Provider. 32 Version: 1.7.1 TF3520
## Page 33

Analytics Workflow - First Steps 11. You can drag and drop the variables into the inputs of the specific algorithm. In most algorithms, conditions such as thresholds, time intervals, logical operators etc. can be specified. These settings are made in the middle of each module. ð Finally, your first Analytics Project is complete. To start the analysis, click Start Analytics. To stop the analysis, click Stop Analytics. ð Before starting the analysis or during runtime, you can click the Add Reference Scope button. This will automatically create a Scope configuration that matches your Analytics project. TF3520 Version: 1.7.1 33
## Page 34

Analytics Workflow - First Steps ð The analysis results can be displayed in the Scope View graphs using drag-and-drop. For example, a mean value can be displayed as a new channel in the view. Timestamps as markers on the X-axes show significant values. 4.6 24h Analytics application The last major step in the Twin CAT Analytics workflow is the continuous 24-hour machine analysis. It runs in parallel with the machine applications in the field. To make this very easy, the Twin CAT Analytics Workbench can automatically generate PLC code and an HTML5-based dashboard of your Analytics configuration. Both can be downloaded into a Twin CAT Analytics Runtime (TC3 PLC and HMI Server) and provide the same analysis results as the configurator tool in the engineering environment. ü First, save your configuration and open the Analytics Deploy Runtime Wizard. This can be done from the context menu in the Analytics Project tree item or from the start page. 1. When the wizard is open, you can click through some tabs. The first one is called Solution. Here you can decide how your Analytics project should be used in the PLC code: As... completely new solution. 34 Version: 1.7.1 TF3520
## Page 35

Analytics Workflow - First Steps part of an existing solution. update of an existing Analytics solution. TF3520 Version: 1.7.1 35
## Page 36

Analytics Workflow - First Steps 2. In the Twin CAT PLC Target tab you can select the ADS target system that runs the Twin CAT Analytics Runtime (TF3550). The created project is immediately executable. For this purpose you can set the Activate PLC Runtime option. In addition, it can be selected that a boot project is created directly. 3. Especially for virtual machines, it is important to run the project on isolated cores, which is also an option in this tab. The next tab Results is needed only if you have selected the Stream Results option in the algorithm properties. If you want to send results, you can decide here in which way (locally in a file/ through MQTT) and which format (binary/JSON) this should be done. This is also generated automatically and executed immediately after activation. 36 Version: 1.7.1 TF3520
## Page 37

Analytics Workflow - First Steps Downsampling of the results is possible by specifying a cycle time. The next tab is for the HMI Dashboard. A prerequisite for the automatic generation of the dashboard is the selection of HMI Controls for the corresponding algorithms whose results are to be displayed in the dashboard. TF3520 Version: 1.7.1 37
## Page 38

Analytics Workflow - First Steps 4. You can choose different options for your Analytics Dashboard, such as a start page with a map, layouts, sorting algorithms, custom colors and logos. If you select multiple languages for the Analytics Controls, a language switching menu will also be generated. 38 Version: 1.7.1 TF3520
## Page 39

Analytics Workflow - First Steps 5. Select one of the installed versions of Visual Studio® and, whether the instance should start visibly or just be set up and activated in the background. TF3520 Version: 1.7.1 39
## Page 40

Analytics Workflow - First Steps ð At last you can find an overview. 40 Version: 1.7.1 TF3520
## Page 41

Analytics Workflow - First Steps 6. Now you can click the Deploy button to start the generation process. The PLC project and the HMI dashboard are now generated. TF3520 Version: 1.7.1 41
## Page 42

Analytics Workflow - First Steps ð After the "Deploy Runtime succeeded" message, you will find a new Visual Studio®/XAE shell instance on your desktop. The new Solution and both projects are created. 42 Version: 1.7.1 TF3520
## Page 43

Technical introduction 5 Technical introduction The basic idea of the Twin CAT Analytics Storage Provider (ASP) is to have a gateway that largely frees the user from configuring a data sink, i.e. a storage or a database. The user does not need to set up his own table structure in a database. He only has to configure which of the supported data sinks he wants to use for storing his data. Service Management The Analytics Storage Provider service can run anywhere on the network. It is implemented as a Windows service. The service can run on hardware devices, such as industrial PCs or embedded PCs in the local network, and also on virtual machines in the same network, or in a cloud system, for example. Data Management The Storage Provider works with the binary format of Twin CAT Analytics. This allows it to receive and store streams from an MQTT message broker and to create and send new streams itself. The user only needs the recorder, which is integrated with the Twin CAT Analytics Workbench or the service tool in his own engineering system. The variables themselves are displayed in the Twin CAT Target Browser. For the Analytics binary format, they are divided into live and historical data. Live data can be used as input to the Analytics Storage Provider. Historical data are the values from the database/storage provided by the Storage Provider. Topologies The many degrees of freedom offered by Io T technologies enable different topologies. The following picture shows the most important constellations. 1. Each SW package runs on its own HW device or virtual machine. 2. The Analytics Storage Provider Windows service runs on the same device as the database/storage. 3. Analytics Engineering, Analytics Storage Provider, and database or storage are on the same device. Only the Message Broker and Analytics Logger (data source) run on other devices. TF3520 Version: 1.7.1 43
## Page 44

Technical introduction 4. In this topology view, only the Analytics Logger runs on its own PC. This may be the case in a machine application. All other tools in the Analytics tool chain reside on one device, including the MQTT message broker. Topologies with additional ASP clients Currently, two additional clients are available from the Analytics Storage Provider perspective. A command line based client that allows execution from almost any application. And a PLC library that can also be used to influence the actions of the storage provider. 44 Version: 1.7.1 TF3520
## Page 45

Configuration 6 Configuration The configuration of the Analytics Storage Provider is divided into two main parts. First, you need to configure the service with its stores. This is done in the Twin CAT Analytics Storage Provider Configurator. You must also configure the recordings and pipelines yourself. In other words, which variables should be stored in which stores under which conditions. To do this, go to the Twin CAT Analytics Storage Provider Manager. In this chapter you will also find the supported databases and storages. 6.1 Configurator You can configure the service with its message brokers and stores in the Twin CAT Analytics Storage Provider Configurator. A distinction is made between a Host Message Broker and various Data Message Brokers. A Host Message Broker can also be a Data Message Broker at the same time. The Host Message Broker is special because this is where the Storage Provider's service registers. 6.1.1 Generic Configurations General settings for the Storage Provider service can be made in the Generic Configurations. TF3520 Version: 1.7.1 45
## Page 46

Configuration GENERIC CONFIGURATIONS Provider Alias: Each Analytics Storage Provider Service has its own GUID for identification. You can enter a provider alias in the general configuration area so that this can be replaced by a meaningful name. Main Topic: Basically, you do not have to worry about the topic. However, you can enter the so-called Main Topic here. It describes the first part of the overall topic. Beckhoff-specific additions are then added. This results in a very simple plug-and-play system with Twin CAT Analytics. Host Message Broker: The so-called Host Message Broker can be configured here. The Storage Provider Service logs on to this message broker itself and makes its data available. This broker can also be a so-called Data Message Broker, which is used to receive data to the Storage Provider Service. There can only be one Host Broker in the system, but several Data Brokers. Logging: Check various logging options. Service: Displays the status and default settings for starting the Windows service. 6.1.2 Additional Configurations The Stores and Data Message Broker of the Storage Provider Services are set under Additional Configurations. ADDITIONAL CONFIGURATIONS Storages The stores that are available to the Analytics Storage Provider Service can be created in the Storages tab. These can be of various types, such as Microsoft SQL® or csv files. New stores can be added in binary or plain text format using the plus sign in the bottom right-hand corner. The configuration of the individual stores is described in this sub-chapter [} 47]. 46 Version: 1.7.1 TF3520
## Page 47

Configuration Data Message Broker 1. In the Data Message Broker tab, you can create the message brokers that can provide input data for the Storage Provider in addition to the Host Message Broker. 2. The settings can be checked using the Check Connection button. ð The result is displayed in the following window: If you click on Save, the settings are saved in the directory C:\Twin CAT\Functions\TF3520-Analytics- Storage Provider\Configurations. 6.1.2.1 Databases/Stores The following overview shows which database connections are supported by which platform. TF3520 Version: 1.7.1 47
## Page 48

Configuration Database Windows Twin CAT/BSD Local Remote Local Remote Analytics File X X X X CSV X X X X MS SQL X X - X Postgre SQL X X - X Influx DB 2 X X - X Azure Blob - X - X 6.1.2.1.1 Twin CAT Analytics Binary File Twin CAT Analytics Binary File is a Twin CAT-specific Storage. Therefore no external software is necessary. You can use this type of storage directly after installing the Analytics Storage Provider. This is the same file that the Twin CAT Analytics Logger provides in its “offline” configuration without MQTT Message Broker. Storage Name: Assign a descriptive name that describes the purpose of the storage well. It will reappear in various places during configuration in the Manager. Max Write Length: The amount of data that is written to the .tay file in one call is specified here. Storage Comment: You can enter additional information about the storage here. 48 Version: 1.7.1 TF3520
## Page 49

Configuration File Path: For the configuration, you must select your preferred folder on the local device on which the Storage Provider is running. The folder used is displayed in the Connection String window for confirmation. The folder architecture that is created after the pipeline is started is currently divided into five hierarchical levels: 1. System ID (this is a GUID of the system sending the data) - can be replaced by the alias name in the Analytics Logger settings or in local engineering by the Machine Administration Page. 2. Recording name - can be set using the configurator. 3. Record name - can be set by the configurator and automatically influenced with auto IDs, date, cycle time and other placeholders at runtime. 4. Internal ID - cannot be changed. 5. Layout GUID - cannot be changed and corresponds to the data storage of the Analytics Logger. Max Duration: This value specifies in seconds how much data is written to a .tay file. After time X, the data is written to a new .tay file. Ring buffers that are configured in the Manager only affect complete .tay files of a current recording. 6.1.2.1.2 Microsoft SQL (binary / plain text) With Microsoft SQL Server, you have another On-Premises solution for storing the Analytics binary data. TF3520 Version: 1.7.1 49
## Page 50

Configuration Enter the Connection String for your MS SQL server here. Storage Name: Assign a descriptive name that describes the purpose of the storage well. It will reappear in various places during configuration in the Manager. Max Write Length: The amount of data that is saved in a tbl_Data record is specified here. Storage Comment: You can enter additional information about the storage here. Connection String: Click on the Conn String button to open the input mask. Make the configuration settings there, including for remote databases that are accessible via network connections. 50 Version: 1.7.1 TF3520
## Page 51

Configuration After starting the storage, communication with the database begins. At this point, the Storage Provider itself creates the four required tables. Each recording configuration is saved in a separate table. As an example, you can see the following screenshot from Microsoft SQL Server Management Studio. The ring buffer functionality for the Microsoft SQL® database can also be set in the Manager. 6.1.2.1.3 CSV file Storage Name: Assign a descriptive name that describes the purpose of the storage well. It will reappear in various places during configuration in the Manager. Max Write Length: The amount of data that is written to the .csv file in one call is specified here. Storage Comment: You can enter additional information about the storage here. TF3520 Version: 1.7.1 51
## Page 52

Configuration File Path: For the configuration, you must select your preferred folder on the local device on which the Storage Provider is running. The folder used is displayed in the Connection String window for confirmation. Max Duration: It is also possible to define the timespan to be saved in a CSV file. The decimal places of floating point numbers can also be limited. Decimalplaces: The number of decimal places can be set here. The value "-1" stands for unspecified, the value "2" stands for two decimal places, for example. 6.1.2.1.4 Influx DB 2 With the support of Influx DB, you have another on-premises solution for storing Analytics binary data. Storage Name: Assign a descriptive name that describes the purpose of the storage well. It will reappear in various places during configuration in the Manager. 52 Version: 1.7.1 TF3520
## Page 53

Configuration Max Write Length: The amount of data that is written to the Influx in one call is specified here. Storage Comment: You can enter additional information about the storage here. Connection String: Click on the Conn String button to open the input mask. You can make the configuration settings there, including for remote databases that are accessible via network connections. The server should always be specified with the port number. After starting the storage, communication with the database begins. 6.1.2.1.5 Microsoft Azure Blob To use Microsoft Azure Blob Storage, you need a Microsoft Azure Cloud account. There you get also your individual Connection String for the configuration of the Twin CAT Analytics Storage Provider. Storage Name: Assign a descriptive name that describes the purpose of the storage well. It will reappear in various places during configuration in the Manager. Max Write Length: The amount of data that is written to the blob store in one call is specified here. Storage Comment: You can enter additional information about the storage here. Copy the Connection String into the description field. The storage must be created in Azure itself. TF3520 Version: 1.7.1 53
## Page 54

Configuration Select Storage accounts (classic). After creating the Storage, you will find the secondary Connection String under Access keys. This string must be used in the configuration of the Analytics Storage Provider. 54 Version: 1.7.1 TF3520
## Page 55

Configuration 6.1.3 Importing/converting Analytics Files In the following it is assumed that you have installed Twin CAT under "C:/Twin CAT". Otherwise, you must adjust the specified paths accordingly. You can import recordings from the Analytics Logger stored in Analytics File Format (Analytics.tas, Analytics- <Date>.tay) into the Storage Provider. In general, you can convert data saved by the Storage Provider as an Analytics File into other formats. Analytics File is always the source format. To do this, perform the following steps: 1. Save the folder with the Analytics Files in your Storage Provider location By default, here: C:\Twin CAT\Functions\TF3520-Analytics-Storage Provider\Storage\Analytics Storage Provider (create the folders manually if they do not exist) 2. Open Twin CAT. Analytics. Storage Provider. Configurator.exe. The program can be found under the path C:\Twin CAT\Functions\TF3520-Analytics-Storage Provider\Tools 3. Select the Storage into which the Analytics File is to be imported and press the Data Import button. TF3520 Version: 1.7.1 55
## Page 56

Configuration 4. Then select the path to the Analytics File in the Data Import dialog and enter all other known parameters for this recording. 5. Press OK and the data import begins. ð Now you can see your imported data in the Twin CAT Target Browser [} 95]. ð You may have to wait a short time or restart your Storage Provider. 6.2 Manager You can define your data pipelines in the Twin CAT Analytics Storage Provider Manager. A pipeline is defined in such a way that you can select the data sources (Data Message Broker) and the symbols available there. Optionally pre-process the data using rules and ultimately store the data in one or more stores. The stores must have been created in the Configurator [} 45] for this. 56 Version: 1.7.1 TF3520
## Page 57

Configuration 6.2.1 Manager ("Recorder") The Analytics Storage Provider Recorder is part of the Analytics Engineering setups. Therefore, you can find the recorder in the installation of the Twin CAT Analytics Workbench and the Twin CAT Analytics Service Tool. With the Storage Provider Recorder recording definitions can be created, started and managed. In addition, it is possible to manage the data memories of individual Analytics Storage Providers. All important properties of the found Analytics Storage Providers and historized data are clearly displayed. TF3520 Version: 1.7.1 57
## Page 58

Configuration Toolbar Manager window ("OVERVIEW") 1 Add new broker 2 Remove selected broker 3 Refresh display 4 Collapse all nodes 5 View switch between dark/light mode Function Manager window ("OVERVIEW") First assign a Recorder Alias. This helps to group the started recordings and to find its self started ones again. After that, one or more brokers can be set up. This is done via the already known input mask for MQTT connection properties. 58 Version: 1.7.1 TF3520
## Page 59

Configuration Once a connection to the broker could be established, all Analytics Storage Providers connected to it will be listed. "Storage" status 1 Storage Online 2 Storage Offline 3 Storage starts 4 Storage starts with error. Still trying to start it 5 Storage is shut down 6 Storage is in the error state Toolbar Manager window ("CONFIGURATIONS") TF3520 Version: 1.7.1 59
## Page 60

Configuration 1 Create a new pipeline 2 Create a new pipeline with Rule Engine 3 Open Target Browser for connecting simple pipelines 4 Edit a selected pipeline 5 Delete a selected pipeline 6 Start a selected pipeline Function Manager window ("CONFIGURATIONS") The window is divided into two tabs. Pipelines and Live Status. Under Pipelines you will find the configurations of your pipelines. You can define new pipelines from here. Edit existing. Delete or start. To create a new simple pipeline, click the "Create new pipeline" button. The following dialog opens. You can now drag and drop the symbols you want to record from the Target Browser into the dialog. You also assign a Recording Alias and a Record Name. Various placeholders are available for the Record Name: 60 Version: 1.7.1 TF3520
## Page 61

Configuration "{Auto ID}" "{Topic}" "{System ID}" "{Layout}" "{Cycle Time}" "{Sample Size}" "{Record Start}" You can also configure recording names and a duration (otherwise the recording will run endlessly until it is stopped manually). A ring buffer can be set according to storage space or time. The entries are confirmed with OK and a new local recording definition is created. It is now possible to start this definition directly via the toolbar or the context menu. However, it is also possible to make the definition globally accessible. This can be done via the context menu with the entry Publish Recording. The following dialog then opens: Here you can now select the desired Analytics Storage Provider via which the definition is to be published. In addition, the definition is assigned a Storage and a Data Broker of the selected Analytics Storage Provider. After the selection, the recording definition is confirmed with OK and published to the selected Analytics Storage Provider. This means that it can be found by any Storage Provider Manager that is connected to the MQTT Broker. After starting a pipeline, the view automatically jumps to the second tab, the Live Status. TF3520 Version: 1.7.1 61
## Page 62

Configuration All active recordings from all users are listed here. The recordings can be ended in this tab and it is also possible to jump to the resulting record. Furthermore, a download function is available in the Manager to download data to your local engineering. Select the corresponding record and start the download via the context menu. Regardless of the store in which the data is stored, it is always saved as an Analytics File on the local engineering system. In combination with the data import function [} 55] in the Storage Configurator, the Analytics File also serves as a practical exchange format. Both functions can be used to move data from a CSV file to an Influx DB2, for example, or from a Microsoft SQL® to a CSV file, etc. 6.2.2 Data handling with Rule Engine Rule Engine pipelines can be used to automatically map a wide variety of storage scenarios. Data can be collected from multiple MQTT data sources, processed and stored in Recordings. In addition, data from recordings can be read and processed on an event or timer-controlled basis, as well as manually triggered. Possible processing operations include aggregating, filtering and sampling the data sources. 62 Version: 1.7.1 TF3520
## Page 63

Configuration The Rule Engine pipeline function is available from Storage Provider version 3.15. onwards. 6.2.2.1 Configuration A Rule Engine pipeline is configured graphically in the Rule Engine Pipeline Editor. This can be opened via the Manager. First, the desired Storage Provider must be selected. A Rule Engine pipeline is developed specifically for a Storage Provider. This cannot be changed subsequently. In the Rule Engine Pipeline Editor, the name of the selected Storage Provider is displayed at the top right. TF3520 Version: 1.7.1 63
## Page 64

Configuration An alias can be assigned for a Rule Engine pipeline. The alias is freely selectable and can be edited via the text field at the top left of the Rule Engine Pipeline Editor. 64 Version: 1.7.1 TF3520
## Page 65

Configuration 6.2.2.1.1 Adding elements Various elements can be dragged from the toolbox (left) in the Rule Engine Pipeline Editor into the configuration area (center). The following elements can be used: MQTT-Source: Corresponds to a data source (e.g. Twin CAT Analytics Logger or Twin CAT Io T Data Agent). Rule: Pre-processing steps can be defined with a rule. Recording: Describes a data recording (previously known recording). TF3520 Version: 1.7.1 65
## Page 66

Configuration 6.2.2.1.2 Linking elements The added elements can be linked to each other via connections. A connection can be created via the output of an element. The mouse can be used to drag the connection to another element and thus create a link in the data flow. If you release the connector in the free area, a subsequent element is automatically created or a selection of available element types is offered. 66 Version: 1.7.1 TF3520
## Page 67

Configuration The following connections are possible: MQTT-Source - Rule: An MQTT-Source can be used in several rules. In addition, several MQTT-Sources can be used in a rule, provided they run with similar system times. If this is not the case, the editor recognizes this and issues a corresponding warning. TF3520 Version: 1.7.1 67
## Page 68

Configuration Rule - Recording: Any number of recordings for saving data can be defined for a rule. However, only data from one rule can flow into a recording. For a recording, any number of rules can be created for further processing of the saved data. Here too, a rule can only accept data from a recording. No subsequent rules can currently be created for recordings with a ring buffer. It is also not possible to combine data from an MQTT-Source and a recording as data sources for a rule. 68 Version: 1.7.1 TF3520
## Page 69

Configuration MQTT-Source - Recording: This connection is not possible. A rule must always be inserted as an intermediate connection. 6.2.2.1.3 Viewing properties of the elements The properties of the selected element can be viewed via the Properties window on the right-hand side of the Rule Engine Pipeline Editor. The properties vary depending on the type. MQTT source • Broker: Message broker of the data source. • Stream: Topic of the data source • System ID: System ID of the data source • Cycle time: Cycle time of the data source • Symbols: Number and names of the symbols selected at the data source. TF3520 Version: 1.7.1 69
## Page 70

Configuration Rule • Alias: Alias name of the rule • Type: Type of rule. There are currently Streaming Data Rules and Batch Data Rules. Streaming Data Rules refer to streamed data and run continuously to process the streamed data. Batch Data Rules refer to data that has already been saved. These do not run continuously, but are started via triggers. They process a certain amount of stored data and then shut down again. • Nodes: Number of nodes in a rule. • Symbols: Number and names of the symbols that are output from the rule. • Rule Trigger (only for Batch Data Rules): Number of configured triggers. 70 Version: 1.7.1 TF3520
## Page 71

Configuration Recording • Cycle Time: Cycle time of the data recording • Storage: Name of the data memory • Mode: Recording mode (Infinite duration or ring buffer) • Symbols: Number and names of the selected symbols TF3520 Version: 1.7.1 71
## Page 72

Configuration 6.2.2.1.4 Editing elements Editing takes place in a separate window. This can be opened by double-clicking on the element or by clicking the Edit icon in the properties. 72 Version: 1.7.1 TF3520
## Page 73

Configuration 6.2.2.1.4.1 Editing MQTT-Sources An MQTT-Source is edited in the MQTT Source Configurator. First, a message broker must be selected at Broker. All Data Brokers of the Storage Provider used for the Rule Engine pipeline that have also been added in the Manager can be selected. An MQTT-Stream (Stream) can then be selected based on its topic. As soon as a stream has been added, the available symbols for the stream are displayed. Individual symbols can be selected using the checkboxes. All displayed symbols can be selected or deselected using the Select all symbols and Clear selection buttons. If the selected symbols include arrays or structures, a user dialog will prompt you to confirm whether the subsymbols should be added as separate symbols when the configuration is saved. If these are added, the subsymbols can be used individually in rules or recordings for processing and saving. TF3520 Version: 1.7.1 73
## Page 74

Configuration 6.2.2.1.4.2 Editing rules A rule is edited in the Rule Configurator. An alias name can be assigned to a rule at the top left. A rule consists of the following elements: Rule Input: Corresponds to the start point within a rule. All symbols added to the rule are displayed here. For Batch Data Rules, the time range for which the data is loaded when the rule is triggered can be selected here. Rule Output: Corresponds to the output of the rule. All symbols of the blocks that are connected to the rule output can be selected for subsequent recordings. Additional blocks can be added for processing via the toolbox (left). A distinction is made between Condition blocks and Action blocks. Condition blocks serve as filters and enable incoming data to be filtered on a time basis or data basis. Action blocks offer the option of aggregating data (e.g. calculating min/max/avg or other operations). All blocks can be linked to each other with connections. 74 Version: 1.7.1 TF3520
## Page 75

Configuration 6.2.2.1.4.2.1 Input Corresponds to the start point within a rule. All symbols added to the rule are displayed here. For Batch Data Rules, the time range for which the data is loaded when the rule is triggered can be selected here. 6.2.2.1.4.2.2 Output Corresponds to the output of the rule. All symbols of the blocks that are connected to the rule output can be selected for subsequent recordings. Variables can be renamed at the output node. To do this, you can click on the Edit icon behind the corresponding variable. TF3520 Version: 1.7.1 75
## Page 76

Configuration 6.2.2.1.4.2.3 Conditions Condition blocks serve as filters and enable incoming data to be filtered on a time basis or data basis. 6.2.2.1.4.2.3.1 Eventbased The Event Based-Condition can be used to filter for data within the data where a specific event must have occurred. The following configurations can be made: • Variable: Selection of the variables to which the condition must apply. • Operator: Selection of the operator for the condition • Compare value: Selection of the comparison value ◦ Constant value: Comparison with a constant value ◦ Variable: Comparison with another variable ◦ Previous value: Comparison with the previous value of the selected variable. • Duration: Selection of how long the data should be forwarded once the condition is fulfilled. ◦ As long as condition is fulfilled: The data is forwarded as long as the condition is fulfilled. ◦ Single Flag: The data is forwarded once when the condition is fulfilled, i.e. as with a rising edge. ◦ Alternatively, the data can also be forwarded for a specific time, which can be configured manually. 76 Version: 1.7.1 TF3520
## Page 77

Configuration 6.2.2.1.4.2.3.2 Timebased The Timebased-Condition can be used to filter within the data on a time basis. The time on which this is based is the respective timestamp of the data. The following configurations can be made: • Type: Selection of the type of Timebased Condition. Three types are available ◦ Interval: The condition is fulfilled as soon as the interval is reached. ◦ Weekly: The condition is fulfilled on certain days of the week at a certain time. ◦ Monthly: The condition is fulfilled in certain months on a certain day at a certain time. • Duration: Selection of how long the data should be forwarded once the condition is fulfilled. ◦ Single Flag: The data is forwarded once when the condition is fulfilled, i.e. as with a rising edge. ◦ Time: Alternatively, the data can also be forwarded for a specific time, which can be configured manually. TF3520 Version: 1.7.1 77
## Page 78

Configuration 6.2.2.1.4.2.4 Actions Action blocks offer the option of aggregating data (e.g. calculating min/max/avg or other operations). 6.2.2.1.4.2.4.1 Downsampling Configuration of a downsampling of individual variables. The sampling factor indicates the sampling rate. In the Selected Symbols area, you can select the variables to which downsampling is to be applied. If you want to sample all the data in a rule, you can configure this in the corresponding recording. The following symbols are provided for each selected symbol: • _Sampled: Sampled symbol 78 Version: 1.7.1 TF3520
## Page 79

Configuration 6.2.2.1.4.2.4.2 Edge Counter Determines the edges within the data. The Operator Edge and the threshold value to be compared must be configured for this. In the Selected Symbols area, you can select the variables to which the Edge Counter is to be applied. The following symbols are provided for each selected symbol: • _Count: Number of edges counted • _Edge: Boolean whether an edge is detected • _Last Event: Timestamp of the last event 6.2.2.1.4.2.4.3 Math Operation Symbols can be calculated with each other using the Math Operation block. The symbols to be calculated with each other can be selected under Selected Symbols. The operator can be selected under Math Operator. If the result is to be calculated with an additional constant value, activate the Add value on variable result checkbox. In the Configuration area, you can configure the operator and the constant value with which the result of the symbols is to be calculated. The following symbol is provided: • Result: Result of the calculation TF3520 Version: 1.7.1 79
## Page 80

Configuration 6.2.2.1.4.2.4.4 Min/Max/Avg Calculation of the minimum/maximum and average value for selected symbols. In the Selected Symbols area, you can select the variables for which the values are to be calculated. If you activate the With Interval checkbox, you can configure an interval for which the values are to be calculated. The following symbols are provided for each selected symbol: • _Min: Minimum of the symbol • _Max: Maximum of the symbol • _Avg: Average of the symbol 80 Version: 1.7.1 TF3520
## Page 81

Configuration 6.2.2.1.4.2.4.5 Standard Deviation Calculation of the standard deviation for selected symbols. In the Selected Symbols area, you can select the variables for which the values are to be calculated. The calculation of the standard deviation can be configured according to the block. The following symbols are provided for each selected symbol: • _STD: Standard deviation of the symbol 6.2.2.1.4.2.5 Trigger Triggers can be configured for a Batch Data Rule (based on data from a recording). These triggers allow the rule to start time-based or event-based. A rule can also be triggered manually at any time. The triggers are configured in the Rule Trigger Configurator. This can be opened via the property window of the rule. The time-based and event-based triggers can be configured in the Rule Trigger Configurator. Currently, only time-based or event-based triggers can be used for a rule. TF3520 Version: 1.7.1 81
## Page 82

Configuration 82 Version: 1.7.1 TF3520
## Page 83

Configuration 6.2.2.1.4.2.5.1 Eventbased The configuration of an event-based trigger corresponds to an Eventbased-Condition [} 76] within the rule. The underlying data is the data that is currently being written to the recording that serves as the data source for the rule. It is not possible or necessary to configure the duration. If the trigger condition is fulfilled, the rule runs until the data has been processed accordingly and is then disabled until the next trigger. 6.2.2.1.4.2.5.2 Timebased The configuration of a time-based trigger corresponds to a Timebased-Condition [} 77] within the rule. The underlying time is the system time of the device on which the Storage Provider is running. It is not possible or necessary to configure the duration. If the trigger condition is fulfilled, the rule runs until the data has been processed accordingly and is then disabled until the next trigger. TF3520 Version: 1.7.1 83
## Page 84

Configuration 6.2.2.1.4.3 Editing recordings Recordings are edited in the Single Recording Configurator. The following settings are available here: • Alias: Recording alias • Storage: Data sink for recording (configured in the Storage Provider Configurator) • Record Name: Name of the record • Mode: Mode of the record ◦ Infinite Duration: The record runs as long as the Rule Engine pipeline is running and data is available for the record. ◦ Timebased Ringbugger: The record runs as long as the Rule Engine pipeline is running and data is available for the record. The data is stored in a time-limited ring buffer. ◦ Databased Ringbugger: The record runs as long as the Rule Engine pipeline is running and data is available for the record. The data is stored in a ring buffer with a limited storage capacity. • Symbols: Selection of symbols to be saved with the recording. You can choose between cyclic data or data filtered by conditions. • Cycle Time: Selection of the cycle time for recording. This selection can only be made if you want to save cyclic (unfiltered) data. You have the option of configuring a cycle time based on the variables or your own cycle time for recording. Note that data will be lost if the cycle time is greater than the minimum cycle time of the data. In this case, a message appears. 84 Version: 1.7.1 TF3520
## Page 85

Configuration 6.2.2.1.5 Saving a Rule Engine pipeline A configured Rule Engine pipeline can be saved using the Save button (bottom right) in the Rule Engine Pipeline Editor. Saving is only possible if the configuration is error-free. Any errors are displayed via a message and the incorrectly configured blocks are highlighted graphically. TF3520 Version: 1.7.1 85
## Page 86

Configuration 6.2.2.1.6 Subsequent editing of a Rule Engine pipeline A Rule Engine pipeline that has been created can be edited and modified by right-clicking Edit in the context menu. Please note that existing data can no longer be used in the pipeline if the recordings contained in the pipeline are adjusted. 86 Version: 1.7.1 TF3520
## Page 87

Configuration 6.2.2.2 Application 6.2.2.2.1 Starting/stopping a Rule Engine pipeline A configured Rule Engine pipeline can be started via the Start icon. A start is only possible if the Storage Provider is running and the Rule Engine in the Storage Provider is running without errors. The status of the Rule Engine can be read via the information of the respective Storage Provider. TF3520 Version: 1.7.1 87
## Page 88

Configuration You can view Rule Engine pipelines that have been started in the Live Status tab and stop them there again. 6.2.2.2.2 Publishing Rule Engine pipelines Locally configured Rule Engine pipelines can be published to the stored Storage Provider. This means that the Rule Engine pipeline is also stored at the Storage Provider and is globally available. Other users can download, edit or start it. To publish, right-click on the locally configured pipeline in the Publish Rule Engine Pipeline context menu. In addition, Rule Engine pipelines can be published in Autostart mode. This means that the pipelines are started automatically after the Storage Provider Service is started. 88 Version: 1.7.1 TF3520
## Page 89

Configuration All published pipelines can be viewed under Live Status. These can be started, deleted or downloaded. 6.2.2.2.3 Rule Engine pipeline states A Rule Engine pipeline can have the following states: TF3520 Version: 1.7.1 89
## Page 90

Configuration Not Initalized: The started Rule Engine pipeline was received by the Storage Provider but not processed. Initalizing: The Rule Engine pipeline is initialized. The corresponding rules are created. Is Starting: The Rule Engine pipeline is starting. Included rules are started. Running: The Rule Engine pipeline is running. All rules are at least in the Pending state and there is no error. Is Stopping: The Rule Engine pipeline is stopping, all rules are stopping. Stopped: The Rule Engine pipeline is stopped, all rules are stopped. Error: There is an error in at least one rule. 6.2.2.2.4 Rule states You can also view the state of the rules from the Rule Engine pipeline in the Live Status. A rule has the following states: Not Initialized: The configuration of the rule has been read, but the rule has not yet been processed. Initializing: The rule is initializing. The necessary sources are generated in the Rule Engine. Deactivated: The rule is ready for activation. It is not currently running. Activating: The rule will be activated. Any triggers will be started. Pending: The rule is activated. Any triggers have been started. The rule can now be triggered manually. Streaming Data Rules go directly from the Pending state to the Starting state. Starting: The rule is starting. The corresponding processing modules are generated and a connection to the data sources is established. Running: The rule runs and processes data. Stopping: The rule is stopping. Stopping is called when the Rule Engine pipeline will be stopped. Batch Data Rules also stop when all data has been processed. The rule then switches to the Pending state. Deactivating: The rule will be deactivated. It is no longer possible to start up using a trigger. Invalid: The rule is in an invalid state and must be restarted. Error: The rule is in an error state. This can happen, for example, if a data source (Analytics Logger) is not available. In this case, the rule restarts automatically as soon as the data source is available again. For all other errors, the rule must be restarted manually. This can be done via the Storage Provider Manager or the API or PLC library. 6.2.2.2.5 Messages from Rule Engine pipelines The messages from Rule Engine pipelines provide more detailed information about errors and status information. The messages can be viewed via the context menu of a running Rule Engine pipeline. To do this, click on Show Messages of Rule Engine-Pipeline. A window with all available messages opens. If there are no messages in the window, these messages are no longer available. More accurate information can be found in the log from the Storage Provider, if this is activated. 90 Version: 1.7.1 TF3520
## Page 91

Configuration 6.2.2.2.6 Triggering and restarting rules Rules can be triggered or restarted via the Rule Engine Pipeline Editor. The editor must be opened via a running Rule Engine pipeline (context menu > Show Pipeline). The Rule Engine Pipeline Editor opens in view mode. Processing is not possible there. In view mode, the status of the individual elements can be viewed and the corresponding rule can be triggered or restarted. Triggering is only possible when the rule is in the Pending state. A restart can only be performed if the rule is in an error state or has not been initialized. TF3520 Version: 1.7.1 91
## Page 92

Configuration 6.2.2.2.7 Download Rule Engine pipelines A copy of a running Rule Engine pipeline can be downloaded to the local manager by right-clicking > Download. If a Rule Engine pipeline with the same pipeline ID already exists, you can choose to overwrite it or create a copy with a new pipeline ID. 92 Version: 1.7.1 TF3520
## Page 93

Configuration 6.2.2.2.8 Rule Engine error case If an error occurs within the Rule Engine that it cannot resolve on its own, the Rule Engine is restarted. All existing Rule Engine pipelines are shut down for this purpose. After the Rule Engine is restarted, the Rule Engine pipelines are created again. However, the rules contained therein are not automatically restarted. This must be done manually [} 91] using the Rule Engine Pipeline Editor. TF3520 Version: 1.7.1 93
## Page 94

Configuration For a more detailed description of the error, see the messages or the log of the Storage Provider. 6.2.2.3 Converting old recordings An existing pipeline (previous recording) can be converted into a Rule Engine by right-clicking Convert to Rule Engine-Pipeline. The new configuration opens in the Rule Engine Pipeline Editor and can be edited there. 94 Version: 1.7.1 TF3520
## Page 95

Configuration 6.3 Working with Historical Data Historical Data can be analysed with the Analytics Workbench or the Analytics Service Tool. To see your recorded data, you need the Twin CAT Target Browser. Selection of data from the Twin CAT Target Browser The historical data can be pulled directly from the Target Browser to an input of an analysis algorithm. 1. First, you need to click Tc Analytics in the left corner of Target Browser. There you can see your configured broker, which lists live and historical data from your various devices. This should look like the following figure. 2. Go to the historical stream you created and select the recording to be analyzed. All your records are listed in the Record window on the right. The last recording is selected by default. TF3520 Version: 1.7.1 95
## Page 96

Configuration 3. When you record live, the time range of the recording is updated every few seconds. The entire time range of a recording is used by default. You can also edit the start and end time to analyze your desired data area. This can be done with a slider, text fields or in a graphical calendar view. If you click on the symbol to the right of the text fields, the calendar view will be displayed. 4. After these steps, you can drag and drop a symbol to an input of an algorithm just as you do with the symbols of the live data. ð A new input source for your historical stream is then generated and can be displayed in the Solution Explorer of your Visual Studio®. First, the dragged symbol and a timestamp of the current device time are listed under this stream. Also new drawn symbols of this stream are listed there. Analyse your historical data in the Analytics Configurator To analyse your historical data press on the Start Analytics button. In contrast to analysing live data, a green progress bar appears. The speed of your analysis depends on your record length, the amount and size of your symbols as well as on your broadband speed to the broker. The analysis stops automatically when the progress bar ends. The results will remain visible. 96 Version: 1.7.1 TF3520
## Page 97

Configuration Analysis of your historical data in your Analytics Runtime You can provide the configuration with your historical data to an Analytics Runtime (PLC). In addition to the historical data, the live data is also analyzed. This allows you to switch between them and not lose live data by streaming historical data. The reason for this is that they are separated into two different tasks. The start of the analysis of historical data must be triggered. Computing time for historical data Unlike the Analytics Configurator, the analysis of historical data in the PLC takes a similar amount of time as the original recording of the data. Depending on the packet size and the set sampling rate, the processing of the data can be shortened compared to the recording. However, cycle overruns due to excessively large packets must be taken into account. Main differences of the folder structure in the created PLC project: TF3520 Version: 1.7.1 97
## Page 98

Configuration NOTICE Implementation of the logic in your Twin CAT HMI The preparation and writing of values in your PLC are for testing purposes. It is recommended to implement this and other logic in the PLC code with interactions from your Twin CAT HMI application if required. You can start historical data analysis by triggering b Get Historical Data in st Ctrl_T1. The cancellation takes place by triggering b Cancel Historical Data. This can be done in the MAIN_Analytics_Historical file as shown in the following figure: 98 Version: 1.7.1 TF3520
## Page 99

Configuration To switch between live and historical data results for your HMI dashboard, you can set the b Historical symbol in the Analytics HMI GVL. With this option, you do not need any additional controls to display historical data (of course, you can also use your own controls for historical data). The analysis of the live data is not interrupted by calling up some historical data. After viewing the historical data, you can switch back to the current live results. This change only affects the variables in your GVL. 6.4 Console Configurator/Client In addition to the graphical configurator and the recorder, the Analytics Storage Provider can also be operated via a console. This means that configuration and access to a Storage Provider can also be performed under Twin CAT/BSD in addition to Windows. In addition, the console application can be used to generate Batch files for control [} 107] of the Analytics Storage Provider. After launching the console client, there are four options to choose from: 1 Opens the console Configurator [} 100] for the local Analytics Storage Provider 2 Opens the Analytics Storage Provider Client [} 104] c Clears the console configurator/client history q Closes the console configurator/client By entering one of the identifiers and confirming with the [Enter] key, the corresponding function is executed. TF3520 Version: 1.7.1 99
## Page 100

Configuration 6.4.1 Configurator In this menu, the local Analytics Storage Provider can be configured. The following additional inputs are available for this purpose: 1 Starts a dialog for configuring the local Analytics Storage Provider. 2 Outputs the configuration of the local Analytics Storage Provider. 10 Starts a dialog for configuring a Data Message Broker. 11 Returns all Data Message Broker configurations. 12 Resetting the Master Message Broker (commands can only be received via this broker). 13 Deletes a Data Message Broker. 20 Starts a dialog for configuring a Storage. 21 Returns all Storage configurations. 22 Resetting the Master Storage (default Storage). 23 Deleting a Storage configuration. 30 Starts the Analytics Storage Provider with the local configuration. 31 Stops the local Analytics Storage Provider. 32 Outputs the status of the Analytics Storage Provider. 40 Importing binary Analytics files into an existing Storage. b Switches back to the main menu. The configuration parameters are the same as in the graphical configurator of the Analytics Storage Provider. By pressing the [ESC] key, the dialog can be aborted at any time and you can return to the configuration menu. Use This section describes step by step how you can configure and start the local Storage Provider using the Storage Provider Client. Adding a Message Broker configuration To create a new Message Broker configuration, first press the "10" key. A dialog box will then open in which you must enter all the necessary parameters (e.g. host, port, user name, password, etc.). Once the entry is complete, the configuration data is automatically saved. 100 Version: 1.7.1 TF3520
## Page 101

Configuration Creating a Storage configuration (data sink) To configure a new Storage, such as a CSV storage, press the "20" key. Here, too, a dialog box will ask you for all the necessary parameters step by step. The following is an example of the configuration of a CSV storage: The data entered is automatically applied and saved. Configuration of the Storage Provider To configure the Storage Provider yourself, press the "1" key. Enter the necessary parameters in the dialog that appears. You can also define the Host Message Broker and the Default Storage here. Once the dialog has been completed, the configuration is saved automatically. TF3520 Version: 1.7.1 101
## Page 102

Configuration Starting the local Storage Provider Once all settings have been made, the Storage Provider is configured and ready to start. To do this, press the "30" key in the client. If the local Storage Provider Service has been started successfully, the following message appears: Importing historical data In this dialog, Analytics Files can be imported into a selected storage. First choose the storage you want to import to by entering the number in the brackets on the left-hand side of the storage in the list. Once you have made your choice of storage, you will be asked to enter the folder path to the files to be imported. You can then specify whether the contents of the folder should be copied or moved. You must also enter some basic information about the data to be imported: 102 Version: 1.7.1 TF3520
## Page 103

Configuration The import configuration is now complete - you can check it again in the next step. You can use CTRL + Z to correct entries. Otherwise, the import can be started. As soon as the import is successfully completed, the following message is displayed: TF3520 Version: 1.7.1 103
## Page 104

Configuration 6.4.2 Client In the Analytics Storage Provider Client it is possible to connect to an Analytics Storage Provider. This does not necessarily have to run locally on the device, but can also be addressed via an external MQTT broker. Thus, it is not mandatory to configure an Analytics Storage Provider locally to use the client. Recordings can then be started and stopped on a connected Analytics Storage Provider, as well as historical data streams to a configurable MQTT topic. The following inputs are available for this: 1 Establishes a connection with the MQTT broker from the local configuration and selects the configured Analytics Storage Provider. 2 Starts a dialog to connect to an MQTT broker. 3 Closes the connection to the current MQTT broker. 10 Provides the Analytics Storage Providers that are available under the MQTT broker for selection. 15 Reads a configuration file from an Analytics Storage Provider Recorder, with the recordings configured in it. 21 Starts a Rule Engine pipeline. 22 Stops a Rule Engine pipeline. 23 Restarts a rule of a Rule Engine pipeline. 31 Starts a recording. 32 Stops a recording based on the alias and the MQTT topic. 33 Starts a historical data stream. 34 Stops a historical data stream based on the result MQTT topic. 35 Updates streaming parameters of a running historical data stream. 36 Checks whether a recording is active. 40 Stops all active recordings. 41 Stops all historical data streams. 50 Deletes recordings that are older than a certain date. Optionally, a data stream can be specified. 104 Version: 1.7.1 TF3520
## Page 105

Configuration Use To use the Analytics Storage Provider Client, a connection to an MQTT broker must first be established. Entering "2" starts a dialog in which the already configured MQTT brokers are presented for selection. There a new MQTT broker can be configured and connected by entering "0". Then, by entering "10" in the client main menu, an Analytics Storage Provider can be selected, which is available under the MQTT broker. Alternatively, by entering "1" in the client main menu, a connection to the MQTT broker and Analytics Storage Provider can be established directly from the local configuration file. After the connection is successfully established, information about the connected MQTT broker and the selected Analytics Storage Provider is displayed in the prompter display before the prompt: (MQTT broker: Analytics Storage Provider) To start recordings, a dialog is started by entering "31". It is possible to start a recording that has already been created, provided that recordings have already been configured or read in via a recorder configuration file. In addition to the listed recordings, a new recording can also be configured and started by entering "0": TF3520 Version: 1.7.1 105
## Page 106

Configuration The configuration parameters correspond to the known parameters from the graphical Analytics Storage Provider Recorder. The default values can be deleted if necessary and replaced by individual entries. With the recording mode after the input "Subset" a subset of the symbolism can be defined by the recording data. Immediately after configuration, a command to start recording is sent to the connected Analytics Storage Provider. Running recordings can be stopped by entering "32". They are referenced by the MQTT topic from which the data comes and by the recording alias. If a recording is to be stopped by another client, the corresponding Recorder Guid must also be specified. The Recorder Guid is displayed together with the Recorder Alias above the input options in the client main menu (red 1). Recording configurations created in the console client are not persisted. So after closing the client, the list of recording configurations is no longer available. Therefore, reading recorder configuration files (enter "20" in the client main menu) can be very helpful. The configuration file of a recorder is stored on Windows systems under the path C:\Users\*** \App Data\Roaming\Beckhoff\Twin CAT Analytics Storage Provider (replace *** with the corresponding user). The historized data of the Analytics Storage Provider can be transmitted as a data stream to a definable result MQTT topic via the input "33". This also starts a dialog in which a previously configured data stream can be started. By entering "0", a new historical data stream can also be configured: 106 Version: 1.7.1 TF3520
## Page 107

Configuration The parameters define a historized recording, whereby the parameter result topic defines the MQTT topic to which the data is to be streamed. After configuration, a command to start the historical stream is automatically sent to the Analytics Storage Provider. By entering "35" in the client main menu, the parameters of an active historical stream can be adjusted. The historical stream is referenced by its result MQTT topic. The parameters can be used, for example, to adjust the speed or packet size of the data stream while it is running. Canceling a historical data stream is possible by entering "34" and specifying the result MQTT topic. Rule Engine pipelines It is also possible to start Rule Engine pipelines that have already been published. The entry "21" must be made for this. The appropriate Rule Engine pipeline can then be selected. Rule Engine pipelines can also be stopped (input "22"). It is also possible to restart individual rules (input "23"). 6.4.3 Batch files for control The console client can be used to create batch files to control the Analytics Storage Provider. Some parameters are provided for this purpose: -Help / -H / -? Returns a description of all parameters. Parameters for the configuration settings: TF3520 Version: 1.7.1 107
## Page 108

Configuration -Create ASPConfig Create a new Analytics Storage Provider settings XML. -Main Topic <main Topic> Analytics Storage Provider Main Topic. -Comment <comment> Analytics Storage Provider comment. -Event Log Trace <True|False> Trace to the event log. -Debug Log <True|False> Additional Debug Log. -Storage Type <type> Storage type (ANALYTICSFILE, AZURESQL, AZUREBLOB). -Storage Conn String <conn String> Connection string or path to storage. -Tls Type <Tls1.0|Tls1.1|Tls1.2> Tls type (for Azure Blob). -Max Duration <duration (sec)> Maximum duration of a TAY file. -Max Write Len <write Len (bytes)> Maximum length of a data packet. Configuration parameters: -Local Provider Use the connection settings of the locally installed Analytics Storage Provider. -Config File <path> Use all configurations from the configuration file of an Analytics Storage Provider Recorder window. -Provider Guid <guid> Provider of the Analytics Storage Provider to be used. -Config Cmd ID <id> ID number of the preconfigured recording in the configuration file. -Config Cmd Alias <alias> Alias of the preconfigured recording in the configuration file. Connection parameters: -Broker /-Host <hostname> Host name or IP address of the broker used. -Port <port> Broker port (default value: 1883). -User <username> Username for the connection. -Password / -Pwd <password> Password for the connection. -CA <path> Path to the CA certificate for the connection. -Cert <path> Path to the certificate for the connection. -Key_Cert <path> Path to the key file for the connection. -Key_Pwd <password> Password for the key file for the connection. Function parameters: -Start Record Sends a Start Record command. -Stop Record Sends a Stop Record command. -Is Recording Active Checks whether a recording is currently active. -Get Historical Sends a Get Historical Data command. -Stop Historical Sends a Stop Historical Data command. -Update Historical Sends a Historical Update command. -Cancel All Rec Sends a Cancel command to all active recordings. -Cancel All Hist Sends a Cancel command to all active historical data streams. -Start Pipeline Sends a Start Rule Engine pipeline command. -Stop Pipeline Sends a Stop Rule Engine pipeline command. -Restart Rule Sends a Restart Rule command. -Delete Recordings Older Than Deletes recordings whose end time is older than a specified timestamp. Optionally, the topic of the historical stream can also be specified. Only the active historical streams are taken into account. 108 Version: 1.7.1 TF3520
## Page 109

Configuration Recording start/stop parameters: -Alias <alias> Alias name of the recording. -Rec Name <record> Alias name of the data set. -Topic <topic> Topic to be included. -Data Format <Bin|Json> Data format of the live data stream. -Duration <seconds> Recording duration. -Ringbuffer <None|Time Based|Data Based> Ring buffer mode (default value: Default). -Rinbuffer Para <minutes/MB> Parameters for the ring buffer (in seconds or megabytes). -Mode <All|Subset> Mode of recording. Takes all symbols and a subset of the symbols. -Symbols / -Sym <Symbol1,Symbol2> List of symbol subset as comma-separated list. -Recorder Guid <guid> Guid of the Analytics Storage Provider Recorder window. -Storage <guid> Guid from Storage where to write. -Sub Broker <guid> Guid from the Sub Broker from which the data is to be recorded. Historical data stream start/stop parameters: -System ID <system ID guid> System ID of the recorded data set. -Topic <topic> Topic of the recorded data set. -Layout <layout guid> Layout of the recorded data set. -Record ID <id> ID of the data set to be streamed. -Start Time <time ns> Start time of the data set to be streamed in nanoseconds. -End Time <time ns> End time of the data set to be streamed in nanoseconds. -Max Samples <samples> Maximum number of samples (default value: 5000) -Usr Sample Time <ms> Sampling rate. (Default value: -1; sampling rate of the recording) -Data Format <Bin|Json> Data format of the data stream. -Result Topic <topic> Result MQTT topic to which the data will be streamed. -Mode <All|Subset> Streaming mode. Streams all or a subset of the symbols. -Symbols / -Sym <Symbol1,Symbol2> List of symbol subset as comma-separated list. Historical data stream update parameters: -Max Samples <samples> Maximum number of samples (default value: 5000) -Usr Sample Time <ms> Sampling rate. (Default value: -1; sampling rate of the recording) -Max Pack Size <samples> Maximum message size in kilobytes -Send Duration <ms> Waiting time between sending messages in milliseconds. -Result Topic <topic> Result MQTT topic to which the data will be streamed. Rule Engine pipeline parameter: - Pipeline Guid <guid> Guid of the Rule Engine pipeline. - Rule ID <id> ID of the rule within a Rule Engine pipeline. Delete recordings Parameters: TF3520 Version: 1.7.1 109
## Page 110

Configuration - Date Time Older Than <datetime> Timestamp in the format "yyyy-MM-dd hh:mm". Any recording with an end time older than this timestamp will be deleted. - Historical Stream Topic <topic> Topic of the historical stream (optional). Data Import Parameters: -Source Path <Path> Path to the folder with the files to be imported. -Storage Type <Storage Type> Choice of Storage type: Analytics File Apache_Io TDB Azure Blob CSVFile Influx DB_Plain Ms SQL Ms SQL_Plain Postgre SQL -Storage <Storage Guid> Guid of the Storage. -Copy Data <true|false> Copy or move the data. -Topic <string> Topic name of the data to be imported. -Alias <string> Alias name of the data to be imported. -Rec Name <string> Record name of the data to be imported -System ID <Guid> System ID of the data to be imported. -Sys IDAlias <string> System alias of the data to be imported. -Address <string> Address of the data to be imported. -Latitude <double> Latitude of the data to be imported. -Longitude <double> Longitude of the data to be imported. Command line samples: Create configuration: Twin CAT. Analytics. Storage Provider. Client -Create ASPConfig -Main Topic Beckhoff/ASPTest -Comment Analytics Storage Provider (Test) -Event Log Trace False -Debug Log False -Storage Type ANALYTICSFILE -Storage Conn String C:\Twin CAT\Functions\TF3520-Analytics-Storage Provider\Storage -Max Duration 120 -Max Write Len 2048 -Broker 172.17.62.135 -Port 1883 -User tcanalytics -Pwd 123 Start recording with local Analytics Storage Provider: Twin CAT. Analytics. Storage Provider. Client -localprovider -startrecord -alias cmd Test -recname cmd Rec1 -topic Test Signals/Test Stream -dataformat Bin -Duration 30 -mode Subset -Symbols Variables.f Cosine,Variables.f Sine Start configuration file of a recording: 110 Version: 1.7.1 TF3520
## Page 111

Configuration Twin CAT. Analytics. Storage Provider. Client -Config File "C: \Users\User\App Data\Roaming\Beckhoff\Twin CAT Analytics Storage Provider\Tc Analytics Storage Provider_R ecorder.xml" -Provider Guid 76141a7f-e580-4281-99d8-1b8a75ca014d -startrecord -Config Cmd Alias cmd Test Check recording status Twin CAT. Analytics. Storage Provider. Client -Broker 172.17.62.135 -Port 1883 -User tcanalytics -Pwd 123 -Provider Guid 76141a7f-e580-4281-99d8-1b8a75ca014d -Is Recording Active -alias cmd Test -recorder Guid a8e171d2-712d-bd8e-da15-7eef28b71ad2 Stop all recordings: Twin CAT. Analytics. Storage Provider. Client -Broker 172.17.62.135 -Port 1883 -User tcanalytics -Pwd 123 -Provider Guid 76141a7f-e580-4281-99d8-1b8a75ca014d -Cancel All Rec Start historical data stream: Twin CAT. Analytics. Storage Provider. Client -localprovider -Get Historical -system ID c29ac2d4-76ce-ff44-4d7f-355ffbcca6bf -layout 9a8e171d-712d-bd8e-da15-7eef28b71ad2 -topic Test Signals/Test Stream -record ID 1 -start Time 132696863612730000 -end Time 132696864177720000 -max Samples 5000 -usr Sample Time -1 -result Topic _Test Signals/Test Stream/123 -dataformat Bin -mode Subset -symbols Variables.f Sine Start Rule Engine pipeline: Twin CAT. Analytics. Storage Provider. Client -localprovider -Start Pipeline -Pipeline Guid d00c5366-4cf5-4d4e-a2f6-9dbe759e9dd2 Stop Rule Engine pipeline: Twin CAT. Analytics. Storage Provider. Client -localprovider -Stop Pipeline -Pipeline Guid d00c5366-4cf5-4d4e-a2f6-9dbe759e9dd2 Start a special rule of a Rule Engine pipeline: Twin CAT. Analytics. Storage Provider. Client -localprovider -Restart Rule -Pipeline Guid d00c5366-4cf5-4d4e-a2f6-9dbe759e9dd2 -Rule ID 2 Delete old recordings: Twin CAT. Analytics. Storage Provider. Client -localprovider -Delete Recordings Older Than -Date Time Older Than yyyy-MM-dd 00:00 - Historical Stream Topic Beckhoff /Tc Analytics Storage Provider/41cfa2be- ca72-4145-9e37-875851502aa6/Historical/Stream_65 TF3520 Version: 1.7.1 111
## Page 112

Configuration Importing data Twin CAT. Analytics. Storage Provider. Client -Data Import -Source Path C:\\temp\\ED6A9F45-04D7-2D3A-7834-D3D1CF5EB21D -Storage Type CSVFile -Storage e5a61c3d-dd98-40fc-a63f-4c41f6f19729 -Copy Data true -Topic Analytics Storage Provider/Unknown Analytics File -Alias Unknown -Rec Name Record -System ID 53fae9bf-03fa-48ac-81e7-74f042eec6c2 -Sys IDAlias Unknown Analytics File -Address Test Address -Latitude 1.0 -Longitude 5.0 112 Version: 1.7.1 TF3520
## Page 113

PLC API 7 PLC API 7.1 Function blocks 7.1.1 Topic Architecture 7.1.1.1 Commands 7.1.1.1.1 T_ALY_SPCancel_Cmd Syntax Definition: FUNCTION_BLOCK T_ALY_SPCancel_Cmd EXTENDS T_ALY_Json Payload VAR_INPUT e Cancel Type : E_Cancel Type; arr Parameter : ARRAY [0..99] OF T_Max String; END_VAR Inheritence hierarchy T_ALY_Json Payload T_ALY_SPCancel_Cmd Inputs Name Type Description e Cancel Type E_Cancel Type [} 145] arr Parameter ARRAY [0..99] OF T_Max String Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider TF3520 Version: 1.7.1 113
## Page 114

PLC API 7.1.1.1.2 T_ALY_SPGet Historical_Cmd Syntax Definition: FUNCTION_BLOCK T_ALY_SPGet Historical_Cmd EXTENDS T_ALY_Json Payload VAR_INPUT n Recording ID : LINT; s Sub Broker : GUID; s Topic : T_Max String; s Layout : GUID; e Mode : E_Symbol Mode := E_Symbol Mode. All; e Output Format : E_Raw Data Format := E_Raw Data Format. Bin; n Max Sample Count : UDINT := 3000; n User Sample Time : DINT := -1; n Record ID : DINT; n Start Timestamp : LINT; n End Timestamp : LINT; s Result Topic : T_Max String; arr Symbol : ARRAY [0..255] OF T_ALY_Symbol; END_VAR Inheritence hierarchy T_ALY_Json Payload T_ALY_SPGet Historical_Cmd 114 Version: 1.7.1 TF3520
## Page 115

PLC API Inputs Name Type Description n Recording ID LINT Specific ID of the recording to be used s Sub Broker GUID Individual GUID of the Message Broker via which the data is sent. s Topic T_Max String Topic name of the recorded live stream. s Layout GUID Layout GUID of the recording. e Mode E_Symbol Mode [} 150] Get all symbols or only a subset. e Output Format E_Raw Data Format [} 147] Format of the returned data (actually only “Bin” supported). n Max Sample Count UDINT Maximum number of samples in a payload package. n User Sample Time DINT Sample time in milliseconds of the returned stream. (-1 uses the recorded sample time.) n Record ID DINT Number of the record. n Start Timestamp LINT Start time n End Timestamp LINT End time s Result Topic T_Max String Topic name of the result stream. arr Symbol ARRAY [0..255] OF T_ALY_Symbol If Symbol Mode is Subset, only the list of this symbols will be [} 130] returned. Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.1.3 T_ALY_SPRead Stream Records_Cmd TF3520 Version: 1.7.1 115
## Page 116

PLC API Syntax Definition: FUNCTION_BLOCK T_ALY_SPRead Stream Records_Cmd EXTENDS T_ALY_Json Payload VAR_INPUT n Recording ID : LINT := -1; s Stream Topic : STRING(255); s Stream System ID : GUID; s Stream Layout : GUID; n Record Start Index : DINT; n Max Record Count : DINT; s Result Topic : T_Max String; END_VAR Inheritance hierarchy T_ALY_Json Payload T_ALY_SPRead Stream Records_Cmd Inputs Name Type Description n Recording ID LINT Specific ID of the recording to be used s Stream Topic STRING(255) Topic name of the recorded live stream. s Stream System ID GUID System ID of the target system from where the live stream was sent s Stream Layout GUID Layout GUID of the recording n Record Start Index DINT Start index of the first record to be read. n Max Record Count DINT Total number of records to be read. s Result Topic T_Max String Topic name of the result stream Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 116 Version: 1.7.1 TF3520
## Page 117

PLC API 7.1.1.1.4 T_ALY_SPRecord Data_Cmd Syntax Definition: FUNCTION_BLOCK T_ALY_SPRecord Data_Cmd EXTENDS T_ALY_Json Payload VAR_INPUT s Record Data Key : GUID; s Storage : GUID; s Sub Broker : GUID; s Pipeline : GUID; s Alias : T_Max String; s Record Name : T_Max String; e Recording : E_Record Mode; s Recorder : GUID; s Recorder Alias : T_Max String; s Topic : T_Max String; e Data Format : E_Raw Data Format; e Duration Time Mode : E_Time Mode := E_Time Mode. Minutes; n Duration : DINT; e Ring Buffer Mode : E_Ring Buffer Mode; n Ring Buffer Parameter : DINT; e Mode : E_Symbol Mode; s Symbol Layout : GUID; arr Symbols : ARRAY [0..255] OF T_ALY_Symbol; END_VAR Inheritence hierarchy T_ALY_Json Payload T_ALY_SPRecord Data_Cmd TF3520 Version: 1.7.1 117
## Page 118

PLC API Inputs Name Type Description s Record Data Key GUID Individual GUID to identify the recording s Storage GUID Individual GUID of the Storage to be used s Sub Broker GUID Individual GUID of the message broker to be used s Pipeline GUID Optional – GUID of the Rule Engine pipeline s Alias T_Max String Alias name for the recording. s Record Name T_Max String Name for this record. e Recording E_Record Mode [} 147] Start or Stop the recording. s Recorder GUID Individual GUID of the recorder. s Recorder Alias T_Max String Alias name for the recorder. s Topic T_Max String Topic name of the live stream. e Data Format E_Raw Data Format [} 147] Saving the data format (currently only binary format is supported). e Duration Time Mode E_Time Mode [} 148] Resolution of the n Duration parameter n Duration DINT Duration of the recording in minutes. (-1 unlimited) e Ring Buffer Mode E_Ring Buffer Mode [} 148] Ring buffer mode n Ring Buffer Parameter DINT Time Based => parameter in minutes. Data Based => parameter in megabytes. e Mode E_Symbol Mode [} 150] Record all symbols or only a subset. s Symbol Layout GUID arr Symbols ARRAY [0..255] OF T_ALY_Symbol If Symbol Mode is Subset, only the list of this symbols will be [} 130] recorded. Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.1.5 T_ALY_SPReload Historical Streams_Cmd 118 Version: 1.7.1 TF3520
## Page 119

PLC API Syntax Definition: FUNCTION_BLOCK T_ALY_SPReload Historical Streams_Cmd EXTENDS T_ALY_Json Payload VAR_INPUT e Reload Type : E_Reload Type; arr Parameter : ARRAY [0..9] OF ARRAY [0..1] OF T_Max String; END_VAR Inheritance hierarchy T_ALY_Json Payload T_ALY_SPReload Historical Streams_Cmd Inputs Name Type Description e Reload Type E_Reload Type [} 148] Update mode selection arr Parameter ARRAY [0..9] OF ARRAY [0..1] Additional parameters OF T_Max String Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.1.6 T_ALY_SPRule Engine Pipeline_Cmd Syntax Definition: FUNCTION_BLOCK T_ALY_SPRecord Data_Cmd EXTENDS T_ALY_Json Payload VAR_INPUT s Rule Engine Pipeline : GUID; e Cmd Type : E_Pipeline Cmd Type; s Recorder : GUID; s Recorder Alias : T_Max String; END_VAR Inheritance hierarchy T_ALY_Json Payload T_ALY_SPRule Engine Pipeline_Cmd TF3520 Version: 1.7.1 119
## Page 120

PLC API Inputs Name Type Description s Rule Engine Pipeline GUID Individual GUID of the Rule Engine pipeline. e Cmd Type E_Pipeline Cmd Type [} 150] Start or Stop the recording. s Recorder GUID Individual GUID of the recorder. s Recorder Alias T_Max String Alias name for the recorder. Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.1.7 T_ALY_SPSet Get Historical Data State_Cmd Syntax Definition: FUNCTION_BLOCK T_ALY_SPSet Get Historical Data State_Cmd EXTENDS T_ALY_Json Payload VAR_INPUT s Result Topic : T_Max String; e State : E_Set Get Historical Data State; n Send Duration_ms : DINT; n Restart Timestamp : LINT; n Max Sample Count : UDINT; n Max Package Size_KB: DINT; n User Sample Time : LINT; END_VAR Inheritance hierarchy T_ALY_Json Payload T_ALY_SPSet Get Historical Data State_Cmd 120 Version: 1.7.1 TF3520
## Page 121

PLC API Inputs Name Type Description s Result Topic T_Max String Topic name of the result stream (used like a handle). e Sta.te E_Set Get Historical Data State [} 149] Historical stream state n Send Duration_ms DINT Waiting time between sending the individual packages n Restart Timestamp LINT Timestamp at which the result stream is continued. n Max Sample Count UDINT Maximum number of entries in a package n Max Package Size_KB DINT Maximum size of a package n User Sample Time LINT Sample time in milliseconds of the returned stream (-1 uses the recorded sample time). Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.1.8 T_ALY_SPStorage Ctrl_Cmd Syntax Definition: FUNCTION_BLOCK T_ALY_SPStorage Ctrl_Cmd EXTENDS T_ALY_Json Payload VAR_INPUT e Ctrl Mode : E_Control Mode; s Storage Guid : GUID; END_VAR Inheritence hierarchy T_ALY_Json Payload T_ALY_SPRecord Data_Cmd Inputs Name Type Description e Ctrl Mode E_Control Mode [} 146] Start, stop, etc. of the Storage. s Storage Guid GUID Individual GUID of the Storage. TF3520 Version: 1.7.1 121
## Page 122

PLC API Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.2 Descriptions 7.1.1.2.1 T_ALY_Historical Stream_Desc Syntax Definition: FUNCTION_BLOCK T_ALY_Historical Stream_Desc EXTENDS T_ALY_Json Payload VAR_OUTPUT s Source : STRING(255); s Stream Topic : STRING(255); s Stream Alias : STRING(255); s Stream System ID : GUID; s Layout : GUID; n Cycle Time: UDINT; n Data Size : UDINT; arr Records : ARRAY [0..c Max Record Count] OF T_Record Timestamps; s Storage : GUID; n Hist Stream ID : LINT; s System Alias : STRING(255); s Record Name : STRING(255); s Address : STRING(255); e Symbol Mode : E_Symbol Mode; arr Symbols : ARRAY [0..255] OF T_ALY_Symbol; END_VAR Inheritance hierarchy T_ALY_Json Payload T_ALY_Historical Stream_Desc 122 Version: 1.7.1 TF3520
## Page 123

PLC API Outputs Name Type Description s Source STRING(255) Data source name s Stream Topic STRING(255) Topic name of the recorded stream s Stream Alias STRING(255) Alias name of the stream s Stream System ID GUID System ID GUID of the stream s Layout GUID Layout GUID of the recording n Cycle Time UDINT Cycle time of the recording n Data Size UDINT Data size of an entry of the recording arr Records ARRAY [0..c Max Record Count] OF Timestamp of the various T_Record Timestamps [} 130] recordings s Storage GUID Individual GUID of the Storage n Hist Stream ID LINT Specific ID of the historical stream s System Alias STRING(255) The system alias of the recorded data. s Record Name STRING(255) The name of the recording s Address STRING(255) The address of the recorded data. e Symbol Mode E_Symbol Mode [} 150] Recording mode arr Symbols ARRAY [0..255] OF T_ALY_Symbol List of recorded symbols Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.2.2 T_ALY_SPInstance_Desc Syntax Definition: FUNCTION_BLOCK T_ALY_SPInstance_Desc EXTENDS T_ALY_Json Payload VAR_OUTPUT dt Timestamp : DATE_AND_TIME; b Online : BOOL; s Name : STRING; s Version : STRING; st Info : T_ALY_SPInstance Info; END_VAR Inheritance hierarchy T_ALY_Json Payload TF3520 Version: 1.7.1 123
## Page 124

PLC API T_ALY_SPInstance_Desc Outputs Name Type Description dt Timestamp DATE_AND_TIME Start time of the Storage Provider Service b Online BOOL Indicates whether the service is online. s Name STRING Io T Device name "Twin CAT Analytics Storage Provider" s Version STRING Version of the Storage Provider st Info T_ALY_SPInstance Info [} 127] Detailed description of the Storage Provider Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.2.3 T_ALY_SPRecord Data_Desc Syntax Definition: FUNCTION_BLOCK T_ALY_SPRecord Data_Desc EXTENDS T_ALY_Json Payload VAR_OUTPUT s Record Data Guid : GUID; n Start Timestamp : LINT; e Status : E_Recording State; n Record ID : LINT; st Record : T_ALY_SPRecord Data_Cmd; END_VAR Inheritance hierarchy T_ALY_Json Payload T_ALY_SPRecord Data_Desc 124 Version: 1.7.1 TF3520
## Page 125

PLC API Outputs Name Type Description s Record Data Guid GUID GUID "Key" to identify the recording n Start Timestamp LINT Start time of the recording. e Status E_Recording State [} 147] Recording state n Record ID LINT Recording ID st Record T_ALY_SPRecord Data_Cmd [} 117] Associated recording command Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.2.4 T_ALY_SPRecording_Desc Syntax Definition: FUNCTION_BLOCK T_ALY_SPRecording_Desc EXTENDS T_ALY_Json Payload VAR_OUTPUT a Recordings : ARRAY [0..99] OF T_ALY_SPRecord Data_Desc; END_VAR Inheritance hierarchy T_ALY_Json Payload T_ALY_SPRecording_Desc Outputs Name Type Description a Recordings ARRAY [0..99] OF List of all current recordings T_ALY_SPRecord Data_Desc [} 124] Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider TF3520 Version: 1.7.1 125
## Page 126

PLC API 7.1.1.3 Info 7.1.1.3.1 T_ALY_Read Stream Record_Info Syntax Definition: FUNCTION_BLOCK T_ALY_Historical Stream_Desc EXTENDS T_ALY_Json Payload VAR_OUTPUT n Record Count All: UDINT; n Record Count : UDINT; arr Records : ARRAY [0..c Max Record Count] OF T_Record Timestamps; END_VAR Inheritance hierarchy T_ALY_Json Payload T_ALY_Read Stream Record_Info Outputs Name Type Description n Record Count All UDINT Number of all existing records n Record Count UDINT Number of records read out arr Records ARRAY [0..c Max Record Count] OF Timestamp of the records read T_Record Timestamps [} 130] out Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.4 Sub Types 7.1.1.4.1 T_ALY_SPData Storage Info Syntax Definition: FUNCTION_BLOCK T_ALY_SPData Storage Info EXTENDS T_ALY_Json Payload VAR_OUTPUT st Storage : T_ALY_SPStorage Info; 126 Version: 1.7.1 TF3520
## Page 127

PLC API e Status : E_Storage State; s Status Message : STRING(255); END_VAR Inheritance hierarchy T_ALY_Json Payload T_ALY_SPData Storage Info Outputs Name Type Description st Storage T_ALY_SPStorage Info [} 128] Detailed Storage information e Status E_Storage State [} 149] Status of the Storage s Status Message STRING(255) Storage status message Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.4.2 T_ALY_SPInstance Info Syntax Definition: FUNCTION_BLOCK T_ALY_SPInstance Info EXTENDS T_ALY_Json Payload VAR_OUTPUT s Provider Guid : GUID; s Service Type : STRING; s Data Store Type : STRING(255); s Comment : STRING(255); s Default Storage Guid : GUID; arr Data Storages : ARRAY [0..49] OF T_ALY_SPData Storage Info; END_VAR Inheritance hierarchy T_ALY_Json Payload T_ALY_SPInstance Info TF3520 Version: 1.7.1 127
## Page 128

PLC API Outputs Name Type Description s Provider Guid GUID Individual GUID of a Storage Provider instance s Service Type STRING Service type s Data Store Type STRING(255) Storage type s Comment STRING(255) Comment on the Storage Provider instance s Default Storage Guid GUID Storage GUID of the Standard Storage arr Data Storages ARRAY [0..49] OF List of configured Storages T_ALY_SPData Storage Info [} 126] Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.4.3 T_ALY_SPStorage Info Syntax Definition: FUNCTION_BLOCK T_ALY_SPStorage Info EXTENDS T_ALY_Json Payload VAR_OUTPUT s Storage Guid : GUID; s Storage Name : STRING; e Data Storage Type : E_Data Storage Type; s Comment : STRING(255); END_VAR Inheritance hierarchy T_ALY_Json Payload T_ALY_SPStorage Info Outputs Name Type Description s Storage Guid GUID Individual GUID of a Storage s Storage Name STRING Name of the Storage e Data Storage Type E_Data Storage Type [} 146] Storage type s Comment STRING(255) Comment on Storage 128 Version: 1.7.1 TF3520
## Page 129

PLC API Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.4.4 T_ALY_SPSub Broker Info Syntax Definition: FUNCTION_BLOCK T_ALY_SPSub Broker Info EXTENDS T_ALY_Json Payload VAR_OUTPUT s Alias : STRING(255); s Broker Guid : GUID; s Broker Host : STRING(255); n Broker Port : DINT; b Secure : BOOL; END_VAR Inheritance hierarchy T_ALY_Json Payload T_ALY_SPSub Broker Info Outputs Name Type Description s Alias STRING(255) Alias name of the broker configuration s Broker Guid GUID Individual GUID of the broker configuration s Broker Host STRING(255) Broker Host Name n Broker Port DINT Broker port b Secure BOOL TRUE if communication is established via certificates. Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider TF3520 Version: 1.7.1 129
## Page 130

PLC API 7.1.1.4.5 T_ALY_Symbol Syntax Definition: FUNCTION_BLOCK T_ALY_Symbol EXTENDS T_ALY_Json Payload VAR_INPUT s Name : T_Max String; s Base Type : T_Max String; n Bit Offset : UDINT; n Bit Size : UDINT; END_VAR Inheritence hierarchy T_ALY_Json Payload T_ALY_Symbol Inputs Name Type Description s Name T_Max String Name of the symbol s Base Type T_Max String Data Type of the symbol n Bit Offset UDINT Bit Offset of the symbol n Bit Size UDINT Bit Size of the symbol Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.1.4.6 T_Record Timestamps Syntax Definition: FUNCTION_BLOCK T_Record Timestamps EXTENDS T_ALY_Json Payload VAR_OUTPUT n Record ID : DINT; s Alias : STRING(255); 130 Version: 1.7.1 TF3520
## Page 131

PLC API n Start Timestamp : LINT; n End Timestamp : LINT; END_VAR Inheritance hierarchy T_ALY_Json Payload T_Record Timestamps Outputs Name Type Description n Record ID DINT Recording number s Alias STRING(255) Alias name of the recording n Start Timestamp LINT Start timestamp of the recording n End Timestamp LINT End timestamp of the recording Methods Name Definition location Description Reset Reset all values in the payload FB. Init_Json Value Inherited from Initialization of the FB with JSON T_ALY_Json Payload object. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.2 FB_ALY_Storage Provider The FB_ALY_Storage Provider is a client FB for communication with a Storage Provider instance. The FB provides methods to trigger historical data or start/stop recordings. Syntax Definition: FUNCTION_BLOCK FB_ALY_Storage Provider VAR_INPUT st Config : ST_ALY_SP_Config; END_VAR VAR_OUTPUT b Busy : BOOL; b Error : BOOL; ip Result Message : I_Tc Message; e Connection State : ETc Iot Mqtt Client State; END_VAR Inputs Name Type Description st Config ST_ALY_SP_Config [} 144] Structure for the configuration of the FB. TF3520 Version: 1.7.1 131
## Page 132

PLC API Outputs Name Type Description b Busy BOOL TRUE as soon as a method of the function block is active. b Error BOOL Becomes TRUE when an error situation occurs. ip Result Message I_Tc Message Message interface of the Twin CAT 3 Event Logger, which provides further information about the return value. e Connection State ETc Iot Mqtt Client State Specifies the state of the connection between client and broker as an enumeration ETc Iot Mqtt Client State. Methods Name Definition location Description Call [} 132] Local Method for background communication with the Twin CAT driver. The method must be called cyclically. Cancel [} 133] Local Method for aborting activities of the Twin CAT Analytics Storage Provider. Get Historical Data [} 133] Local Method for requesting historical data. Get Instance Info [} 134] Local Method for receiving the instance information of the Storage Provider. Get Recording Info By Alias Local Method for receiving recording information. [} 134] Get Recording Info By Key [} 135] Local Method for receiving recording information. Read Historical Streams [} 135] Local Method for reading all historical streams. Read Stream Records [} 136] Local Method for reading all records of a historical stream. Read Sub Broker [} 136] Local Method for reading all declared message brokers. Reset Communication [} 137] Local Method to reset the MQTT connection to the broker. Restart Pipeline Rule [} 137] Loca Restarts a rule of a pipeline. Send Command [} 138] Local Generic method for sending various commands. Set Historical Data State [} 138] Local Method for setting various parameters of a historical stream. Start Pipeline [} 139] Local Starts recording a live MQTT binary stream. Start Record [} 139] Local Starts recording a live MQTT binary stream. Start Record Ex [} 140] Local Starts recording a live MQTT binary stream. Stop Pipeline [} 140] Local Stops the selected recording. Stop Record [} 141] Local Stops the selected recording. Storage Controlling [} 141] Local Method for controlling the declared storages. Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider 7.1.2.1 Call 132 Version: 1.7.1 TF3520
## Page 133

PLC API Syntax METHOD Call : BOOL Return value Name Type Description Call BOOL 7.1.2.2 Cancel Syntax METHOD Cancel : BOOL VAR_INPUT st Cmd : REFERENCE TO T_ALY_SPCancel_Cmd; END_VAR Inputs Name Type Description st Cmd REFERENCE TO JSON command to cancel T_ALY_SPCancel_Cmd [} 113] operations of the Twin CAT Analytics Storage Provider. Return value Name Type Description Cancel BOOL Is TRUE if done 7.1.2.3 Get Historical Data Syntax METHOD Get Historical Data : BOOL VAR_INPUT st Cmd : REFERENCE TO T_ALY_SPHistorical_Cmd; END_VAR Inputs Name Type Description st Cmd REFERENCE TO JSON command to get historical T_ALY_SPGet Historical_Cmd [} 114] data from Twin CAT Analytics Storage Provider. Return value Name Type Description Get Historical Data BOOL Is TRUE if done TF3520 Version: 1.7.1 133
## Page 134

PLC API 7.1.2.4 Get Instance Info Syntax METHOD Get Instance Info : BOOL VAR_INPUT t Timeout : TIME; st Instance Info : T_ALY_SPInstance_Desc; END_VAR Inputs Name Type Description t Timeout TIME Duration until the procedure is aborted. st Instance Info T_ALY_SPInstance_Desc [} 123] JSON description of the Storage Provider instance. Return value Name Type Description Get Instance Info BOOL Is TRUE when completed 7.1.2.5 Get Recording Info By Alias Syntax METHOD Get Recording Info By Alias : BOOL VAR_INPUT s Recording Alias : STRING(255); t Timeout : TIME; st Recording Info : T_ALY_SPRecord Data_Desc; END_VAR Inputs Name Type Description s Recording Alias STRING(255) Search criterion "Alias" t Timeout TIME Duration until the procedure is aborted. st Recording Info T_ALY_SPRecord Data_Desc [} 124] JSON description of the recording. Return value Name Type Description Get Recording Info By Alias BOOL Is TRUE when completed 134 Version: 1.7.1 TF3520
## Page 135

PLC API 7.1.2.6 Get Recording Info By Key Syntax METHOD Get Recording Info By Key : BOOL VAR_INPUT s Record Data Key : GUID; t Timeout : TIME; st Recording Info : T_ALY_SPRecord Data_Desc; END_VAR Inputs Name Type Description s Record Data Key GUID Search criterion "Record Data Key" t Timeout TIME Duration until the procedure is aborted. st Recording Info T_ALY_SPRecord Data_Desc [} 124] JSON description of the recording. Return value Name Type Description Get Recording Info By Key BOOL Is TRUE when completed 7.1.2.7 Read Historical Streams Syntax METHOD Get Historical Data : BOOL VAR_INPUT t Search Duration : TIME := TIME#5s0ms a Historical Streams : POINTER TO T_ALY_Historical Stream_Desc; END_VAR VAR_OUTPUT n Stream Count : INT; END_VAR Inputs Name Type Description t Search Duration TIME Time period in which to wait for feedback. a Historical Streams POINTER TO Description of the different T_ALY_Historical Stream_Desc historical streams [} 122] TF3520 Version: 1.7.1 135
## Page 136

PLC API Return value Name Type Description Read Historical Streams BOOL Is TRUE when completed n Stream Count INT Number of streams read out 7.1.2.8 Read Stream Records Syntax METHOD Read Stream Records : BOOL VAR_INPUT st Cmd : REFERENCE TO T_ALY_SPRead Stream Records_Cmd; t Search Timeout : TIME := TIME#5s0ms; a Stream Records : POINTER TO T_Record Timestamps; END_VAR VAR_OUTPUT n Record Count : DINT; END_VAR Inputs Name Type Description st Cmd REFERENCE TO JSON command to get recordings T_ALY_SPRead Stream Records_Cmd of a historical stream from Twin CAT Analytics Storage [} 115] Provider. t Search Timeout TIME Waiting time for the response. a Stream Records POINTER TO T_Record Timestamps Recordings read out [} 130] Return value Name Type Description Read Stream Records BOOL Is TRUE when completed n Record Count DINT Number of records read out 7.1.2.9 Read Sub Broker Syntax METHOD Read Sub Broker : BOOL VAR_INPUT t Search Duration : TIME; a Sub Broker Infos : POINTER TO T_ALY_SPSub Broker Info; END_VAR VAR_OUTPUT n Broker Count :INT; END_VAR 136 Version: 1.7.1 TF3520
## Page 137

PLC API Inputs Name Type Description t Search Duration TIME Duration until the search is completed a Sub Broker Info POINTER TO Address to an array in which the T_ALY_SPSub Broker Info [} 129] broker information found is stored. Return value Name Type Description Read Sub Broker BOOL Is TRUE when completed. n Broker Count INT Number of brokers found. 7.1.2.10 Reset Communication Syntax METHOD Reset Communication : BOOL VAR_INPUT END_VAR Return value Name Type Description Reset Communication BOOL Is TRUE when completed 7.1.2.11 Restart Pipeline Rule Syntax METHOD Restart Pipeline Rule : BOOL VAR_INPUT st Cmd : REFERENCE TO T_ALY_SPRule Engine Pipeline_Cmd; n Rule Id : DINT; END_VAR Inputs Name Type Description st Cmd REFERENCE TO JSON command to start the T_ALY_SPRule Engine Pipeline_Cmd recording a live stream. [} 119] n Rule Id DINT ID of the rule to be restarted. TF3520 Version: 1.7.1 137
## Page 138

PLC API Return value Name Type Description Restart Pipeline Rule BOOL Is TRUE when completed. 7.1.2.12 Send Command Syntax METHOD Send Command : BOOL VAR_INPUT st Cmd : I_ALY_SPCommand; END_VAR Inputs Name Type Description st Cmd I_ALY_SPCommand JSON command to interact with the Twin CAT Analytics Storage Provider. Return value Name Type Description Send Command BOOL Is TRUE when completed 7.1.2.13 Set Historical Data State Syntax METHOD Set Historical Data State : BOOL VAR_INPUT st Cmd : REFERENCE TO T_ALY_SPGet Set Historical Data State_Cmd; END_VAR Inputs Name Type Description st Cmd REFERENCE TO JSON command to set parameters T_ALY_SPGet Set Historical Data State of a started historical stream from the Twin CAT Analytics Storage _Cmd [} 120] Provider. 138 Version: 1.7.1 TF3520
## Page 139

PLC API Return value Name Type Description Set Historical Data State BOOL Is TRUE when completed 7.1.2.14 Start Pipeline Syntax METHOD Start Pipeline : BOOL VAR_INPUT st Cmd : REFERENCE TO T_ALY_SPRule Engine Pipeline_Cmd; END_VAR Inputs Name Type Description st Cmd REFERENCE TO JSON command to start the T_ALY_SPRule Engine Pipeline_Cmd recording a live stream. [} 119] Return value Name Type Description Start Pipeline BOOL Is TRUE when completed. 7.1.2.15 Start Record Syntax METHOD Start Record : BOOL VAR_INPUT st Cmd : REFERENCE TO T_ALY_SPRecord Data_Cmd; END_VAR Inputs Name Type Description st Cmd REFERENCE TO JSON command to start the T_ALY_SPRecord Data_Cmd [} 117] recording a live stream. Return value Name Type Description Start Record BOOL Is TRUE if done TF3520 Version: 1.7.1 139
## Page 140

PLC API 7.1.2.16 Start Record Ex In contrast to the Start Record [} 139] method, a Record Data Key can be specified here. This key makes it easier to find the recording you have started in order to check the status of the recording. The Get Recording Info By Key [} 135] method can be used to retrieve the recording information. Syntax METHOD Start Record Ex : BOOL VAR_INPUT st Cmd : REFERENCE TO T_ALY_SPRecord Data_Cmd; s Record Data Key : GUID; END_VAR Inputs Name Type Description st Cmd REFERENCE TO JSON command to start the T_ALY_SPRecord Data_Cmd [} 117] recording a live stream. s Record Data Key GUID Guid "Key" to identify the recording that has been started. Return value Name Type Description Start Record Ex BOOL Is TRUE when completed 7.1.2.17 Stop Pipeline Syntax METHOD Stop Pipeline : BOOL VAR_INPUT st Cmd : REFERENCE TO T_ALY_SPRule Engine Pipeline_Cmd; END_VAR Inputs Name Type Description st Cmd REFERENCE TO JSON command to stop recording T_ALY_SPRule Engine Pipeline_Cmd of a live stream. [} 119] Return value Name Type Description Stop Pipeline BOOL Is TRUE when completed. 140 Version: 1.7.1 TF3520
## Page 141

PLC API 7.1.2.18 Stop Record Syntax METHOD Stop Record : BOOL VAR_INPUT st Cmd : REFERENCE TO T_ALY_SPRecord Data_Cmd; END_VAR Inputs Name Type Description st Cmd REFERENCE TO JSON command to stop recording T_ALY_SPRecord Data_Cmd [} 117] of a live stream. Return value Name Type Description Stop Record BOOL Is TRUE when completed. 7.1.2.19 Storage Controlling Syntax METHOD Storage Controlling : BOOL VAR_INPUT st Cmd : REFERENCE TO T_ALY_SPStorage Ctrl_Cmd; END_VAR Inputs Name Type Description st Cmd REFERENCE TO JSON command to control the T_ALY_SPStorage Ctrl_Cmd [} 121] storages. Return value Name Type Description Storage Controlling BOOL Is TRUE when completed 7.1.3 FB_ALY_Active Recordings TF3520 Version: 1.7.1 141
## Page 142

PLC API The FB_ALY_Active Recordings is a client FB for monitoring the active recordings of a Storage Provider instance. The FB provides methods to start and stop monitoring. The active recordings found are output at arr Recordings with the corresponding status information. Syntax Definition: FUNCTION_BLOCK FB_ALY_Active Recordings VAR_INPUT st Config : ST_ALY_SP_Config; END_VAR VAR_IN_OUT arr Recordings : ARRAY [*] OF ST_Recording Status; END_VAR VAR_OUTPUT b Error : BOOL; ip Result Message : I_Tc Message; e Connection State : ETc Iot Mqtt Client State; b Is Monitoring : BOOL; n Active Recordings : UDINT; n Received Messages : ULINT; END_VAR Inputs Name Type Description st Config ST_ALY_SP_Config [} 144] Structure for the configuration of the FB. Inputs/outputs Name Type Description arr Recordings ARRAY [*] OF List of active recordings found ST_Recording Status [} 145] Outputs Name Type Description b Error BOOL Becomes TRUE when an error situation occurs. ip Result Message I_Tc Message Message interface of the Twin CAT 3 Event Logger, which provides further information about the return value. e Connection State ETc Iot Mqtt Client State Specifies the state of the connection between client and broker as an enumeration ETc Iot Mqtt Client State. b Is Monitoring BOOL Becomes TRUE as soon as monitoring is started. n Active Recordings UDINT Number of active recordings found n Received Messages ULINT Number of MQTT messages received Methods Name Definition location Description Call [} 143] Local Method for background communication with the Twin CAT driver. The method must be called cyclically. Start Monitoring [} 143] Local Starts the monitoring of active recordings Stop Monitoring [} 143] Local Stops the monitoring of active recordings 142 Version: 1.7.1 TF3520
## Page 143

PLC API Development Environment Target platform PLC libraries to include Twin CAT v3.1.4022.25 PC or CX (x64, x86, Arm®) Tc3_Analytics Storage Provider Also see about this 2 ST_Recording Status [} 145] 2 ST_Recording Status [} 145] 7.1.3.1 Call Syntax METHOD Call : BOOL Return value Name Type Description Call BOOL 7.1.3.2 Start Monitoring Syntax METHOD Start Monitoring : BOOL VAR END_VAR Return value Name Type Description Start Monitoring BOOL Is TRUE when completed 7.1.3.3 Stop Monitoring Syntax METHOD Stop Monitoring : BOOL VAR_INPUT END_VAR TF3520 Version: 1.7.1 143
## Page 144

PLC API Return value Name Type Description Stop Monitoring BOOL Is TRUE when completed. 7.2 Data types 7.2.1 ST_ALY_SP_Config Syntax Definition: TYPE ST_Msg : STRUCT s Main Topic : T_Max String; s Provider Guid : GUID; st Conn Settings : ST_Connection Settings END_STRUCT END_TYPE Parameter Name Type Descriptiom s Main Topic T_Max String The main topic where the Twin CAT Analytics Storage Provider is located on the message broker s Provider Guid GUID The individual GUID of the Twin CAT Analytics Storage Provider Instance st Conn Settings ST_Connection Settings [} 144] MQTT connection settings to connect with the message broker 7.2.2 ST_Connection Settings Syntax Definition: TYPE ST_Connection Settings : STRUCT s Host Name : T_Max String; n Host Port : UINT := 1883; s User Id : T_Max String; s Password : T_Max String; b With Certificate : BOOL := FALSE; s CA : T_Max String; s Cert : T_Max String; s Key : T_Max String; s Key Pwd : T_Max String; END_STRUCT END_TYPE 144 Version: 1.7.1 TF3520
## Page 145

PLC API Parameter Name Type Descriptiom s Host Name T_Max String s Host Name can be specified as name or as IP address. If no information is provided, the local host is used. n Host Port UINT The host port can be specified here. The default is 1883. s User Id T_Max String Optionally, a user name can be specified. s Password T_Max String A password for the user name can be entered here. b With Certificate BOOL If TRUE the certificates will be used for communication s CA T_Max String Certificate of the certificate authority (CA) s Cert T_Max String Client certificate to be used for authentication at the broker s Key T_Max String Private key of the client s Key Pwd T_Max String Password of the private key, if applicable 7.2.3 ST_Recording Status Syntax Definition: TYPE ST_Recording Status : STRUCT s Alias : T_Max String; s Record Name : T_Max String; s Record Data Key : GUID; n Record ID : LINT; n Start Timestamp : LINT; n Last Received Sample : LINT; e Status : E_Recording State; END_STRUCT END_TYPE Parameters Name Type Description s Alias T_Max String s Alias contains the alias name of the recording. s Record Name T_Max String s Record Name contains the name of the record. s Record Data Key GUID Individual GUID of the recording. n Record ID LINT Specific ID of the recording. n Start Timestamp LINT Start time of the recording. n Last Received Sample LINT Time of the last received data packet of the recording. e Status E_Recording State [} 147] Status of the recording. 7.2.4 E_Cancel Type Syntax Definition: TYPE E_Cancel Type : ( Historical Data := 0, All Record Data )INT; END_TYPE TF3520 Version: 1.7.1 145
## Page 146

PLC API Parameter Name Descriptiom Historical Data Canceled the selected historical data stream All Record Data Canceled all running recordings 7.2.5 E_Control Mode Syntax Definition: TYPE E_Control Mode : ( Start := 0, Stop, Delete Settings )INT; END_TYPE Parameters Name Description Start Starting the "Storage" is triggered. Stop Stopping the "Storage" is triggered. Delete Settings "Storage" configuration should be deleted (only works if the "Storage" is not online). 7.2.6 E_Data Storage Type Syntax Definition: TYPE E_Data Storage Type : ( Empty := 0, Analytics File, Azure Blob, Ms SQL_Binary, Influx DB, Ms SQL_Plain, CSVFile )INT; END_TYPE Parameters Name Description Empty Unknown "Storage" type Analyttics File Analytics File (Twin CAT Analytics own data format) Azure Blob Microsoft Azure Blob Ms SQL_Binary Microsoft SQL Server (data in binary format) Influx DB Influx 2.x database Ms SQL_Plain Microsoft SQL Server (data in plain text) CSVFile CSV file 146 Version: 1.7.1 TF3520
## Page 147

PLC API 7.2.7 E_Raw Data Format Syntax Definition: TYPE E_Raw Data Format : ( Bin := 0, Json )INT; END_TYPE Parameter Name Descriptiom Bin Analytics Binary Stream Format Json Twin CAT Json Format (actually not supported) 7.2.8 E_Recording State Syntax Definition: TYPE E_Recording State : ( Not_Initialized := 0, Initializing, Recording Canceled, Running, Running_Queue Hyst, Stopping_Receiving Data Stopped, Recording Done, Waiting For Data, Error )INT; END_TYPE Parameters Name Description Not_Initialized Recording triggered. Waits for input data description for initialization. Initialized Recording successfully initialized. Recording Canceled Recording canceled. Running Recording running. Input data is saved. Running_Queue Hyst Recording running. Input data cannot be saved fast enough. Data loss! Stopping_Receiving Data Stopped Recording is stopped. Recording Done Recording is done. Waiting For Data Recording running. No data arrives from the Analytics Logger. Error An error has occurred during recording. 7.2.9 E_Record Mode Syntax Definition: TF3520 Version: 1.7.1 147
## Page 148

PLC API TYPE E_Record Mode : ( Start := 0, Stop )INT; END_TYPE Parameter Name Descriptiom Start Starts the recording of the configured record Stop Stops the recording 7.2.10 E_Reload Type Syntax Definition: TYPE E_Reload Type : ( All := 0, Specific )INT; END_TYPE Parameters Name Description All All records are read in again. Specific Only one specific record will be reread. 7.2.11 E_Ring Buffer Mode Syntax Definition: TYPE E_Ring Buffer Mode: ( None := 0, Time Based, Data Based )INT; END_TYPE Parameter Name Descriptiom None Recording without ringbuffer mode Time Based Ringbuffer based on a given time periode Data Based Ringbuffer based on a given max data size 7.2.12 E_Time Mode Syntax Definition: TYPE E_Time Mode : ( Milliseconds := 0, Seconds, Minutes, 148 Version: 1.7.1 TF3520
## Page 149

PLC API Hours, Days ) INT; END_TYPE Parameters Name Description Milliseconds Resolution of the "Duration" parameter in milliseconds. Seconds Resolution of the "Duration" parameter in seconds. Minutes Resolution of the "Duration" parameter in minutes. Hours Resolution of the "Duration" parameter in hours. Days Resolution of the "Duration" parameter in days. 7.2.13 E_Set Get Historical Data State Syntax Definition: TYPE E_Set Get Historical Data State : ( Pause, Continue_, Restart, Stop, Update )INT; END_TYPE Parameters Name Description Break Playback of the recording is paused. Continue_ Playback of the recording continues. Restart Playback of the recording is restarted. Stop Playback of the recording is stopped. Update Parameters for playing the recording are updated. 7.2.14 E_Storage State Syntax Definition: TYPE E_Storage State : ( unknown := 0, error, starting, online, shutting Down, offline )INT; END_TYPE TF3520 Version: 1.7.1 149
## Page 150

PLC API Parameters Name Description Unknown Status of the "Storage" is unknown Error "Storage" is in the error state. No more requests will be processed. Starting The "Storage" starts up and connects. Online The "Storage" is running and ready for requests. Shutting Down The "Storage" is shut down. Offline The "Storage" is off and cannot be reached. 7.2.15 E_Symbol Mode Syntax Definition: TYPE E_Symbol Mode : ( All := 0, Subset )INT; END_TYPE Parameter Name Descriptiom All All symbols of the stream will be used Subset Only a subset of symbols will be used 7.2.16 E_Pipeline Cmd Type Syntax Definition: TYPE E_Pipeline Cmd Type : ( Restart Rule := 5, Start := 6, Stop := 7 )INT; END_TYPE Parameters Name Description Restart Rule Restarts a specific rule of a Rule Engine Pipeline. Start Starts a Rule Engine Pipeline Stop Stops a Rule Engine Pipeline 150 Version: 1.7.1 TF3520
## Page 151

Samples 8 Samples 8.1 PLC Client This PLC sample shows the use of the Twin CAT Analytics Storage Provider library. The sample code shows reading and writing. For the sample to work coherently, both the use of the Analytics Logger for sending measured data to an MQTT Message Broker and the import of historical data via the Analytics Stream Helper are shown. The basis is an appropriately set up native MQTT Message Broker and an Analytics Storage Provider service. The PLC sample shows the following steps: 1. Analytics Logger: stream of variables from a Global Variable List to a MQTT Message Broker. 2. Analytics Storage Provider: starting and stopping stores and recordings, as well as reading recordings and historical data. 3. Analytics Stream Helper: receiving the historical data from the Analytics Storage Provider and mapping the data into a Global Variable List for the historical data. Analytics Storage Provider GUID Glossary When using the Storage Provider, various GUIDs occur that are required to identify services and data. The following describes where the GUIDs come from, what purpose they serve and where they can be viewed if necessary. The analytics data stream sent by the Analytics Logger is basically described by three parameters: 1. Topic [STRING] Where is the data sent to? 2. Twin CAT System ID [GUID] From which Twin CAT system is the data sent? 3. Layout ID / Symbol Info ID [GUID] What does the data look like? The GUID is a hash of the symbol information. The above parameters are required to identify recordings at the Storage Provider. Several Storage Providers can be connected to an MQTT Message Broker. Two parameters are required to identify a Storage Provider. 1. Main Topic [STRING](Where are the data/services provided?) 2. Provider Guid [GUID](Unique identifier of the service) Recorder Guid is used to recognize who has started a recording at the Storage Provider. This GUID is automatically generated at each Storage Provider Manager or client and attached to the Start Record commands. In the PLC, this can be freely assigned at the Start Record command. From version 3.2.14, three additional GUIDs can be specified on the Start Record command. 1. Storage [GUID] This GUID specifies the storage in which the data is to be saved. The GUID is generated automatically in the Analytics Storage Provider Configurator. It can be read there or in the Analytics Storage Provider Manager. If no GUID is specified, the Master Storage is used. 2. Sub Broker/Data Broker [GUID] This GUID specifies the data broker from which the Analytics Stream is to be received. The Analytics Storage Provider offers the option of recording from several message brokers. The GUID is generated automatically in the Analytics Storage Provider Configurator. It can be read there or in the Analytics Storage Provider Manager. If no GUID is specified, the Master Data Broker is used. TF3520 Version: 1.7.1 151
## Page 152

Samples 3. Data Key [GUID] The Data Key can be used to find, read and monitor recordings in progress. This Data Key can be freely selected in the PLC. If no Data Key is specified, a Data Key is automatically generated by the Storage Provider. The following screenshots contain the parameters and GUIDs described above. Topic / Twin CAT system ID / Layout (System Info ID) 152 Version: 1.7.1 TF3520
## Page 153

Samples Storage Provider Main Topic / Provider Guid / Recorder Guid Storage GUID TF3520 Version: 1.7.1 153
## Page 154

Samples Data Broker GUID Sample code architecture All relevant parts of the configuration and the program code are marked in the following picture: 154 Version: 1.7.1 TF3520
## Page 155

Samples Stream Helper For receiving the historical data sent by the Analytics Storage Provider via MQTT. Variable Live/Historical The GVL is for the live data and the GVL_Hist is for the historical data. Storage Provider Command Helper Functions These Helper Functions generate the commands for communication with the Storage Provider Service in JSON format. MAIN program TF3520 Version: 1.7.1 155
## Page 156

Samples The Main program invokes communication to the Analytics Storage Provider. The Main Historical program implements the mapping of historical data from the Stream Helper into the GVL_Hist. Analytics Logger Sends the variables of the GVL to an MQTT Message Broker. Sample Start NOTICE Too little router memory can lead to system crashes Increase the router memory in the real-time settings to 256 MB. It is also recommended to increase the maximum stack size of the global task configuration to 512 KB. Before the sample can be started, you must set the MQTT Message Broker you are using in three different places. Analytics Stream Helper: Analytics Logger: 156 Version: 1.7.1 TF3520
## Page 157

Samples MAIN program: You must then change the s Main Topic and the s Provider GUID for the FB_ALY_Storage Provider. This can be found as described above in this document. Now go to the MAIN program to control the sample. With the enum e Ctrl you can set the action you would like to perform. The available options are: • Read ASPDescription (also contains the storage description) • Read Data Broker • Start Storage • Shutdown Storage • Start Record • Stop Record • Is Recording Running • Read Historical Streams TF3520 Version: 1.7.1 157
## Page 158

Samples • Read Records • Get Historical With a rising edge at the variable b Execute the action selected in the enum is executed. If you have made more than one record, you can see this in the array a Record Info. With the index it is then possible to select the different records. The timespans are also displayed, you could theoretically still adjust these within the timespan. To do this, you would need to modify the logic of the sample in the helper function F_Create Aly SPGet Hist Cmd accordingly. The Storage Provider Recorder GUID selected in the document above can optionally be set in the PLC in the F_Create Aly SPStart Record Cmd function. Theoretically, it can be any GUID, it is only used to identify the recorder. Download: https://infosys.beckhoff.com/content/1033/tf3500_tc3_analytics_logger/Resources/ 11270100747.zip 158 Version: 1.7.1 TF3520
## Page 159

Appendix 9 Appendix 9.1 Glossary The following table explains key terms in connection with the Storage Provider. TF3520 Version: 1.7.1 159
## Page 160

Appendix Name Function Unique identifier Descriptive parame- (automatically gener- ters ated) (configurable by the user) Storage Software tool for configuring the local Provider Storage Provider. Configurator Storage Software tool for working with the Storage Provider Provider. This can be used to start, stop and Manager manage data recordings. The Storage Provider Manager can be used for both the local and the remote Storage Provider. Storage Command line tool for interactions with the Provider CLI Storage Provider. Client Storage PLC library for interactions with the Storage Provider PLC Provider. library Storage Software application for historicizing data Provider Guid Provider Alias Provider such as from the Analytics Logger. Both the acquisition of the data to be stored and the provision of stored data is carried out via MQTT. The Storage Provider can be operated under Windows and Free BSD®. Storage Data sink of a Storage Provider (e.g. MS Storage Guid Storage Alias SQL or CSV). Message MQTT message broker via which data can Broker be transmitted using the MQTT protocol. Host Broker Central message broker on which the Broker Guid Broker Alias Storage Provider provides information and receives commands. Data Broker Additional message broker from which the Broker Guid Broker Alias Storage Provider can obtain data to be recorded. Pipeline Description of a data flow. Components are Pipeline Guid Pipeline Alias (can be data sources (Input Sources), processing set before starting the steps (Rules) and data storage pipeline) (Recordings). Rule Engine Processing unit within the Storage Provider. Rule Processing rule within a pipeline that is used Rule ID Rule Alias for filtering, aggregating and sampling data. Recording Recording configuration for data. This Recording ID Recording Alias (can includes which data should be recorded in be set before starting which storage. the pipeline) Record Data recording based on a defined recording Record ID, additionally Record Alias (can be configuration (Recording). Record Data Key if set before starting the recording is running pipeline) Input Source Data source for a pipeline. This can be an MQTT livestream or a Historical Stream. Live Stream Data from the Analytics Logger, Io T Data Agent or EK9160. Historical Stre MQTT data stream from the Storage Hist Stream ID Hist Stream Alias - this am Provider, which contains all information corresponds to the about a recording and the associated Recording Alias records. One Historical Stream is generated per recording. 160 Version: 1.7.1 TF3520
## Page 161

Appendix Name Function Unique identifier Descriptive parame- (automatically gener- ters ated) (configurable by the user) Recorder Identification of the client that communicates Recorder Guid Recorder Alias with the provider. It is provided when the pipeline is started in order to be able to trace who started the pipeline. 9.2 FAQ - frequently asked questions and answers In this section frequently asked questions are answered, in order to facilitate your work with the Twin CAT Analytics Storage Provider (ASP). If you have any further questions, please contact our support team at support@beckhoff.com. 1. How can I manage the table schema of MS SQL with ASP? [} 161] 2. Can I control the Storage Provider in a programmable way? [} 161] 3. Is it also possible to save results from the Analytics Runtime? [} 161] 4. Are open source software components used in Twin CAT Measurement products? [} 161] 5. What factors influence the data throughput of the storage provider? [} 161] How can I manage the table schema of MS SQL with ASP? You don't have to worry about the table schema. This is done completely by the Analytics Storage Provider. You only have to specify on which database server the data should be stored. If you want to see data in your own table structure, you have to stream the data into a Twin CAT Analytics Runtime and have the Twin CAT Database Server write the data in your structure. Can I control the Storage Provider in a programmable way? Yes, via the PLC interface for the Twin CAT Storage Provider. You can start/stop recordings or retrieve historical data (raw data or result data). Is it also possible to save results from the Analytics Runtime? Yes, this is possible. For this purpose, you can choose to send the results to an MQTT Message Broker when generating the Analytics Runtime from the Analytics Workbench configurator. This data stream can be captured by the Storage Provider. Are open source software components used in Twin CAT Measurement products? Yes, various open source components are used. Please see the information on the page Third-party components [} 162]. What factors influence the data throughput of the storage provider? The data throughput depends on many influencing variables. Primarily of system and network resources. An overview: • System properties (CPU, RAM) • Writing speed and quality of the storage medium (SSD) • Network properties • Complexity of symbolism (data type, structures, arrays, etc.) • Mode of historization (total symbolism allows higher throughput, a subset may be more costly depending on its size) • Compression level of the stream (the stronger the compression, the higher the system load) • The size of a sample • Total size of a data packet (number of samples per packet) • The number of parallel recordings that the storage provider manages TF3520 Version: 1.7.1 161
## Page 162

Appendix 9.3 Third-party components This software contains third-party components. Please refer to the license file provided in the following folder for further information: C:\Program Files(x86)\Beckhoff\Legal\Twin CAT-XAR-Analytics Storage Provider 162 Version: 1.7.1 TF3520
## Page 163

Support and Service 10 Support and Service Beckhoff and their partners around the world offer comprehensive support and service, making available fast and competent assistance with all questions related to Beckhoff products and system solutions. Download finder Our download finder contains all the files that we offer you for downloading. You will find application reports, technical documentation, technical drawings, configuration files and much more. The downloads are available in various formats. Beckhoff's branch offices and representatives Please contact your Beckhoff branch office or representative for local support and service on Beckhoff products! The addresses of Beckhoff's branch offices and representatives round the world can be found on our internet page: www.beckhoff.com You will also find further documentation for Beckhoff components there. Beckhoff Support Support offers you comprehensive technical assistance, helping you not only with the application of individual Beckhoff products, but also with other, wide-ranging services: • support • design, programming and commissioning of complex automation systems • and extensive training program for Beckhoff system components Hotline: +49 5246 963-157 e-mail: support@beckhoff.com Beckhoff Service The Beckhoff Service Center supports you in all matters of after-sales service: • on-site service • repair service • spare parts service • hotline service Hotline: +49 5246 963-460 e-mail: service@beckhoff.com Beckhoff Headquarters Beckhoff Automation Gmb H & Co. KG Huelshorstweg 20 33415 Verl Germany Phone: +49 5246 963-0 e-mail: info@beckhoff.com web: www.beckhoff.com TF3520 Version: 1.7.1 163
## Page 164

Trademark statements Beckhoff®, ATRO®, Ether CAT®, Ether CAT G®, Ether CAT G10®, Ether CAT P®, MX-System®, Safety over Ether CAT®, TC/BSD®, Twin CAT®, Twin CAT/BSD®, Twin SAFE®, XFC®, XPlanar® and XTS® are registered and licensed trademarks of Beckhoff Automation Gmb H. Third-party trademark statements Arm, Arm9 and Cortex are trademarks or registered trademarks of Arm Limited (or its subsidiaries or affiliates) in the US and/or elsewhere. Free BSD is a registered trademark of The Free BSD Foundation and is used by Beckhoff with the permission of The Free BSD Foundation. Microsoft, Microsoft Azure, Microsoft Edge, Power Shell, Visual Studio, Windows and Xbox are trademarks of the Microsoft group of companies.
## Page 165

More Information: www.beckhoff.com/tf3520 Beckhoff Automation Gmb H & Co. KG Hülshorstweg 20 33415 Verl Germany Phone: +49 5246 9630 info@beckhoff.com www.beckhoff.com
